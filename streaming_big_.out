15/04/13 21:21:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 35
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1013748491891138785.jar tmpDir=null
15/04/13 21:21:56 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:21:56 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:21:56 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:21:57 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:21:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4754
15/04/13 21:21:58 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4754
15/04/13 21:21:58 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4754/
15/04/13 21:21:58 INFO mapreduce.Job: Running job: job_1422482982071_4754
15/04/13 21:22:04 INFO mapreduce.Job: Job job_1422482982071_4754 running in uber mode : false
15/04/13 21:22:04 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:22:15 INFO mapreduce.Job:  map 12% reduce 0%
15/04/13 21:22:16 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 21:22:18 INFO mapreduce.Job:  map 27% reduce 0%
15/04/13 21:22:19 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 21:22:21 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 21:22:22 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 21:22:24 INFO mapreduce.Job:  map 50% reduce 0%
15/04/13 21:22:25 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 21:22:27 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 21:22:28 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:22:31 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:23:14 INFO mapreduce.Job:  map 72% reduce 0%
15/04/13 21:23:15 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:23:16 INFO mapreduce.Job:  map 90% reduce 0%
15/04/13 21:23:17 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 21:23:18 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:23:19 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:23:20 INFO mapreduce.Job:  map 100% reduce 3%
15/04/13 21:23:21 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:23:22 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:23:27 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:23:28 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:23:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000030_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000027_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000027_0/part-00027 (inode 3633756): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000027_0_1537754232_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000027_0/part-00027 (inode 3633756): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000027_0_1537754232_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000009_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000009_0/part-00009 (inode 3633744): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000009_0_-665184625_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000000_0/part-00000 (inode 3633750): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000000_0_-1466083544_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000014_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000014_0/part-00014 (inode 3633764): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000014_0_-1471811186_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000014_0/part-00014 (inode 3633764): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000014_0_-1471811186_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000023_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000023_0/part-00023 (inode 3633742): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000023_0_-1354863302_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000033_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000033_0/part-00033 (inode 3633732): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000033_0_-1962996096_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000011_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000031_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000006_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000018_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:30 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000020_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:30 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000002_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000002_1/part-00002 (inode 3633790): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000002_1_-581344134_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:30 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000030_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000018_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000018_1/part-00018 (inode 3633800): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000018_1_205632486_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000018_1/part-00018 (inode 3633800): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000018_1_205632486_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000027_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000027_1/part-00027 (inode 3633786): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000027_1_417316915_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000027_1/part-00027 (inode 3633786): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000027_1_417316915_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000023_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000023_1/part-00023 (inode 3633788): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000023_1_1185040057_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000009_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000009_1/part-00009 (inode 3633802): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000009_1_-53473070_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000009_1/part-00009 (inode 3633802): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000009_1_-53473070_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000000_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000033_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000020_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:32 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000033_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000033_2/part-00033 (inode 3633818): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000033_2_2044344988_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:32 INFO mapreduce.Job: Task Id : attempt_1422482982071_4754_r_000002_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4754_r_000002_2/part-00002 (inode 3633820): File does not exist. Holder DFSClient_attempt_1422482982071_4754_r_000002_2_463433459_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:23:33 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 21:23:34 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:23:35 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:23:40 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:23:41 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:23:41 INFO mapreduce.Job: Job job_1422482982071_4754 completed successfully
15/04/13 21:23:41 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=70110743
		FILE: Number of bytes written=156464070
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=499
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=66
	Job Counters 
		Failed reduce tasks=24
		Launched map tasks=134
		Launched reduce tasks=59
		Data-local map tasks=62
		Rack-local map tasks=72
		Total time spent by all maps in occupied slots (ms)=18520992
		Total time spent by all reduces in occupied slots (ms)=473968
		Total time spent by all map tasks (ms)=9260496
		Total time spent by all reduce tasks (ms)=236984
		Total vcore-seconds taken by all map tasks=9260496
		Total vcore-seconds taken by all reduce tasks=236984
		Total megabyte-seconds taken by all map tasks=74972975616
		Total megabyte-seconds taken by all reduce tasks=2843808000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70138673
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70138673
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =4690
		Failed Shuffles=0
		Merged Map outputs=4690
		GC time elapsed (ms)=50456
		CPU time spent (ms)=7500860
		Physical memory (bytes) snapshot=267899088896
		Virtual memory (bytes) snapshot=1697570549760
		Total committed heap usage (bytes)=444813361152
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:23:41 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m50.831s
user	0m11.625s
sys	0m0.746s
50 35
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3711615323185078118.jar tmpDir=null
15/04/13 21:23:46 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:46 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:47 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:23:47 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:23:47 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
Streaming Command Failed!

real	0m5.644s
user	0m7.965s
sys	0m0.446s
15/04/13 21:23:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 25
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6831228147851094016.jar tmpDir=null
15/04/13 21:23:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:53 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:23:54 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:23:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4757
15/04/13 21:23:55 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4757
15/04/13 21:23:55 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4757/
15/04/13 21:23:55 INFO mapreduce.Job: Running job: job_1422482982071_4757
15/04/13 21:23:59 INFO mapreduce.Job: Job job_1422482982071_4757 running in uber mode : false
15/04/13 21:23:59 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:24:09 INFO mapreduce.Job:  map 1% reduce 0%
15/04/13 21:24:10 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:24:11 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 21:24:13 INFO mapreduce.Job:  map 26% reduce 0%
15/04/13 21:24:14 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 21:24:16 INFO mapreduce.Job:  map 38% reduce 0%
15/04/13 21:24:17 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 21:24:18 INFO mapreduce.Job:  map 40% reduce 0%
15/04/13 21:24:19 INFO mapreduce.Job:  map 49% reduce 0%
15/04/13 21:24:20 INFO mapreduce.Job:  map 51% reduce 0%
15/04/13 21:24:22 INFO mapreduce.Job:  map 60% reduce 0%
15/04/13 21:24:23 INFO mapreduce.Job:  map 62% reduce 0%
15/04/13 21:24:25 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:24:26 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:24:29 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:25:09 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 21:25:10 INFO mapreduce.Job:  map 70% reduce 0%
15/04/13 21:25:11 INFO mapreduce.Job:  map 75% reduce 0%
15/04/13 21:25:12 INFO mapreduce.Job:  map 79% reduce 0%
15/04/13 21:25:13 INFO mapreduce.Job:  map 86% reduce 0%
15/04/13 21:25:14 INFO mapreduce.Job:  map 91% reduce 0%
15/04/13 21:25:15 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 21:25:16 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:25:17 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:25:18 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:25:19 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:25:20 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 21:25:21 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 21:25:22 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:25:23 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:25:24 INFO mapreduce.Job: Job job_1422482982071_4757 completed successfully
15/04/13 21:25:25 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110683
		FILE: Number of bytes written=155471270
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=477
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=25
		Data-local map tasks=63
		Rack-local map tasks=71
		Total time spent by all maps in occupied slots (ms)=19022800
		Total time spent by all reduces in occupied slots (ms)=515852
		Total time spent by all map tasks (ms)=9511400
		Total time spent by all reduce tasks (ms)=257926
		Total vcore-seconds taken by all map tasks=9511400
		Total vcore-seconds taken by all reduce tasks=257926
		Total megabyte-seconds taken by all map tasks=77004294400
		Total megabyte-seconds taken by all reduce tasks=3095112000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70130633
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70130633
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =3350
		Failed Shuffles=0
		Merged Map outputs=3350
		GC time elapsed (ms)=48197
		CPU time spent (ms)=8135830
		Physical memory (bytes) snapshot=265962242048
		Virtual memory (bytes) snapshot=1564378611712
		Total committed heap usage (bytes)=423752171520
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:25:25 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m38.142s
user	0m12.324s
sys	0m0.713s
15/04/13 21:25:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 25
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6601885379206700333.jar tmpDir=null
15/04/13 21:25:29 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:25:29 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:25:30 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:25:34 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:25:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4760
15/04/13 21:25:35 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4760
15/04/13 21:25:35 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4760/
15/04/13 21:25:35 INFO mapreduce.Job: Running job: job_1422482982071_4760
15/04/13 21:25:41 INFO mapreduce.Job: Job job_1422482982071_4760 running in uber mode : false
15/04/13 21:25:41 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:25:51 INFO mapreduce.Job:  map 11% reduce 0%
15/04/13 21:25:52 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 21:25:53 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 21:25:54 INFO mapreduce.Job:  map 26% reduce 0%
15/04/13 21:25:55 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:25:56 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 21:25:57 INFO mapreduce.Job:  map 37% reduce 0%
15/04/13 21:25:58 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 21:25:59 INFO mapreduce.Job:  map 43% reduce 0%
15/04/13 21:26:00 INFO mapreduce.Job:  map 49% reduce 0%
15/04/13 21:26:01 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 21:26:02 INFO mapreduce.Job:  map 55% reduce 0%
15/04/13 21:26:03 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 21:26:04 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:26:05 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:26:06 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:26:21 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 21:26:22 INFO mapreduce.Job:  map 75% reduce 0%
15/04/13 21:26:23 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:26:24 INFO mapreduce.Job:  map 89% reduce 0%
15/04/13 21:26:25 INFO mapreduce.Job:  map 94% reduce 0%
15/04/13 21:26:26 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:26:27 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:26:28 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:26:32 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 21:26:33 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 21:26:35 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 21:26:38 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 21:26:41 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 21:26:44 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 21:26:45 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 21:26:47 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 21:26:50 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 21:26:53 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 21:26:56 INFO mapreduce.Job:  map 100% reduce 43%
15/04/13 21:26:57 INFO mapreduce.Job:  map 100% reduce 44%
15/04/13 21:26:59 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 21:27:01 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 21:27:02 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 21:27:06 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:27:07 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:27:09 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:27:11 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:27:12 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:13 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000020_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000020_0/part-00020 (inode 3634487): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000020_0_-1604754721_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000020_0/part-00020 (inode 3634487): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000020_0_-1604754721_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:14 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:27:15 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:16 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:27:17 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:27:17 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000023_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000023_0/part-00023 (inode 3634493): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000023_0_-1119099448_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000023_0/part-00023 (inode 3634493): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000023_0_-1119099448_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:18 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:19 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:27:20 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:27:22 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:27:22 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000003_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000003_0/part-00003 (inode 3634495): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000003_0_1735915618_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000003_0/part-00003 (inode 3634495): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000003_0_1735915618_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:22 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000019_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000019_0/part-00019 (inode 3634499): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000019_0_491310908_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000019_0/part-00019 (inode 3634499): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000019_0_491310908_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:22 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000024_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000024_0/part-00024 (inode 3634491): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000024_0_1437310280_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000024_0/part-00024 (inode 3634491): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000024_0_1437310280_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:23 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 21:27:24 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:27:26 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:28 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:27:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000015_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000015_0/part-00015 (inode 3634497): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000015_0_1721789613_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000015_0/part-00015 (inode 3634497): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000015_0_1721789613_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:30 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000014_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000014_0/part-00014 (inode 3634505): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000014_0_225053388_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000014_0/part-00014 (inode 3634505): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000014_0_225053388_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:32 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:27:33 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:27:34 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:27:36 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:27:39 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:27:41 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:42 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:27:43 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:27:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000017_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_0/part-00017 (inode 3634550): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_0_-47299855_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_0/part-00017 (inode 3634550): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_0_-47299855_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:45 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:46 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:27:49 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:27:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000012_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_0/part-00012 (inode 3634507): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_0_-2049460128_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_0/part-00012 (inode 3634507): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_0_-2049460128_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:51 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:27:52 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:55 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:27:56 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:28:00 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:28:01 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:28:02 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:28:03 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:28:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_0/part-00002 (inode 3634509): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_0_-1906150472_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_0/part-00002 (inode 3634509): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_0_-1906150472_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:04 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:28:05 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000010_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_0/part-00010 (inode 3634501): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_0_1854123220_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_0/part-00010 (inode 3634501): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_0_1854123220_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:06 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:28:09 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:28:13 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:28:14 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:28:16 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:28:17 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:28:19 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:28:22 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:28:22 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000013_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000013_0/part-00013 (inode 3634515): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000013_0_-1214686882_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000013_0/part-00013 (inode 3634515): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000013_0_-1214686882_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:23 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:28:27 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:28:33 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:28:36 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:39 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:28:42 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:28:45 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:28:46 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:28:49 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:28:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000015_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000015_1/part-00015 (inode 3634689): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000015_1_-1136552470_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000015_1/part-00015 (inode 3634689): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000015_1_-1136552470_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:51 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:28:53 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:28:57 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000014_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000014_1/part-00014 (inode 3634697): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000014_1_105426327_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000014_1/part-00014 (inode 3634697): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000014_1_105426327_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:58 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:59 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:29:02 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:29:04 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:29:07 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:29:08 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000008_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000008_0/part-00008 (inode 3634623): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000008_0_-1585773435_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000008_0/part-00008 (inode 3634623): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000008_0_-1585773435_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:29:09 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:29:12 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:29:13 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000016_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000016_0/part-00016 (inode 3634537): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000016_0_-1682064713_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000016_0/part-00016 (inode 3634537): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000016_0_-1682064713_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:29:14 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:29:16 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:29:18 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:29:19 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:29:20 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:29:20 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000017_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_1/part-00017 (inode 3634725): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_1_-790619185_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_1/part-00017 (inode 3634725): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_1_-790619185_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:29:21 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:29:22 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:29:22 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000020_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000020_1/part-00020 (inode 3634653): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000020_1_1882038980_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:23 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:29:24 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:29:27 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:29:32 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000012_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_1/part-00012 (inode 3634713): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_1_-1962177303_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_1/part-00012 (inode 3634713): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_1_-1962177303_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:29:33 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:29:33 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000023_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000023_1/part-00023 (inode 3634659): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000023_1_-855988832_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:34 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:29:36 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000001_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000001_0/part-00001 (inode 3634567): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000001_0_-118550865_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:37 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:29:37 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000024_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000024_1/part-00024 (inode 3634671): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000024_1_-1610974645_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:37 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000021_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000021_0/part-00021 (inode 3634565): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000021_0_985098926_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:38 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:29:40 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:29:41 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000000_0/part-00000 (inode 3634637): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000000_0_2073880755_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000000_0/part-00000 (inode 3634637): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000000_0_2073880755_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:29:42 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:29:43 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:29:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000019_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000019_1/part-00019 (inode 3634673): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000019_1_1698792559_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:29:44 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:29:47 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:29:48 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:29:49 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:29:52 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:29:54 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:29:56 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:29:58 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:29:59 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000003_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000003_1/part-00003 (inode 3634681): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000003_1_-155332973_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:30:00 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:30:01 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:30:03 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:30:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000002_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_1/part-00002 (inode 3634741): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_1_610997343_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_1/part-00002 (inode 3634741): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_1_610997343_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:05 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:30:08 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:30:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000010_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_1/part-00010 (inode 3634739): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_1_1410181439_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_1/part-00010 (inode 3634739): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_1_1410181439_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:10 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:30:11 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:30:12 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000007_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000007_0/part-00007 (inode 3634639): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000007_0_1878757501_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000007_0/part-00007 (inode 3634639): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000007_0_1878757501_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:13 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:30:15 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:30:16 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:30:19 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:30:20 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:30:23 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:30:24 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:30:26 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:30:28 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:30:30 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:30:31 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:30:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000022_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000022_0/part-00022 (inode 3634625): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000022_0_-976340339_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:30:32 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:30:33 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:30:33 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000011_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000011_0/part-00011 (inode 3634629): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000011_0_-1387477303_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:30:34 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:30:35 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:30:40 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:30:42 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:30:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000004_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000004_0/part-00004 (inode 3634631): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000004_0_-239003664_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:30:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000019_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000019_2/part-00019 (inode 3634904): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000019_2_-1532001825_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000019_2/part-00019 (inode 3634904): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000019_2_-1532001825_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000017_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_2/part-00017 (inode 3634892): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_2_341756251_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000017_2/part-00017 (inode 3634892): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000017_2_341756251_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:43 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:30:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000013_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000013_1/part-00013 (inode 3634875): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000013_1_-1400007807_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000013_1/part-00013 (inode 3634875): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000013_1_-1400007807_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:45 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:30:46 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000024_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000024_2/part-00024 (inode 3634906): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000024_2_-957917209_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000024_2/part-00024 (inode 3634906): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000024_2_-957917209_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:47 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:30:48 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:30:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000009_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000009_0/part-00009 (inode 3634633): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000009_0_-110535046_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:30:50 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:30:53 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:30:54 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:30:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000021_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000021_1/part-00021 (inode 3634920): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000021_1_-2024445848_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000021_1/part-00021 (inode 3634920): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000021_1_-2024445848_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:30:57 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:30:58 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:31:00 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:31:00 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000003_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000003_2/part-00003 (inode 3634910): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000003_2_-629402605_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000003_2/part-00003 (inode 3634910): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000003_2_-629402605_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:31:01 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:31:03 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:31:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000015_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000015_2/part-00015 (inode 3634877): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000015_2_1398918523_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:31:05 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:31:05 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000012_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_2/part-00012 (inode 3634900): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_2_-1795752218_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000012_2/part-00012 (inode 3634900): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000012_2_-1795752218_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:31:06 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:31:07 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000001_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000001_1/part-00001 (inode 3634922): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000001_1_-2022397881_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000001_1/part-00001 (inode 3634922): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000001_1_-2022397881_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:31:08 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:31:09 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:31:11 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:31:13 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:31:15 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:31:16 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:31:17 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:31:20 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:31:22 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:31:23 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:31:24 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:31:25 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:31:26 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:31:28 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:31:29 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:31:30 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:31:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000020_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000020_2/part-00020 (inode 3634881): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000020_2_1781734974_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:31:32 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:31:33 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:31:35 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:31:36 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:31:39 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000014_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000014_2/part-00014 (inode 3634879): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000014_2_600829130_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:31:40 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:31:42 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:31:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000023_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000023_2/part-00023 (inode 3634883): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000023_2_-300832001_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:31:43 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:31:47 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:31:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000002_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_2/part-00002 (inode 3634986): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_2_2060182510_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000002_2/part-00002 (inode 3634986): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000002_2_2060182510_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:31:51 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:31:54 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:31:56 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:31:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000008_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000008_1/part-00008 (inode 3634902): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000008_1_464834129_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000008_1/part-00008 (inode 3634902): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000008_1_464834129_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:31:57 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:32:00 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:32:03 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:32:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000010_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_2/part-00010 (inode 3635058): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_2_990734013_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000010_2/part-00010 (inode 3635058): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000010_2_990734013_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:32:04 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:32:06 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:32:08 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:32:09 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:32:11 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:32:12 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:32:13 INFO mapreduce.Job: Task Id : attempt_1422482982071_4760_r_000018_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4760_r_000018_0/part-00018 (inode 3634649): File does not exist. Holder DFSClient_attempt_1422482982071_4760_r_000018_0_-488253342_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:32:14 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:32:15 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:32:16 INFO mapreduce.Job: Job job_1422482982071_4760 failed with state FAILED due to: Task failed task_1422482982071_4760_r_000015
Job failed as tasks failed. failedMaps:0 failedReduces:1

15/04/13 21:32:16 INFO mapreduce.Job: Counters: 39
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=30437042163
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=402
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Failed reduce tasks=50
		Killed reduce tasks=23
		Launched map tasks=134
		Launched reduce tasks=73
		Data-local map tasks=58
		Rack-local map tasks=76
		Total time spent by all maps in occupied slots (ms)=10602144
		Total time spent by all reduces in occupied slots (ms)=17308118
		Total time spent by all map tasks (ms)=5301072
		Total time spent by all reduce tasks (ms)=8654059
		Total vcore-seconds taken by all map tasks=5301072
		Total vcore-seconds taken by all reduce tasks=8654059
		Total megabyte-seconds taken by all map tasks=42917478912
		Total megabyte-seconds taken by all reduce tasks=103848708000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424235893
		Input split bytes=21172
		Combine input records=0
		Spilled Records=3130842760
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=35375
		CPU time spent (ms)=6147710
		Physical memory (bytes) snapshot=260527370240
		Virtual memory (bytes) snapshot=1229740478464
		Total committed heap usage (bytes)=371125686272
	File Input Format Counters 
		Bytes Read=17901389521
15/04/13 21:32:16 ERROR streaming.StreamJob: Job not Successful!
Streaming Command Failed!

real	6m51.296s
user	0m11.360s
sys	0m0.785s
15/04/13 21:32:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 15
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3129898538704884204.jar tmpDir=null
15/04/13 21:32:21 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:21 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:21 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:32:21 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:32:21 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
Streaming Command Failed!

real	0m5.170s
user	0m7.281s
sys	0m0.400s
15/04/13 21:32:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 15
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob4651503030860604679.jar tmpDir=null
15/04/13 21:32:26 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:26 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:26 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:32:26 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:32:26 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
Streaming Command Failed!

real	0m5.323s
user	0m7.153s
sys	0m0.461s
15/04/13 21:32:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 10
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob4725840773004913416.jar tmpDir=null
15/04/13 21:32:32 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:32 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:33 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:32:33 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:32:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4769
15/04/13 21:32:34 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4769
15/04/13 21:32:34 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4769/
15/04/13 21:32:34 INFO mapreduce.Job: Running job: job_1422482982071_4769
15/04/13 21:32:39 INFO mapreduce.Job: Job job_1422482982071_4769 running in uber mode : false
15/04/13 21:32:39 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:32:49 INFO mapreduce.Job:  map 8% reduce 0%
15/04/13 21:32:50 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 21:32:51 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:32:52 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 21:32:53 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:32:54 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 21:32:55 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 21:32:56 INFO mapreduce.Job:  map 40% reduce 0%
15/04/13 21:32:57 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 21:32:58 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 21:32:59 INFO mapreduce.Job:  map 52% reduce 0%
15/04/13 21:33:00 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:33:01 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 21:33:02 INFO mapreduce.Job:  map 63% reduce 0%
15/04/13 21:33:03 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 21:33:04 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:33:06 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:33:49 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 21:33:50 INFO mapreduce.Job:  map 70% reduce 0%
15/04/13 21:33:51 INFO mapreduce.Job:  map 77% reduce 0%
15/04/13 21:33:52 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:33:53 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 21:33:54 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 21:33:55 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:33:56 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:33:57 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:34:00 INFO mapreduce.Job:  map 100% reduce 10%
15/04/13 21:34:01 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:34:03 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:34:04 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:34:05 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:34:06 INFO mapreduce.Job: Job job_1422482982071_4769 completed successfully
15/04/13 21:34:06 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110593
		FILE: Number of bytes written=153982070
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=432
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=10
		Data-local map tasks=63
		Rack-local map tasks=71
		Total time spent by all maps in occupied slots (ms)=19172352
		Total time spent by all reduces in occupied slots (ms)=218542
		Total time spent by all map tasks (ms)=9586176
		Total time spent by all reduce tasks (ms)=109271
		Total vcore-seconds taken by all map tasks=9586176
		Total vcore-seconds taken by all reduce tasks=109271
		Total megabyte-seconds taken by all map tasks=77609680896
		Total megabyte-seconds taken by all reduce tasks=1311252000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70118573
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70118573
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =1340
		Failed Shuffles=0
		Merged Map outputs=1340
		GC time elapsed (ms)=42959
		CPU time spent (ms)=9372520
		Physical memory (bytes) snapshot=262886850560
		Virtual memory (bytes) snapshot=1364142088192
		Total committed heap usage (bytes)=392174297088
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:34:06 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m39.218s
user	0m10.895s
sys	0m0.645s
15/04/13 21:34:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 10
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8943954504880252466.jar tmpDir=null
15/04/13 21:34:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:34:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:34:12 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:34:13 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:34:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4772
15/04/13 21:34:13 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4772
15/04/13 21:34:13 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4772/
15/04/13 21:34:13 INFO mapreduce.Job: Running job: job_1422482982071_4772
15/04/13 21:34:20 INFO mapreduce.Job: Job job_1422482982071_4772 running in uber mode : false
15/04/13 21:34:20 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:34:31 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:34:32 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 21:34:34 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 21:34:35 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 21:34:36 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 21:34:37 INFO mapreduce.Job:  map 40% reduce 0%
15/04/13 21:34:38 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 21:34:39 INFO mapreduce.Job:  map 43% reduce 0%
15/04/13 21:34:40 INFO mapreduce.Job:  map 52% reduce 0%
15/04/13 21:34:41 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:34:42 INFO mapreduce.Job:  map 55% reduce 0%
15/04/13 21:34:43 INFO mapreduce.Job:  map 63% reduce 0%
15/04/13 21:34:44 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 21:34:45 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:34:46 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:35:01 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 21:35:02 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 21:35:03 INFO mapreduce.Job:  map 79% reduce 0%
15/04/13 21:35:04 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:35:05 INFO mapreduce.Job:  map 85% reduce 0%
15/04/13 21:35:06 INFO mapreduce.Job:  map 89% reduce 0%
15/04/13 21:35:07 INFO mapreduce.Job:  map 94% reduce 0%
15/04/13 21:35:08 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 21:35:09 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:35:10 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:35:12 INFO mapreduce.Job:  map 99% reduce 12%
15/04/13 21:35:14 INFO mapreduce.Job:  map 100% reduce 12%
15/04/13 21:35:22 INFO mapreduce.Job:  map 100% reduce 13%
15/04/13 21:35:34 INFO mapreduce.Job:  map 100% reduce 14%
15/04/13 21:35:36 INFO mapreduce.Job:  map 100% reduce 15%
15/04/13 21:35:37 INFO mapreduce.Job:  map 100% reduce 17%
15/04/13 21:35:40 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 21:35:43 INFO mapreduce.Job:  map 100% reduce 19%
15/04/13 21:35:49 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 21:35:55 INFO mapreduce.Job:  map 100% reduce 22%
15/04/13 21:35:58 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 21:35:59 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 21:36:01 INFO mapreduce.Job:  map 100% reduce 30%
15/04/13 21:36:04 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 21:36:07 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 21:36:19 INFO mapreduce.Job:  map 100% reduce 35%
15/04/13 21:36:25 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 21:36:28 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 21:36:30 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 21:36:31 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 21:36:35 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 21:36:38 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:36:41 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:36:47 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:36:50 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:36:56 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:36:58 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:36:59 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:37:01 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:37:02 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:37:04 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:37:05 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:37:08 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:37:11 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:37:14 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:37:17 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:37:20 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:37:23 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:37:29 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:37:35 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:37:41 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:37:53 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:38:15 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:38:29 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:38:36 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:38:53 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:39:08 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:39:28 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:39:46 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:39:57 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 21:40:09 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 21:40:22 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 21:40:48 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 21:40:55 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 21:41:24 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 21:41:37 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 21:41:46 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 21:41:55 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 21:42:11 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 21:42:31 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:42:44 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:43:02 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:43:29 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 21:43:44 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:44:02 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:44:51 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 21:45:15 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:45:30 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 21:46:55 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 21:47:23 INFO mapreduce.Job: Task Id : attempt_1422482982071_4772_r_000002_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4772_r_000002_0/part-00002 (inode 3635877): File does not exist. Holder DFSClient_attempt_1422482982071_4772_r_000002_0_-2036437232_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:47:24 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:47:34 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:47:58 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:48:29 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 21:48:50 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:49:02 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:49:11 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 21:49:18 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 21:49:24 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:49:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4772_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4772_r_000000_0/part-00000 (inode 3635896): File does not exist. Holder DFSClient_attempt_1422482982071_4772_r_000000_0_-842024457_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:49:32 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 21:49:43 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 21:50:20 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:50:53 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:51:26 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:51:38 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 21:51:47 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:51:53 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:53:35 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 21:55:56 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 21:58:49 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:59:00 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 21:59:36 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 22:02:01 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:03:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4772_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4772_r_000000_1/part-00000 (inode 3636954): File does not exist. Holder DFSClient_attempt_1422482982071_4772_r_000000_1_-2069147612_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:03:04 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 22:03:14 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 22:03:48 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 22:04:27 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 22:05:06 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 22:05:19 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 22:05:28 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 22:05:34 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 22:12:40 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 22:13:17 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 22:15:52 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:16:58 INFO mapreduce.Job: Job job_1422482982071_4772 completed successfully
15/04/13 22:16:58 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=30424216009
		FILE: Number of bytes written=60862147202
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=432
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Failed reduce tasks=3
		Launched map tasks=134
		Launched reduce tasks=13
		Data-local map tasks=61
		Rack-local map tasks=73
		Total time spent by all maps in occupied slots (ms)=11240682
		Total time spent by all reduces in occupied slots (ms)=15966334
		Total time spent by all map tasks (ms)=5620341
		Total time spent by all reduce tasks (ms)=7983167
		Total vcore-seconds taken by all map tasks=5620341
		Total vcore-seconds taken by all reduce tasks=7983167
		Total megabyte-seconds taken by all map tasks=45502280736
		Total megabyte-seconds taken by all reduce tasks=95798004000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424223833
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424223833
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =1340
		Failed Shuffles=0
		Merged Map outputs=1340
		GC time elapsed (ms)=155977
		CPU time spent (ms)=13424890
		Physical memory (bytes) snapshot=283855036416
		Virtual memory (bytes) snapshot=1364166082560
		Total committed heap usage (bytes)=392672174080
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:16:58 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	42m52.552s
user	0m20.939s
sys	0m2.178s
15/04/13 22:17:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 5
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob9143960490564707520.jar tmpDir=null
15/04/13 22:17:03 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:17:03 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:17:04 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:17:05 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:17:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4791
15/04/13 22:17:05 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4791
15/04/13 22:17:05 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4791/
15/04/13 22:17:05 INFO mapreduce.Job: Running job: job_1422482982071_4791
15/04/13 22:17:10 INFO mapreduce.Job: Job job_1422482982071_4791 running in uber mode : false
15/04/13 22:17:10 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:17:20 INFO mapreduce.Job:  map 7% reduce 0%
15/04/13 22:17:21 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 22:17:22 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:17:23 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 22:17:24 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 22:17:25 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:17:26 INFO mapreduce.Job:  map 34% reduce 0%
15/04/13 22:17:27 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 22:17:28 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 22:17:29 INFO mapreduce.Job:  map 46% reduce 0%
15/04/13 22:17:30 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 22:17:31 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 22:17:32 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 22:17:33 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 22:17:34 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:18:19 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 22:18:20 INFO mapreduce.Job:  map 71% reduce 0%
15/04/13 22:18:21 INFO mapreduce.Job:  map 75% reduce 0%
15/04/13 22:18:22 INFO mapreduce.Job:  map 80% reduce 0%
15/04/13 22:18:23 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 22:18:24 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 22:18:25 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 22:18:26 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 22:18:27 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 22:18:28 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:18:30 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 22:18:32 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 22:18:33 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 22:18:34 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:18:34 INFO mapreduce.Job: Job job_1422482982071_4791 completed successfully
15/04/13 22:18:34 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110563
		FILE: Number of bytes written=153485397
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=417
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=5
		Data-local map tasks=62
		Rack-local map tasks=72
		Total time spent by all maps in occupied slots (ms)=18761396
		Total time spent by all reduces in occupied slots (ms)=107210
		Total time spent by all map tasks (ms)=9380698
		Total time spent by all reduce tasks (ms)=53605
		Total vcore-seconds taken by all map tasks=9380698
		Total vcore-seconds taken by all reduce tasks=53605
		Total megabyte-seconds taken by all map tasks=75946131008
		Total megabyte-seconds taken by all reduce tasks=643260000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70114553
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70114553
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =670
		Failed Shuffles=0
		Merged Map outputs=670
		GC time elapsed (ms)=39262
		CPU time spent (ms)=10069970
		Physical memory (bytes) snapshot=261846212608
		Virtual memory (bytes) snapshot=1296758079488
		Total committed heap usage (bytes)=381648134144
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:18:34 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m35.956s
user	0m11.187s
sys	0m0.606s
15/04/13 22:18:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 5
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob7317473371803842456.jar tmpDir=null
15/04/13 22:18:39 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:18:39 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:18:40 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:18:41 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:18:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4792
15/04/13 22:18:41 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4792
15/04/13 22:18:41 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4792/
15/04/13 22:18:41 INFO mapreduce.Job: Running job: job_1422482982071_4792
15/04/13 22:18:46 INFO mapreduce.Job: Job job_1422482982071_4792 running in uber mode : false
15/04/13 22:18:46 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:18:56 INFO mapreduce.Job:  map 7% reduce 0%
15/04/13 22:18:57 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 22:18:58 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:18:59 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 22:19:00 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 22:19:01 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:19:02 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 22:19:03 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 22:19:04 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 22:19:05 INFO mapreduce.Job:  map 46% reduce 0%
15/04/13 22:19:06 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 22:19:07 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 22:19:08 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 22:19:09 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 22:19:10 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:19:27 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 22:19:28 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 22:19:29 INFO mapreduce.Job:  map 81% reduce 0%
15/04/13 22:19:30 INFO mapreduce.Job:  map 86% reduce 0%
15/04/13 22:19:31 INFO mapreduce.Job:  map 90% reduce 0%
15/04/13 22:19:32 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 22:19:33 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 22:19:34 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 22:19:35 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:19:38 INFO mapreduce.Job:  map 100% reduce 7%
15/04/13 22:20:02 INFO mapreduce.Job:  map 100% reduce 8%
15/04/13 22:20:05 INFO mapreduce.Job:  map 100% reduce 12%
15/04/13 22:20:08 INFO mapreduce.Job:  map 100% reduce 13%
15/04/13 22:20:20 INFO mapreduce.Job:  map 100% reduce 14%
15/04/13 22:20:29 INFO mapreduce.Job:  map 100% reduce 16%
15/04/13 22:20:32 INFO mapreduce.Job:  map 100% reduce 17%
15/04/13 22:20:35 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 22:20:41 INFO mapreduce.Job:  map 100% reduce 19%
15/04/13 22:20:53 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 22:20:57 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 22:21:02 INFO mapreduce.Job:  map 100% reduce 23%
15/04/13 22:21:16 INFO mapreduce.Job:  map 100% reduce 24%
15/04/13 22:21:18 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 22:21:22 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 22:21:30 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 22:21:34 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 22:21:46 INFO mapreduce.Job:  map 100% reduce 30%
15/04/13 22:21:49 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 22:21:52 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 22:21:58 INFO mapreduce.Job:  map 100% reduce 38%
15/04/13 22:22:01 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 22:22:04 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 22:22:10 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 22:22:13 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 22:22:16 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 22:22:19 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 22:22:22 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 22:22:25 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 22:22:28 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 22:22:31 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 22:22:34 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 22:22:37 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:22:43 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:22:52 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:22:55 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 22:22:58 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 22:23:01 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:23:40 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 22:24:26 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 22:24:50 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 22:25:08 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 22:26:02 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:26:38 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:27:23 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:27:48 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 22:28:40 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 22:29:16 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 22:29:46 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:29:52 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:30:25 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 22:30:37 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:31:01 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 22:31:28 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 22:32:02 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 22:32:20 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 22:32:23 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 22:32:44 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 22:33:02 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 22:33:30 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 22:34:18 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 22:34:48 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 22:35:34 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 22:35:58 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 22:36:13 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 22:36:50 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 22:37:44 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 22:38:47 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 22:40:16 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:41:11 INFO mapreduce.Job: Job job_1422482982071_4792 completed successfully
15/04/13 22:41:11 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=30424215991
		FILE: Number of bytes written=60861652141
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=417
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=5
		Data-local map tasks=64
		Rack-local map tasks=70
		Total time spent by all maps in occupied slots (ms)=11109874
		Total time spent by all reduces in occupied slots (ms)=10710520
		Total time spent by all map tasks (ms)=5554937
		Total time spent by all reduce tasks (ms)=5355260
		Total vcore-seconds taken by all map tasks=5554937
		Total vcore-seconds taken by all reduce tasks=5355260
		Total megabyte-seconds taken by all map tasks=44972769952
		Total megabyte-seconds taken by all reduce tasks=64263120000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424219813
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424219813
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =670
		Failed Shuffles=0
		Merged Map outputs=670
		GC time elapsed (ms)=144580
		CPU time spent (ms)=13145430
		Physical memory (bytes) snapshot=273446707200
		Virtual memory (bytes) snapshot=1296758214656
		Total committed heap usage (bytes)=382448029696
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:41:11 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	22m37.246s
user	0m17.342s
sys	0m1.401s
15/04/13 22:41:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 4
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6297200488734794550.jar tmpDir=null
15/04/13 22:41:17 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:41:17 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:41:18 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:41:18 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:41:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4794
15/04/13 22:41:19 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4794
15/04/13 22:41:19 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4794/
15/04/13 22:41:19 INFO mapreduce.Job: Running job: job_1422482982071_4794
15/04/13 22:41:25 INFO mapreduce.Job: Job job_1422482982071_4794 running in uber mode : false
15/04/13 22:41:25 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:41:36 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 22:41:37 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 22:41:38 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:41:39 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 22:41:40 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:41:42 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 22:41:43 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 22:41:45 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 22:41:46 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 22:41:48 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 22:41:49 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 22:41:51 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:42:33 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 22:42:34 INFO mapreduce.Job:  map 70% reduce 0%
15/04/13 22:42:35 INFO mapreduce.Job:  map 74% reduce 0%
15/04/13 22:42:36 INFO mapreduce.Job:  map 83% reduce 0%
15/04/13 22:42:37 INFO mapreduce.Job:  map 91% reduce 0%
15/04/13 22:42:38 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 22:42:39 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 22:42:40 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:42:44 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 22:42:46 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:42:47 INFO mapreduce.Job: Job job_1422482982071_4794 completed successfully
15/04/13 22:42:47 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110557
		FILE: Number of bytes written=153386120
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=414
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=4
		Data-local map tasks=63
		Rack-local map tasks=71
		Total time spent by all maps in occupied slots (ms)=18262300
		Total time spent by all reduces in occupied slots (ms)=66180
		Total time spent by all map tasks (ms)=9131150
		Total time spent by all reduce tasks (ms)=33090
		Total vcore-seconds taken by all map tasks=9131150
		Total vcore-seconds taken by all reduce tasks=33090
		Total megabyte-seconds taken by all map tasks=73925790400
		Total megabyte-seconds taken by all reduce tasks=397080000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70113749
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70113749
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =536
		Failed Shuffles=0
		Merged Map outputs=536
		GC time elapsed (ms)=44555
		CPU time spent (ms)=10003290
		Physical memory (bytes) snapshot=261645123584
		Virtual memory (bytes) snapshot=1284199870464
		Total committed heap usage (bytes)=379554021376
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:42:47 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m35.480s
user	0m10.770s
sys	0m0.697s
15/04/13 22:42:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 4
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6510992660467140792.jar tmpDir=null
15/04/13 22:42:52 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:42:52 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:42:53 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:42:54 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:42:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4797
15/04/13 22:42:54 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4797
15/04/13 22:42:54 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4797/
15/04/13 22:42:54 INFO mapreduce.Job: Running job: job_1422482982071_4797
15/04/13 22:42:59 INFO mapreduce.Job: Job job_1422482982071_4797 running in uber mode : false
15/04/13 22:42:59 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:43:09 INFO mapreduce.Job:  map 8% reduce 0%
15/04/13 22:43:10 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 22:43:11 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:43:12 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 22:43:13 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 22:43:14 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:43:15 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 22:43:16 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 22:43:17 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 22:43:18 INFO mapreduce.Job:  map 45% reduce 0%
15/04/13 22:43:19 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 22:43:20 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 22:43:21 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 22:43:22 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 22:43:23 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:43:39 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 22:43:40 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 22:43:41 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 22:43:42 INFO mapreduce.Job:  map 80% reduce 0%
15/04/13 22:43:43 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 22:43:44 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 22:43:45 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 22:43:46 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 22:43:47 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:43:51 INFO mapreduce.Job:  map 100% reduce 6%
15/04/13 22:44:18 INFO mapreduce.Job:  map 100% reduce 9%
15/04/13 22:44:36 INFO mapreduce.Job:  map 100% reduce 11%
15/04/13 22:44:45 INFO mapreduce.Job:  map 100% reduce 13%
15/04/13 22:45:00 INFO mapreduce.Job:  map 100% reduce 15%
15/04/13 22:45:03 INFO mapreduce.Job:  map 100% reduce 16%
15/04/13 22:45:13 INFO mapreduce.Job:  map 100% reduce 17%
15/04/13 22:45:19 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 22:45:26 INFO mapreduce.Job:  map 100% reduce 19%
15/04/13 22:45:28 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 22:45:31 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 22:45:44 INFO mapreduce.Job:  map 100% reduce 22%
15/04/13 22:45:49 INFO mapreduce.Job:  map 100% reduce 23%
15/04/13 22:45:53 INFO mapreduce.Job:  map 100% reduce 24%
15/04/13 22:45:56 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 22:46:08 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 22:46:11 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 22:46:17 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 22:46:23 INFO mapreduce.Job:  map 100% reduce 30%
15/04/13 22:46:38 INFO mapreduce.Job:  map 100% reduce 32%
15/04/13 22:46:41 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 22:46:44 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 22:46:47 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 22:46:50 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 22:47:14 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 22:47:17 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 22:47:20 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 22:47:23 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 22:47:47 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 22:48:18 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:48:21 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:48:24 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:48:27 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 22:48:30 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 22:48:33 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 22:48:36 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:48:57 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 22:49:18 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 22:49:54 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 22:51:03 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 22:52:53 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:53:35 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:53:41 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:54:08 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 22:54:38 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 22:55:12 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 22:55:48 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:56:18 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:56:42 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:56:54 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 22:57:21 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 22:57:45 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 22:58:12 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 22:58:43 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 22:59:07 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 22:59:34 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:00:13 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 23:00:38 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:01:05 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 23:01:20 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 23:02:02 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 23:03:04 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 23:04:13 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 23:04:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4797_r_000002_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4797_r_000002_0/part-00002 (inode 3637929): File does not exist. Holder DFSClient_attempt_1422482982071_4797_r_000002_0_-1706792605_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 23:04:57 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 23:05:07 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 23:05:32 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 23:06:00 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 23:06:09 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 23:06:30 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 23:06:57 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 23:07:24 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 23:07:48 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 23:07:57 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 23:08:15 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 23:08:18 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 23:08:21 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 23:08:24 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:09:25 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 23:10:17 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:11:44 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 23:14:15 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 23:14:49 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 23:15:33 INFO mapreduce.Job: Task Id : attempt_1422482982071_4797_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077959413_4218754 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/13 23:15:34 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 23:15:44 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 23:16:33 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 23:17:03 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 23:17:33 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 23:18:03 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 23:18:13 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 23:18:36 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 23:19:03 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 23:19:31 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 23:20:01 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 23:20:34 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 23:20:37 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 23:20:40 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 23:20:43 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 23:20:47 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 23:20:50 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:22:09 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 23:23:22 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:25:02 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 23:29:28 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 23:33:02 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 23:35:45 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 23:38:10 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 23:41:00 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 23:43:22 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 23:46:15 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:47:34 INFO mapreduce.Job: Task Id : attempt_1422482982071_4797_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4797_r_000000_1/part-00000 (inode 3638222): File does not exist. Holder DFSClient_attempt_1422482982071_4797_r_000000_1_-577837845_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 23:47:35 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 23:47:46 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 23:48:14 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 23:48:44 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 23:49:14 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 23:49:47 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 23:50:20 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 23:50:50 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 23:51:55 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 23:52:25 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 23:52:28 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 23:52:31 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 23:52:34 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 23:52:37 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:52:40 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:52:43 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 00:01:07 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:04:35 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:07:16 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:09:56 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:12:46 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:15:08 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:18:03 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:19:29 INFO mapreduce.Job: Job job_1422482982071_4797 completed successfully
15/04/14 00:19:29 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=30424215985
		FILE: Number of bytes written=60861553184
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=414
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Failed reduce tasks=3
		Launched map tasks=134
		Launched reduce tasks=7
		Data-local map tasks=64
		Rack-local map tasks=70
		Total time spent by all maps in occupied slots (ms)=11010280
		Total time spent by all reduces in occupied slots (ms)=20847570
		Total time spent by all map tasks (ms)=5505140
		Total time spent by all reduce tasks (ms)=10423785
		Total vcore-seconds taken by all map tasks=5505140
		Total vcore-seconds taken by all reduce tasks=10423785
		Total megabyte-seconds taken by all map tasks=44569613440
		Total megabyte-seconds taken by all reduce tasks=125085420000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424219009
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424219009
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =536
		Failed Shuffles=0
		Merged Map outputs=536
		GC time elapsed (ms)=144510
		CPU time spent (ms)=13050510
		Physical memory (bytes) snapshot=271781089280
		Virtual memory (bytes) snapshot=1283642355712
		Total committed heap usage (bytes)=380900556800
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/14 00:19:29 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	96m41.662s
user	0m29.684s
sys	0m4.097s
15/04/14 00:19:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 35
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1317175755399460496.jar tmpDir=null
15/04/14 00:19:34 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:19:34 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:19:35 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:19:35 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:19:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4811
15/04/14 00:19:36 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4811
15/04/14 00:19:36 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4811/
15/04/14 00:19:36 INFO mapreduce.Job: Running job: job_1422482982071_4811
15/04/14 00:19:40 INFO mapreduce.Job: Job job_1422482982071_4811 running in uber mode : false
15/04/14 00:19:40 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:19:50 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 00:19:51 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:19:52 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 00:19:53 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 00:19:54 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 00:19:55 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:19:56 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 00:19:57 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 00:19:58 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 00:19:59 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 00:20:00 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 00:20:01 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 00:20:02 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 00:20:03 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:20:04 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 00:20:05 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 00:20:06 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:20:08 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:20:12 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:20:53 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 00:20:54 INFO mapreduce.Job:  map 72% reduce 0%
15/04/14 00:20:55 INFO mapreduce.Job:  map 77% reduce 0%
15/04/14 00:20:56 INFO mapreduce.Job:  map 83% reduce 0%
15/04/14 00:20:57 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 00:20:58 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 00:20:59 INFO mapreduce.Job:  map 95% reduce 0%
15/04/14 00:21:00 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 00:21:01 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:21:02 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:21:03 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:21:04 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 00:21:05 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:21:09 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 00:21:10 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:21:11 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:21:14 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:21:15 INFO mapreduce.Job: Job job_1422482982071_4811 completed successfully
15/04/14 00:21:15 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467165
		FILE: Number of bytes written=213727762
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=711
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=35
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=29379586
		Total time spent by all reduces in occupied slots (ms)=985122
		Total time spent by all map tasks (ms)=14689793
		Total time spent by all reduce tasks (ms)=492561
		Total vcore-seconds taken by all map tasks=14689793
		Total vcore-seconds taken by all reduce tasks=492561
		Total megabyte-seconds taken by all map tasks=118928564128
		Total megabyte-seconds taken by all reduce tasks=5910732000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95509375
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95509375
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =7070
		Failed Shuffles=0
		Merged Map outputs=7070
		GC time elapsed (ms)=68709
		CPU time spent (ms)=12202090
		Physical memory (bytes) snapshot=400397230080
		Virtual memory (bytes) snapshot=2322369380352
		Total committed heap usage (bytes)=633133076480
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:21:15 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m46.357s
user	0m11.340s
sys	0m0.625s
15/04/14 00:21:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 35
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob7215469473049239361.jar tmpDir=null
15/04/14 00:21:20 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:21:20 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:21:21 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:21:21 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:21:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4812
15/04/14 00:21:22 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4812
15/04/14 00:21:22 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4812/
15/04/14 00:21:22 INFO mapreduce.Job: Running job: job_1422482982071_4812
15/04/14 00:21:27 INFO mapreduce.Job: Job job_1422482982071_4812 running in uber mode : false
15/04/14 00:21:27 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:21:38 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 00:21:39 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:21:40 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 00:21:41 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 00:21:42 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 00:21:43 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 00:21:44 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 00:21:45 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:21:46 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 00:21:47 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 00:21:48 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:21:49 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:21:50 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 00:21:51 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:21:52 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:21:53 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 00:21:54 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:21:55 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:21:56 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:22:08 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 00:22:09 INFO mapreduce.Job:  map 73% reduce 0%
15/04/14 00:22:10 INFO mapreduce.Job:  map 79% reduce 0%
15/04/14 00:22:11 INFO mapreduce.Job:  map 84% reduce 0%
15/04/14 00:22:12 INFO mapreduce.Job:  map 88% reduce 0%
15/04/14 00:22:13 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 00:22:14 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 00:22:15 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 00:22:16 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:22:17 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:22:19 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 00:22:22 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 00:22:25 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 00:22:28 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:22:31 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 00:22:34 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 00:22:37 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 00:22:40 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 00:22:43 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 00:22:46 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:22:49 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:22:52 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:22:55 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:22:58 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:23:01 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:23:04 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:23:07 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 00:23:10 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 00:23:13 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 00:23:16 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 00:23:19 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 00:23:22 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 00:23:25 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 00:23:28 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 00:23:31 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 00:23:34 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 00:23:37 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 00:23:40 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:23:46 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 00:23:49 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 00:23:55 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 00:23:58 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 00:24:04 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 00:24:07 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 00:24:11 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 00:24:16 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 00:24:22 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 00:24:26 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 00:24:35 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 00:24:41 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 00:24:47 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 00:24:53 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 00:25:02 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 00:25:08 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 00:25:20 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 00:25:41 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 00:25:59 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:26:18 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:27:14 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:27:46 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:28:52 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:29:55 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:31:57 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:37:13 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:38:50 INFO mapreduce.Job: Job job_1422482982071_4812 completed successfully
15/04/14 00:38:50 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322849
		FILE: Number of bytes written=94747363924
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=711
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Killed reduce tasks=3
		Launched map tasks=202
		Launched reduce tasks=38
		Data-local map tasks=150
		Rack-local map tasks=52
		Total time spent by all maps in occupied slots (ms)=16432912
		Total time spent by all reduces in occupied slots (ms)=21027184
		Total time spent by all map tasks (ms)=8216456
		Total time spent by all reduce tasks (ms)=10513592
		Total vcore-seconds taken by all map tasks=8216456
		Total vcore-seconds taken by all reduce tasks=10513592
		Total megabyte-seconds taken by all map tasks=66520427776
		Total megabyte-seconds taken by all reduce tasks=126163104000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362364885
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362364885
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =7070
		Failed Shuffles=0
		Merged Map outputs=7070
		GC time elapsed (ms)=225210
		CPU time spent (ms)=20887010
		Physical memory (bytes) snapshot=446427508736
		Virtual memory (bytes) snapshot=2321568858112
		Total committed heap usage (bytes)=633342668800
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:38:50 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	17m34.789s
user	0m16.244s
sys	0m1.170s
15/04/14 00:38:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 25
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob4445229267005621768.jar tmpDir=null
15/04/14 00:38:55 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:38:56 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:38:56 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:38:57 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:38:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4818
15/04/14 00:38:58 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4818
15/04/14 00:38:58 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4818/
15/04/14 00:38:58 INFO mapreduce.Job: Running job: job_1422482982071_4818
15/04/14 00:39:02 INFO mapreduce.Job: Job job_1422482982071_4818 running in uber mode : false
15/04/14 00:39:02 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:39:12 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 00:39:13 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:39:14 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 00:39:15 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 00:39:16 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 00:39:17 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:39:18 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 00:39:19 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:39:20 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 00:39:21 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 00:39:22 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:39:23 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:39:24 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 00:39:25 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:39:26 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:39:27 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 00:39:28 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:39:29 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:39:30 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:40:14 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 00:40:15 INFO mapreduce.Job:  map 73% reduce 0%
15/04/14 00:40:16 INFO mapreduce.Job:  map 80% reduce 0%
15/04/14 00:40:17 INFO mapreduce.Job:  map 85% reduce 0%
15/04/14 00:40:18 INFO mapreduce.Job:  map 90% reduce 0%
15/04/14 00:40:19 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 00:40:20 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 00:40:21 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:40:22 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:40:25 INFO mapreduce.Job:  map 99% reduce 32%
15/04/14 00:40:26 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 00:40:28 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:40:29 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:40:30 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:40:31 INFO mapreduce.Job: Job job_1422482982071_4818 completed successfully
15/04/14 00:40:32 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467105
		FILE: Number of bytes written=212714562
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=681
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=25
		Data-local map tasks=146
		Rack-local map tasks=56
		Total time spent by all maps in occupied slots (ms)=29204688
		Total time spent by all reduces in occupied slots (ms)=602124
		Total time spent by all map tasks (ms)=14602344
		Total time spent by all reduce tasks (ms)=301062
		Total vcore-seconds taken by all map tasks=14602344
		Total vcore-seconds taken by all reduce tasks=301062
		Total megabyte-seconds taken by all map tasks=118220577024
		Total megabyte-seconds taken by all reduce tasks=3612744000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95497255
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95497255
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =5050
		Failed Shuffles=0
		Merged Map outputs=5050
		GC time elapsed (ms)=63717
		CPU time spent (ms)=12443530
		Physical memory (bytes) snapshot=398347698176
		Virtual memory (bytes) snapshot=2188503855104
		Total committed heap usage (bytes)=612081012736
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:40:32 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m41.743s
user	0m11.740s
sys	0m0.732s
15/04/14 00:40:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 25
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob4343229819193604773.jar tmpDir=null
15/04/14 00:40:37 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:40:37 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:40:38 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:40:38 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:40:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4819
15/04/14 00:40:39 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4819
15/04/14 00:40:39 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4819/
15/04/14 00:40:39 INFO mapreduce.Job: Running job: job_1422482982071_4819
15/04/14 00:40:43 INFO mapreduce.Job: Job job_1422482982071_4819 running in uber mode : false
15/04/14 00:40:43 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:40:53 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 00:40:54 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:40:55 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 00:40:56 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 00:40:58 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 00:40:59 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:41:00 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 00:41:01 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:41:02 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 00:41:03 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 00:41:04 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:41:05 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:41:06 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 00:41:07 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:41:08 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:41:09 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 00:41:10 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:41:11 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:41:12 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:41:25 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 00:41:26 INFO mapreduce.Job:  map 75% reduce 0%
15/04/14 00:41:27 INFO mapreduce.Job:  map 81% reduce 0%
15/04/14 00:41:28 INFO mapreduce.Job:  map 86% reduce 0%
15/04/14 00:41:29 INFO mapreduce.Job:  map 90% reduce 0%
15/04/14 00:41:30 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 00:41:31 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 00:41:32 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:41:33 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:41:36 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 00:41:37 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 00:41:42 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 00:41:57 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 00:42:00 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 00:42:01 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 00:42:03 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:42:06 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 00:42:07 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 00:42:09 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 00:42:10 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 00:42:12 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 00:42:13 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 00:42:15 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 00:42:18 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 00:42:21 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 00:42:24 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 00:42:25 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:42:27 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:42:30 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:42:33 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:42:36 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:42:40 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:42:43 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:42:49 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:42:51 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 00:42:52 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 00:42:55 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 00:43:00 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 00:43:03 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 00:43:06 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 00:43:09 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 00:43:15 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 00:43:21 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 00:43:24 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 00:43:31 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:43:37 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 00:43:47 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 00:43:56 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 00:44:02 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 00:44:08 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 00:44:17 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 00:44:22 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 00:44:26 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 00:44:31 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 00:44:34 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 00:44:44 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 00:44:59 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 00:45:11 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 00:45:23 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 00:45:35 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 00:45:50 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 00:46:05 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 00:46:30 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 00:46:54 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 00:47:15 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:47:55 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:47:58 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:49:13 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:49:21 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:49:53 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:51:35 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:56:07 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:57:01 INFO mapreduce.Job: Task Id : attempt_1422482982071_4819_r_000005_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4819_r_000005_0/part-00005 (inode 3640038): File does not exist. Holder DFSClient_attempt_1422482982071_4819_r_000005_0_-1072096001_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4819_r_000005_0/part-00005 (inode 3640038): File does not exist. Holder DFSClient_attempt_1422482982071_4819_r_000005_0_-1072096001_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:57:02 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:59:03 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:59:54 INFO mapreduce.Job: Task Id : attempt_1422482982071_4819_r_000005_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4819_r_000005_1/part-00005 (inode 3640040): File does not exist. Holder DFSClient_attempt_1422482982071_4819_r_000005_1_1359206376_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4819_r_000005_1/part-00005 (inode 3640040): File does not exist. Holder DFSClient_attempt_1422482982071_4819_r_000005_1_1359206376_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:59:55 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 01:10:59 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 01:13:44 INFO mapreduce.Job: Job job_1422482982071_4819 completed successfully
15/04/14 01:13:44 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=47362322843
		FILE: Number of bytes written=94746353978
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=681
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Failed reduce tasks=2
		Killed reduce tasks=1
		Launched map tasks=202
		Launched reduce tasks=28
		Data-local map tasks=149
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=16549324
		Total time spent by all reduces in occupied slots (ms)=22011238
		Total time spent by all map tasks (ms)=8274662
		Total time spent by all reduce tasks (ms)=11005619
		Total vcore-seconds taken by all map tasks=8274662
		Total vcore-seconds taken by all reduce tasks=11005619
		Total megabyte-seconds taken by all map tasks=66991663552
		Total megabyte-seconds taken by all reduce tasks=132067428000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362352765
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362352765
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =5050
		Failed Shuffles=0
		Merged Map outputs=5050
		GC time elapsed (ms)=228020
		CPU time spent (ms)=20804570
		Physical memory (bytes) snapshot=442880589824
		Virtual memory (bytes) snapshot=2188511780864
		Total committed heap usage (bytes)=612433399808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 01:13:44 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	33m12.209s
user	0m20.847s
sys	0m1.947s
15/04/14 01:13:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 15
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8256339246078126726.jar tmpDir=null
15/04/14 01:13:49 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:13:49 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:13:50 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 01:13:50 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 01:13:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4823
15/04/14 01:13:51 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4823
15/04/14 01:13:51 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4823/
15/04/14 01:13:51 INFO mapreduce.Job: Running job: job_1422482982071_4823
15/04/14 01:13:57 INFO mapreduce.Job: Job job_1422482982071_4823 running in uber mode : false
15/04/14 01:13:57 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 01:14:08 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 01:14:09 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 01:14:10 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 01:14:11 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 01:14:12 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 01:14:13 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 01:14:14 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 01:14:15 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 01:14:16 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 01:14:17 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 01:14:18 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 01:14:19 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 01:14:20 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 01:14:21 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 01:14:22 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 01:14:23 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 01:14:24 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 01:14:25 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 01:15:10 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 01:15:11 INFO mapreduce.Job:  map 74% reduce 0%
15/04/14 01:15:12 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 01:15:13 INFO mapreduce.Job:  map 88% reduce 0%
15/04/14 01:15:14 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 01:15:15 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 01:15:16 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 01:15:17 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 01:15:20 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 01:15:21 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 01:15:22 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:15:23 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 01:15:23 INFO mapreduce.Job: Job job_1422482982071_4823 completed successfully
15/04/14 01:15:23 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467045
		FILE: Number of bytes written=211701362
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=651
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=15
		Data-local map tasks=150
		Rack-local map tasks=52
		Total time spent by all maps in occupied slots (ms)=29091326
		Total time spent by all reduces in occupied slots (ms)=278504
		Total time spent by all map tasks (ms)=14545663
		Total time spent by all reduce tasks (ms)=139252
		Total vcore-seconds taken by all map tasks=14545663
		Total vcore-seconds taken by all reduce tasks=139252
		Total megabyte-seconds taken by all map tasks=117761687648
		Total megabyte-seconds taken by all reduce tasks=1671024000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95485135
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95485135
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =3030
		Failed Shuffles=0
		Merged Map outputs=3030
		GC time elapsed (ms)=68747
		CPU time spent (ms)=13657460
		Physical memory (bytes) snapshot=396201730048
		Virtual memory (bytes) snapshot=2055470198784
		Total committed heap usage (bytes)=591042506752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 01:15:23 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m39.322s
user	0m11.326s
sys	0m0.735s
15/04/14 01:15:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 15
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6816772850899410468.jar tmpDir=null
15/04/14 01:15:28 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:15:29 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:15:29 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 01:15:30 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 01:15:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4824
15/04/14 01:15:30 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4824
15/04/14 01:15:30 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4824/
15/04/14 01:15:30 INFO mapreduce.Job: Running job: job_1422482982071_4824
15/04/14 01:15:37 INFO mapreduce.Job: Job job_1422482982071_4824 running in uber mode : false
15/04/14 01:15:37 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 01:15:48 INFO mapreduce.Job:  map 6% reduce 0%
15/04/14 01:15:49 INFO mapreduce.Job:  map 13% reduce 0%
15/04/14 01:15:50 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 01:15:51 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 01:15:52 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 01:15:53 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 01:15:54 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 01:15:55 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 01:15:56 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 01:15:57 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 01:15:58 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 01:15:59 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 01:16:00 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 01:16:01 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 01:16:02 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 01:16:03 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 01:16:04 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 01:16:06 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 01:16:19 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 01:16:20 INFO mapreduce.Job:  map 80% reduce 0%
15/04/14 01:16:21 INFO mapreduce.Job:  map 85% reduce 0%
15/04/14 01:16:22 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 01:16:23 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 01:16:24 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 01:16:25 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 01:16:30 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 01:16:31 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 01:16:33 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 01:16:54 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 01:16:56 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 01:16:57 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 01:16:59 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 01:17:00 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 01:17:02 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 01:17:06 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 01:17:09 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 01:17:13 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 01:17:16 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 01:17:21 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 01:17:24 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 01:17:25 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 01:17:27 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 01:17:28 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 01:17:30 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 01:17:31 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 01:17:32 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 01:17:33 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 01:17:34 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 01:17:36 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 01:17:37 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 01:17:44 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 01:17:45 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 01:17:52 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 01:17:55 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 01:18:01 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 01:18:10 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 01:18:20 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 01:18:23 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 01:18:27 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 01:18:31 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 01:18:34 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 01:18:36 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 01:18:39 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 01:18:45 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 01:18:50 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 01:18:53 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 01:18:54 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 01:18:56 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 01:19:00 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 01:19:06 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 01:19:13 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 01:19:21 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 01:19:26 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 01:19:34 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 01:19:47 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 01:20:01 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 01:20:09 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 01:20:24 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 01:20:34 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 01:20:45 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 01:20:53 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 01:21:03 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 01:21:20 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 01:21:36 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 01:21:51 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 01:22:18 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 01:22:39 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 01:22:45 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:23:00 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:23:30 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:24:20 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:24:54 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:25:34 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:27:47 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:28:53 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 01:28:56 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:30:05 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:30:57 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 01:31:42 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 01:32:35 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 01:34:37 INFO mapreduce.Job: Job job_1422482982071_4824 completed successfully
15/04/14 01:34:37 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=47362322801
		FILE: Number of bytes written=94745343996
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=651
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=15
		Data-local map tasks=155
		Rack-local map tasks=47
		Total time spent by all maps in occupied slots (ms)=16548064
		Total time spent by all reduces in occupied slots (ms)=17371980
		Total time spent by all map tasks (ms)=8274032
		Total time spent by all reduce tasks (ms)=8685990
		Total vcore-seconds taken by all map tasks=8274032
		Total vcore-seconds taken by all reduce tasks=8685990
		Total megabyte-seconds taken by all map tasks=66986563072
		Total megabyte-seconds taken by all reduce tasks=104231880000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362340645
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362340645
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =3030
		Failed Shuffles=0
		Merged Map outputs=3030
		GC time elapsed (ms)=256033
		CPU time spent (ms)=20855280
		Physical memory (bytes) snapshot=428842811392
		Virtual memory (bytes) snapshot=2054649393152
		Total committed heap usage (bytes)=592264536064
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 01:34:37 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	19m13.568s
user	0m17.177s
sys	0m1.397s
15/04/14 01:34:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 10
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3022557699052168627.jar tmpDir=null
15/04/14 01:34:41 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:34:42 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:34:42 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 01:34:43 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 01:34:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4826
15/04/14 01:34:43 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4826
15/04/14 01:34:43 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4826/
15/04/14 01:34:43 INFO mapreduce.Job: Running job: job_1422482982071_4826
15/04/14 01:34:48 INFO mapreduce.Job: Job job_1422482982071_4826 running in uber mode : false
15/04/14 01:34:48 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 01:34:58 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 01:34:59 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 01:35:00 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 01:35:01 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 01:35:02 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 01:35:03 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 01:35:04 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 01:35:05 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 01:35:06 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 01:35:07 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 01:35:08 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 01:35:09 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 01:35:10 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 01:35:11 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 01:35:12 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 01:35:13 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 01:35:14 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 01:35:15 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 01:35:16 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 01:36:00 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 01:36:01 INFO mapreduce.Job:  map 72% reduce 0%
15/04/14 01:36:02 INFO mapreduce.Job:  map 77% reduce 0%
15/04/14 01:36:03 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 01:36:04 INFO mapreduce.Job:  map 87% reduce 0%
15/04/14 01:36:05 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 01:36:06 INFO mapreduce.Job:  map 95% reduce 0%
15/04/14 01:36:07 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 01:36:08 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 01:36:09 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 01:36:11 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 01:36:12 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 01:36:13 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 01:36:14 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 01:36:15 INFO mapreduce.Job: Job job_1422482982071_4826 completed successfully
15/04/14 01:36:15 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467015
		FILE: Number of bytes written=211194762
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=636
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=10
		Data-local map tasks=147
		Rack-local map tasks=55
		Total time spent by all maps in occupied slots (ms)=29367538
		Total time spent by all reduces in occupied slots (ms)=212212
		Total time spent by all map tasks (ms)=14683769
		Total time spent by all reduce tasks (ms)=106106
		Total vcore-seconds taken by all map tasks=14683769
		Total vcore-seconds taken by all reduce tasks=106106
		Total megabyte-seconds taken by all map tasks=118879793824
		Total megabyte-seconds taken by all reduce tasks=1273272000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95479075
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95479075
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =2020
		Failed Shuffles=0
		Merged Map outputs=2020
		GC time elapsed (ms)=65780
		CPU time spent (ms)=14626030
		Physical memory (bytes) snapshot=395354157056
		Virtual memory (bytes) snapshot=1988058038272
		Total committed heap usage (bytes)=580510064640
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 01:36:15 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m38.688s
user	0m10.429s
sys	0m0.663s
15/04/14 01:36:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 10
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3981435171840396064.jar tmpDir=null
15/04/14 01:36:20 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:36:20 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 01:36:21 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 01:36:21 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 01:36:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4827
15/04/14 01:36:22 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4827
15/04/14 01:36:22 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4827/
15/04/14 01:36:22 INFO mapreduce.Job: Running job: job_1422482982071_4827
15/04/14 01:36:28 INFO mapreduce.Job: Job job_1422482982071_4827 running in uber mode : false
15/04/14 01:36:28 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 01:36:38 INFO mapreduce.Job:  map 1% reduce 0%
15/04/14 01:36:39 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 01:36:40 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 01:36:41 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 01:36:42 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 01:36:43 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 01:36:44 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 01:36:45 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 01:36:46 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 01:36:47 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 01:36:48 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 01:36:49 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 01:36:50 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 01:36:51 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 01:36:52 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 01:36:53 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 01:36:54 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 01:36:55 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 01:36:56 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 01:37:10 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 01:37:11 INFO mapreduce.Job:  map 77% reduce 0%
15/04/14 01:37:12 INFO mapreduce.Job:  map 83% reduce 0%
15/04/14 01:37:13 INFO mapreduce.Job:  map 88% reduce 0%
15/04/14 01:37:14 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 01:37:15 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 01:37:16 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 01:37:17 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 01:37:20 INFO mapreduce.Job:  map 100% reduce 1%
15/04/14 01:37:21 INFO mapreduce.Job:  map 100% reduce 7%
15/04/14 01:37:22 INFO mapreduce.Job:  map 100% reduce 8%
15/04/14 01:37:23 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 01:37:45 INFO mapreduce.Job:  map 100% reduce 10%
15/04/14 01:37:46 INFO mapreduce.Job:  map 100% reduce 11%
15/04/14 01:37:48 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 01:37:49 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 01:37:51 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 01:38:10 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 01:38:13 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 01:38:15 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 01:38:16 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 01:38:19 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 01:38:22 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 01:38:25 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 01:38:38 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 01:38:41 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 01:38:44 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 01:38:47 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 01:38:50 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 01:38:53 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 01:38:56 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 01:38:59 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 01:39:08 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 01:39:09 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 01:39:11 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 01:39:14 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 01:39:17 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 01:39:20 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 01:39:21 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 01:39:23 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 01:39:24 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 01:39:26 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 01:39:29 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 01:39:32 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 01:39:35 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 01:39:41 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 01:39:47 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 01:40:08 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 01:40:14 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 01:40:18 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 01:40:23 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 01:40:29 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 01:40:36 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 01:40:54 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 01:41:09 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 01:41:20 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 01:41:42 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 01:41:45 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 01:41:46 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 01:42:09 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 01:42:25 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 01:42:47 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 01:43:13 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 01:43:31 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 01:43:56 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 01:44:19 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 01:44:47 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 01:45:11 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 01:45:35 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 01:45:57 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 01:46:23 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 01:46:56 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 01:47:05 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 01:47:20 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:48:17 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:48:42 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:48:52 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:49:09 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:49:49 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:50:45 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:51:40 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:53:02 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:55:42 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 01:56:55 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 02:03:52 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 02:04:37 INFO mapreduce.Job: Job job_1422482982071_4827 completed successfully
15/04/14 02:04:37 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=47362322777
		FILE: Number of bytes written=94744839002
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=636
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=10
		Data-local map tasks=150
		Rack-local map tasks=52
		Total time spent by all maps in occupied slots (ms)=16829156
		Total time spent by all reduces in occupied slots (ms)=16992316
		Total time spent by all map tasks (ms)=8414578
		Total time spent by all reduce tasks (ms)=8496158
		Total vcore-seconds taken by all map tasks=8414578
		Total vcore-seconds taken by all reduce tasks=8496158
		Total megabyte-seconds taken by all map tasks=68124423488
		Total megabyte-seconds taken by all reduce tasks=101953896000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362334585
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362334585
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =2020
		Failed Shuffles=0
		Merged Map outputs=2020
		GC time elapsed (ms)=229845
		CPU time spent (ms)=20765250
		Physical memory (bytes) snapshot=417751769088
		Virtual memory (bytes) snapshot=1988024889344
		Total committed heap usage (bytes)=581014147072
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 02:04:37 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	28m21.398s
user	0m18.560s
sys	0m1.622s
15/04/14 02:04:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 5
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1535496744724266450.jar tmpDir=null
15/04/14 02:04:42 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:04:42 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:04:43 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 02:04:43 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 02:04:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4830
15/04/14 02:04:44 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4830
15/04/14 02:04:44 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4830/
15/04/14 02:04:44 INFO mapreduce.Job: Running job: job_1422482982071_4830
15/04/14 02:04:48 INFO mapreduce.Job: Job job_1422482982071_4830 running in uber mode : false
15/04/14 02:04:48 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 02:04:58 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 02:04:59 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 02:05:00 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 02:05:01 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 02:05:02 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 02:05:03 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 02:05:04 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 02:05:05 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 02:05:06 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 02:05:07 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 02:05:08 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 02:05:09 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 02:05:10 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 02:05:11 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 02:05:12 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 02:05:13 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 02:05:14 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 02:05:15 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 02:05:17 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 02:06:03 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 02:06:04 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 02:06:05 INFO mapreduce.Job:  map 76% reduce 0%
15/04/14 02:06:06 INFO mapreduce.Job:  map 81% reduce 0%
15/04/14 02:06:07 INFO mapreduce.Job:  map 88% reduce 0%
15/04/14 02:06:08 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 02:06:09 INFO mapreduce.Job:  map 95% reduce 0%
15/04/14 02:06:10 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 02:06:11 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 02:06:14 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:06:16 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 02:06:16 INFO mapreduce.Job: Job job_1422482982071_4830 completed successfully
15/04/14 02:06:16 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95466985
		FILE: Number of bytes written=210687753
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=621
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=5
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=30173882
		Total time spent by all reduces in occupied slots (ms)=101720
		Total time spent by all map tasks (ms)=15086941
		Total time spent by all reduce tasks (ms)=50860
		Total vcore-seconds taken by all map tasks=15086941
		Total vcore-seconds taken by all reduce tasks=50860
		Total megabyte-seconds taken by all map tasks=122143874336
		Total megabyte-seconds taken by all reduce tasks=610320000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95473015
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95473015
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =1010
		Failed Shuffles=0
		Merged Map outputs=1010
		GC time elapsed (ms)=62609
		CPU time spent (ms)=16494020
		Physical memory (bytes) snapshot=394089172992
		Virtual memory (bytes) snapshot=1921256755200
		Total committed heap usage (bytes)=569980215296
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 02:06:16 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m39.255s
user	0m10.995s
sys	0m0.627s
15/04/14 02:06:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 5
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob5911788881972061806.jar tmpDir=null
15/04/14 02:06:22 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:06:22 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:06:23 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 02:06:23 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 02:06:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4831
15/04/14 02:06:24 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4831
15/04/14 02:06:24 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4831/
15/04/14 02:06:24 INFO mapreduce.Job: Running job: job_1422482982071_4831
15/04/14 02:06:28 INFO mapreduce.Job: Job job_1422482982071_4831 running in uber mode : false
15/04/14 02:06:28 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 02:06:38 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 02:06:39 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 02:06:40 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 02:06:41 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 02:06:42 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 02:06:43 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 02:06:44 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 02:06:45 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 02:06:46 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 02:06:47 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 02:06:48 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 02:06:49 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 02:06:50 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 02:06:51 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 02:06:52 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 02:06:53 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 02:06:54 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 02:06:55 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 02:06:56 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 02:07:11 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 02:07:12 INFO mapreduce.Job:  map 70% reduce 0%
15/04/14 02:07:13 INFO mapreduce.Job:  map 77% reduce 0%
15/04/14 02:07:14 INFO mapreduce.Job:  map 83% reduce 0%
15/04/14 02:07:15 INFO mapreduce.Job:  map 87% reduce 0%
15/04/14 02:07:16 INFO mapreduce.Job:  map 91% reduce 0%
15/04/14 02:07:17 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 02:07:18 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 02:07:19 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 02:07:20 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 02:07:22 INFO mapreduce.Job:  map 100% reduce 4%
15/04/14 02:07:25 INFO mapreduce.Job:  map 100% reduce 5%
15/04/14 02:07:50 INFO mapreduce.Job:  map 100% reduce 7%
15/04/14 02:07:53 INFO mapreduce.Job:  map 100% reduce 8%
15/04/14 02:08:08 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 02:08:16 INFO mapreduce.Job:  map 100% reduce 10%
15/04/14 02:08:17 INFO mapreduce.Job:  map 100% reduce 11%
15/04/14 02:08:20 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 02:08:39 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 02:08:43 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 02:08:50 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 02:09:06 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 02:09:10 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 02:09:21 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 02:09:33 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 02:09:37 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 02:09:39 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 02:09:48 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 02:10:00 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 02:10:08 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 02:10:11 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 02:10:14 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 02:10:17 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 02:10:18 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 02:10:20 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 02:10:24 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 02:10:26 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 02:10:29 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 02:10:32 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 02:10:52 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 02:11:19 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 02:11:22 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 02:11:25 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 02:11:28 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 02:11:31 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 02:11:34 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 02:11:35 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 02:11:38 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 02:11:44 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 02:11:46 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 02:11:47 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 02:11:50 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 02:11:53 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 02:11:56 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 02:12:28 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 02:12:31 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 02:12:34 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:12:40 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:13:07 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:13:10 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:13:13 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:13:16 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:13:19 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 02:13:22 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:13:55 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:14:45 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 02:15:39 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 02:16:20 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 02:17:21 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 02:18:37 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 02:19:39 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 02:21:29 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 02:22:19 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000002_0/part-00002 (inode 3641207): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000002_0_-767238063_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000002_0/part-00002 (inode 3641207): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000002_0_-767238063_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 02:22:20 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:22:30 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:22:57 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:23:19 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:23:27 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 02:24:01 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:24:41 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:24:46 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:24:46 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000001_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000001_0/part-00001 (inode 3641203): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000001_0_782051673_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000001_0/part-00001 (inode 3641203): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000001_0_782051673_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 02:24:47 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 02:24:48 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 02:24:57 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 02:25:14 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 02:25:29 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 02:25:56 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 02:26:01 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:26:26 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:26:35 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:26:56 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:27:04 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:27:27 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:27:40 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000004_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000004_0/part-00004 (inode 3641199): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000004_0_565865113_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 02:27:41 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 02:27:52 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 02:27:53 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 02:27:57 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 02:28:00 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 02:28:03 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 02:28:06 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 02:28:09 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 02:28:15 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 02:28:17 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 02:28:23 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 02:28:45 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 02:28:53 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 02:29:03 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 02:29:10 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:29:13 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:29:16 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:29:19 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:29:22 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:29:25 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:29:28 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 02:29:40 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:30:05 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:30:09 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:30:32 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 02:30:38 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 02:30:38 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000000_0/part-00000 (inode 3641205): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000000_0_2090688898_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000000_0/part-00000 (inode 3641205): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000000_0_2090688898_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 02:30:39 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 02:30:41 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 02:30:44 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 02:30:47 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 02:30:50 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 02:30:53 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 02:30:56 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:31:00 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:31:26 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:31:50 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:32:15 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:32:36 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:33:00 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 02:33:31 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:34:01 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:34:20 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:34:40 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 02:34:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000003_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000003_0/part-00003 (inode 3641201): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000003_0_-715807950_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 02:34:44 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 02:34:55 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 02:35:22 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 02:35:43 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 02:35:46 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 02:35:49 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 02:35:52 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:36:22 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:36:49 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:37:17 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:37:48 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:38:06 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:38:21 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 02:38:45 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:39:15 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:39:18 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:39:21 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 02:39:24 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 02:39:27 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 02:39:30 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 02:40:19 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 02:40:43 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 02:41:06 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 02:42:29 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 02:43:47 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 02:44:18 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 02:44:57 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 02:45:45 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 02:46:21 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 02:47:04 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 02:49:07 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 02:49:22 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 02:49:29 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 02:49:49 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 02:50:35 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 02:51:18 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 02:52:09 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 02:53:51 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 02:55:12 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 02:56:15 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 02:57:59 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 02:58:52 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 02:59:18 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 03:00:25 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 03:01:54 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 03:02:36 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000003_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000003_1/part-00003 (inode 3641523): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000003_1_-553584955_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 03:02:37 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 03:02:40 INFO mapreduce.Job: Task Id : attempt_1422482982071_4831_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4831_r_000000_1/part-00000 (inode 3641521): File does not exist. Holder DFSClient_attempt_1422482982071_4831_r_000000_1_-1321621856_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 03:02:41 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 03:02:48 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 03:02:52 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 03:03:15 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 03:03:45 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 03:04:02 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 03:04:19 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 03:04:47 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 03:05:16 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 03:05:47 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 03:05:51 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 03:06:24 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 03:06:44 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 03:07:01 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 03:07:18 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 03:07:21 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 03:07:24 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 03:07:27 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 03:07:30 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 03:07:33 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 03:08:13 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 03:08:16 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 03:10:35 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 03:14:30 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 03:18:05 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 03:20:38 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 03:22:16 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 03:23:01 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 03:24:57 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 03:26:23 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 03:27:47 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 03:29:23 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 03:30:44 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 03:33:33 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 03:35:06 INFO mapreduce.Job: Job job_1422482982071_4831 completed successfully
15/04/14 03:35:06 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322759
		FILE: Number of bytes written=94744333605
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=621
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Failed reduce tasks=7
		Launched map tasks=202
		Launched reduce tasks=12
		Data-local map tasks=150
		Rack-local map tasks=52
		Total time spent by all maps in occupied slots (ms)=17641736
		Total time spent by all reduces in occupied slots (ms)=37092554
		Total time spent by all map tasks (ms)=8820868
		Total time spent by all reduce tasks (ms)=18546277
		Total vcore-seconds taken by all map tasks=8820868
		Total vcore-seconds taken by all reduce tasks=18546277
		Total megabyte-seconds taken by all map tasks=71413747328
		Total megabyte-seconds taken by all reduce tasks=222555324000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362328525
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362328525
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =1010
		Failed Shuffles=0
		Merged Map outputs=1010
		GC time elapsed (ms)=247625
		CPU time spent (ms)=21357490
		Physical memory (bytes) snapshot=407436685312
		Virtual memory (bytes) snapshot=1921431142400
		Total committed heap usage (bytes)=572378361856
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 03:35:06 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	88m49.579s
user	0m30.009s
sys	0m4.133s
15/04/14 03:35:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 4
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1617332078421860039.jar tmpDir=null
15/04/14 03:35:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:35:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:35:12 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 03:35:12 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 03:35:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4840
15/04/14 03:35:13 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4840
15/04/14 03:35:13 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4840/
15/04/14 03:35:13 INFO mapreduce.Job: Running job: job_1422482982071_4840
15/04/14 03:35:17 INFO mapreduce.Job: Job job_1422482982071_4840 running in uber mode : false
15/04/14 03:35:17 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 03:35:27 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 03:35:28 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 03:35:29 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 03:35:30 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 03:35:31 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 03:35:32 INFO mapreduce.Job:  map 26% reduce 0%
15/04/14 03:35:33 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 03:35:34 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 03:35:35 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 03:35:36 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 03:35:37 INFO mapreduce.Job:  map 45% reduce 0%
15/04/14 03:35:38 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 03:35:39 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 03:35:40 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 03:35:41 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 03:35:42 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 03:35:43 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 03:35:44 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 03:35:45 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 03:35:53 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 03:36:33 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 03:36:34 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 03:36:35 INFO mapreduce.Job:  map 76% reduce 0%
15/04/14 03:36:36 INFO mapreduce.Job:  map 80% reduce 0%
15/04/14 03:36:37 INFO mapreduce.Job:  map 87% reduce 0%
15/04/14 03:36:38 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 03:36:39 INFO mapreduce.Job:  map 95% reduce 0%
15/04/14 03:36:40 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 03:36:41 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 03:36:44 INFO mapreduce.Job:  map 99% reduce 33%
15/04/14 03:36:46 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 03:36:56 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 03:36:57 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 03:36:57 INFO mapreduce.Job: Job job_1422482982071_4840 completed successfully
15/04/14 03:36:57 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95466979
		FILE: Number of bytes written=210586436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=618
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=4
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=30627058
		Total time spent by all reduces in occupied slots (ms)=168336
		Total time spent by all map tasks (ms)=15313529
		Total time spent by all reduce tasks (ms)=84168
		Total vcore-seconds taken by all map tasks=15313529
		Total vcore-seconds taken by all reduce tasks=84168
		Total megabyte-seconds taken by all map tasks=123978330784
		Total megabyte-seconds taken by all reduce tasks=1010016000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95471803
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95471803
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =808
		Failed Shuffles=0
		Merged Map outputs=808
		GC time elapsed (ms)=64753
		CPU time spent (ms)=16876040
		Physical memory (bytes) snapshot=393838510080
		Virtual memory (bytes) snapshot=1908321488896
		Total committed heap usage (bytes)=567876190208
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 03:36:57 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m51.338s
user	0m11.666s
sys	0m0.699s
15/04/14 03:36:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 4
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8273750740151806289.jar tmpDir=null
15/04/14 03:37:02 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:37:02 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:37:03 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 03:37:03 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 03:37:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4841
15/04/14 03:37:04 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4841
15/04/14 03:37:04 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4841/
15/04/14 03:37:04 INFO mapreduce.Job: Running job: job_1422482982071_4841
15/04/14 03:37:08 INFO mapreduce.Job: Job job_1422482982071_4841 running in uber mode : false
15/04/14 03:37:08 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 03:37:18 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 03:37:19 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 03:37:20 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 03:37:21 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 03:37:22 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 03:37:23 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 03:37:24 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 03:37:25 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 03:37:26 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 03:37:27 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 03:37:28 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 03:37:29 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 03:37:30 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 03:37:31 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 03:37:32 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 03:37:33 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 03:37:34 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 03:37:35 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 03:37:37 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 03:37:52 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 03:37:53 INFO mapreduce.Job:  map 75% reduce 0%
15/04/14 03:37:54 INFO mapreduce.Job:  map 80% reduce 0%
15/04/14 03:37:55 INFO mapreduce.Job:  map 85% reduce 0%
15/04/14 03:37:56 INFO mapreduce.Job:  map 90% reduce 0%
15/04/14 03:37:57 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 03:37:58 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 03:37:59 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 03:38:03 INFO mapreduce.Job:  map 99% reduce 4%
15/04/14 03:38:04 INFO mapreduce.Job:  map 100% reduce 4%
15/04/14 03:38:31 INFO mapreduce.Job:  map 100% reduce 5%
15/04/14 03:38:34 INFO mapreduce.Job:  map 100% reduce 7%
15/04/14 03:38:58 INFO mapreduce.Job:  map 100% reduce 8%
15/04/14 03:39:01 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 03:39:05 INFO mapreduce.Job:  map 100% reduce 10%
15/04/14 03:39:26 INFO mapreduce.Job:  map 100% reduce 11%
15/04/14 03:39:29 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 03:39:32 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 03:39:50 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 03:39:56 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 03:40:02 INFO mapreduce.Job:  map 100% reduce 16%
15/04/14 03:40:17 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 03:40:23 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 03:40:29 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 03:40:41 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 03:40:50 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 03:40:56 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 03:41:05 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 03:41:15 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 03:41:27 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 03:41:33 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 03:41:36 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 03:41:39 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 03:41:42 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 03:41:54 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 03:41:57 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 03:42:09 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 03:42:36 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 03:42:51 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 03:42:54 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 03:43:03 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 03:43:06 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 03:43:21 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 03:43:51 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 03:44:21 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 03:44:49 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 03:45:16 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 03:45:47 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 03:46:14 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 03:46:20 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 03:46:23 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 03:46:26 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 03:46:29 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 03:46:34 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 03:46:35 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 03:46:38 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 03:46:55 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 03:48:00 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 03:50:18 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 03:52:17 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 03:52:54 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 03:53:38 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 03:53:45 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 03:54:53 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 03:55:35 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 03:56:27 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 03:57:13 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 03:57:59 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 03:58:44 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 03:59:27 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 03:59:58 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 04:00:44 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 04:01:36 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 04:03:24 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 04:04:33 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 04:05:49 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 04:07:28 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 04:08:00 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 04:08:33 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 04:09:55 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 04:11:33 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 04:16:20 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 04:22:14 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 04:28:11 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 04:31:37 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 04:33:52 INFO mapreduce.Job: Job job_1422482982071_4841 completed successfully
15/04/14 04:33:52 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=47362322759
		FILE: Number of bytes written=94744232614
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=618
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=4
		Data-local map tasks=149
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=17780696
		Total time spent by all reduces in occupied slots (ms)=17388508
		Total time spent by all map tasks (ms)=8890348
		Total time spent by all reduce tasks (ms)=8694254
		Total vcore-seconds taken by all map tasks=8890348
		Total vcore-seconds taken by all reduce tasks=8694254
		Total megabyte-seconds taken by all map tasks=71976257408
		Total megabyte-seconds taken by all reduce tasks=104331048000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362327313
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362327313
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =808
		Failed Shuffles=0
		Merged Map outputs=808
		GC time elapsed (ms)=235249
		CPU time spent (ms)=21414640
		Physical memory (bytes) snapshot=404128608256
		Virtual memory (bytes) snapshot=1907524001792
		Total committed heap usage (bytes)=569487282176
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 04:33:52 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	56m55.218s
user	0m23.156s
sys	0m2.748s
15/04/14 04:33:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
50 35
r googlebooks-eng-all-5gram-20120701-st mapper.R reducer.R
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=r-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.R,./reducer.R -combiner ./reducer.R -mapper ./mapper.R -reducer ./reducer.R -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob7933862660203670490.jar tmpDir=null
15/04/14 04:33:57 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 04:33:58 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 04:33:58 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 04:33:59 INFO mapreduce.JobSubmitter: number of splits:134
15/04/14 04:33:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4844
15/04/14 04:33:59 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4844
15/04/14 04:34:00 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4844/
15/04/14 04:34:00 INFO mapreduce.Job: Running job: job_1422482982071_4844
15/04/14 04:34:04 INFO mapreduce.Job: Job job_1422482982071_4844 running in uber mode : false
15/04/14 04:34:04 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 04:34:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000054_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000076_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000081_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000112_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000131_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:14 INFO mapreduce.Job: Task Id : attempt_1422482982071_4844_m_000131_1, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:34:15 INFO mapreduce.Job:  map 1% reduce 0%
15/04/14 04:34:21 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 04:34:29 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 04:34:36 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 04:34:42 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 04:34:51 INFO mapreduce.Job:  map 6% reduce 0%
15/04/14 04:34:57 INFO mapreduce.Job:  map 7% reduce 0%
15/04/14 04:35:04 INFO mapreduce.Job:  map 8% reduce 0%
15/04/14 04:35:12 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 04:35:18 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 04:35:25 INFO mapreduce.Job:  map 11% reduce 0%
15/04/14 04:35:33 INFO mapreduce.Job:  map 12% reduce 0%
15/04/14 04:35:39 INFO mapreduce.Job:  map 13% reduce 0%
15/04/14 04:35:47 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 04:35:54 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 04:36:01 INFO mapreduce.Job:  map 16% reduce 0%
15/04/14 04:36:08 INFO mapreduce.Job:  map 17% reduce 0%
15/04/14 04:36:15 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 04:36:23 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 04:36:30 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 04:36:37 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 04:36:44 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 04:36:51 INFO mapreduce.Job:  map 23% reduce 0%
15/04/14 04:36:58 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 04:37:05 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 04:37:13 INFO mapreduce.Job:  map 26% reduce 0%
15/04/14 04:37:20 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 04:37:27 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 04:37:34 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 04:37:41 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 04:37:49 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 04:37:56 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 04:38:03 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 04:38:10 INFO mapreduce.Job:  map 34% reduce 0%
15/04/14 04:38:17 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 04:38:24 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 04:38:32 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 04:38:39 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 04:38:46 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 04:38:53 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 04:39:01 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 04:39:08 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 04:39:15 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 04:39:22 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 04:39:29 INFO mapreduce.Job:  map 45% reduce 0%
15/04/14 04:39:37 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 04:39:45 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 04:39:51 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 04:39:59 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 04:40:06 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 04:40:13 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 04:40:21 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 04:40:27 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 04:40:35 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 04:40:42 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 04:40:49 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 04:40:56 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 04:41:03 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 04:41:09 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 04:41:16 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 04:41:23 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 04:41:28 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 04:41:36 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 04:41:42 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 04:41:51 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 04:42:00 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 04:42:12 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 04:52:41 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 04:52:49 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 04:52:53 INFO mapreduce.Job:  map 70% reduce 0%
15/04/14 04:52:55 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 04:52:56 INFO mapreduce.Job:  map 71% reduce 3%
15/04/14 04:52:57 INFO mapreduce.Job:  map 71% reduce 4%
15/04/14 04:53:01 INFO mapreduce.Job:  map 72% reduce 4%
15/04/14 04:53:03 INFO mapreduce.Job:  map 72% reduce 5%
15/04/14 04:53:08 INFO mapreduce.Job:  map 73% reduce 5%
15/04/14 04:53:09 INFO mapreduce.Job:  map 73% reduce 6%
15/04/14 04:53:10 INFO mapreduce.Job:  map 74% reduce 6%
15/04/14 04:53:12 INFO mapreduce.Job:  map 74% reduce 7%
15/04/14 04:53:13 INFO mapreduce.Job:  map 75% reduce 7%
15/04/14 04:53:15 INFO mapreduce.Job:  map 75% reduce 8%
15/04/14 04:53:17 INFO mapreduce.Job:  map 76% reduce 8%
15/04/14 04:53:18 INFO mapreduce.Job:  map 76% reduce 9%
15/04/14 04:53:20 INFO mapreduce.Job:  map 77% reduce 9%
15/04/14 04:53:21 INFO mapreduce.Job:  map 77% reduce 10%
15/04/14 04:53:26 INFO mapreduce.Job:  map 78% reduce 10%
15/04/14 04:53:27 INFO mapreduce.Job:  map 79% reduce 11%
15/04/14 04:53:30 INFO mapreduce.Job:  map 79% reduce 12%
15/04/14 04:53:31 INFO mapreduce.Job:  map 80% reduce 12%
15/04/14 04:53:33 INFO mapreduce.Job:  map 81% reduce 14%
15/04/14 04:53:36 INFO mapreduce.Job:  map 82% reduce 14%
15/04/14 04:53:39 INFO mapreduce.Job:  map 82% reduce 16%
15/04/14 04:53:40 INFO mapreduce.Job:  map 83% reduce 16%
15/04/14 04:53:42 INFO mapreduce.Job:  map 84% reduce 17%
15/04/14 04:53:45 INFO mapreduce.Job:  map 85% reduce 17%
15/04/14 04:53:47 INFO mapreduce.Job:  map 86% reduce 17%
15/04/14 04:53:48 INFO mapreduce.Job:  map 86% reduce 19%
15/04/14 04:53:50 INFO mapreduce.Job:  map 87% reduce 19%
15/04/14 04:53:51 INFO mapreduce.Job:  map 87% reduce 20%
15/04/14 04:53:56 INFO mapreduce.Job:  map 88% reduce 20%
15/04/14 04:53:57 INFO mapreduce.Job:  map 88% reduce 21%
15/04/14 04:53:58 INFO mapreduce.Job:  map 89% reduce 21%
15/04/14 04:54:00 INFO mapreduce.Job:  map 90% reduce 22%
15/04/14 04:54:01 INFO mapreduce.Job:  map 91% reduce 22%
15/04/14 04:54:03 INFO mapreduce.Job:  map 92% reduce 25%
15/04/14 04:54:04 INFO mapreduce.Job:  map 93% reduce 25%
15/04/14 04:54:06 INFO mapreduce.Job:  map 93% reduce 26%
15/04/14 04:54:08 INFO mapreduce.Job:  map 95% reduce 26%
15/04/14 04:54:09 INFO mapreduce.Job:  map 96% reduce 28%
15/04/14 04:54:11 INFO mapreduce.Job:  map 97% reduce 28%
15/04/14 04:54:12 INFO mapreduce.Job:  map 97% reduce 30%
15/04/14 04:54:16 INFO mapreduce.Job:  map 98% reduce 30%
15/04/14 04:54:18 INFO mapreduce.Job:  map 98% reduce 31%
15/04/14 04:54:20 INFO mapreduce.Job:  map 99% reduce 31%
15/04/14 04:54:21 INFO mapreduce.Job:  map 99% reduce 32%
15/04/14 04:54:27 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 04:54:36 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 04:54:39 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 04:54:40 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 04:54:41 INFO mapreduce.Job: Job job_1422482982071_4844 completed successfully
15/04/14 04:54:41 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=70110743
		FILE: Number of bytes written=156461704
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=507
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Failed map tasks=6
		Launched map tasks=140
		Launched reduce tasks=35
		Other local map tasks=6
		Data-local map tasks=64
		Rack-local map tasks=70
		Total time spent by all maps in occupied slots (ms)=312600290
		Total time spent by all reduces in occupied slots (ms)=7835228
		Total time spent by all map tasks (ms)=156300145
		Total time spent by all reduce tasks (ms)=3917614
		Total vcore-seconds taken by all map tasks=156300145
		Total vcore-seconds taken by all reduce tasks=3917614
		Total megabyte-seconds taken by all map tasks=1265405973920
		Total megabyte-seconds taken by all reduce tasks=47011368000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70138673
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70138673
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =4690
		Failed Shuffles=0
		Merged Map outputs=4690
		GC time elapsed (ms)=188548
		CPU time spent (ms)=175728880
		Physical memory (bytes) snapshot=267906150400
		Virtual memory (bytes) snapshot=1698212667392
		Total committed heap usage (bytes)=444817866752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/14 04:54:41 INFO streaming.StreamJob: Output directory: ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st

real	20m49.199s
user	0m16.045s
sys	0m1.407s
15/04/14 04:54:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
50 35
r googlebooks-eng-all-5gram-20120701-st mapper.R reducer.R
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=r-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.R,./reducer.R  -mapper ./mapper.R -reducer ./reducer.R -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8061789994006080463.jar tmpDir=null
15/04/14 04:54:46 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 04:54:47 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 04:54:48 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 04:54:48 INFO mapreduce.JobSubmitter: number of splits:134
15/04/14 04:54:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4846
15/04/14 04:54:49 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4846
15/04/14 04:54:49 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4846/
15/04/14 04:54:49 INFO mapreduce.Job: Running job: job_1422482982071_4846
15/04/14 04:54:53 INFO mapreduce.Job: Job job_1422482982071_4846 running in uber mode : false
15/04/14 04:54:53 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 04:54:58 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_m_000064_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:54:59 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_m_000115_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:54:59 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_m_000120_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:55:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_m_000120_1, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:55:04 INFO mapreduce.Job:  map 1% reduce 0%
15/04/14 04:55:10 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 04:55:17 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 04:55:25 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 04:55:31 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 04:55:40 INFO mapreduce.Job:  map 6% reduce 0%
15/04/14 04:55:46 INFO mapreduce.Job:  map 7% reduce 0%
15/04/14 04:55:53 INFO mapreduce.Job:  map 8% reduce 0%
15/04/14 04:56:01 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 04:56:07 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 04:56:14 INFO mapreduce.Job:  map 11% reduce 0%
15/04/14 04:56:23 INFO mapreduce.Job:  map 12% reduce 0%
15/04/14 04:56:29 INFO mapreduce.Job:  map 13% reduce 0%
15/04/14 04:56:36 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 04:56:44 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 04:56:51 INFO mapreduce.Job:  map 16% reduce 0%
15/04/14 04:56:58 INFO mapreduce.Job:  map 17% reduce 0%
15/04/14 04:57:05 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 04:57:12 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 04:57:19 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 04:57:26 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 04:57:33 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 04:57:40 INFO mapreduce.Job:  map 23% reduce 0%
15/04/14 04:57:47 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 04:57:54 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 04:58:01 INFO mapreduce.Job:  map 26% reduce 0%
15/04/14 04:58:06 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 04:58:14 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 04:58:21 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 04:58:28 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 04:58:35 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 04:58:42 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 04:58:50 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 04:58:57 INFO mapreduce.Job:  map 34% reduce 0%
15/04/14 04:59:04 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 04:59:11 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 04:59:18 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 04:59:27 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 04:59:34 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 04:59:41 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 04:59:48 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 04:59:55 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 05:00:03 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 05:00:10 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 05:00:16 INFO mapreduce.Job:  map 45% reduce 0%
15/04/14 05:00:24 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 05:00:31 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 05:00:38 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 05:00:46 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 05:00:53 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 05:01:00 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 05:01:07 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 05:01:15 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 05:01:22 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 05:01:29 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 05:01:36 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 05:01:43 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 05:01:50 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 05:01:58 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 05:02:05 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 05:02:13 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 05:02:19 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 05:02:27 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 05:02:34 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 05:02:42 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 05:02:46 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 05:02:49 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 05:02:54 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 05:02:56 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 05:03:01 INFO mapreduce.Job:  map 70% reduce 3%
15/04/14 05:03:03 INFO mapreduce.Job:  map 71% reduce 3%
15/04/14 05:03:04 INFO mapreduce.Job:  map 71% reduce 4%
15/04/14 05:03:05 INFO mapreduce.Job:  map 72% reduce 4%
15/04/14 05:03:07 INFO mapreduce.Job:  map 73% reduce 5%
15/04/14 05:03:08 INFO mapreduce.Job:  map 73% reduce 6%
15/04/14 05:03:09 INFO mapreduce.Job:  map 74% reduce 6%
15/04/14 05:03:10 INFO mapreduce.Job:  map 75% reduce 7%
15/04/14 05:03:11 INFO mapreduce.Job:  map 76% reduce 8%
15/04/14 05:03:13 INFO mapreduce.Job:  map 77% reduce 9%
15/04/14 05:03:14 INFO mapreduce.Job:  map 79% reduce 10%
15/04/14 05:03:15 INFO mapreduce.Job:  map 80% reduce 10%
15/04/14 05:03:16 INFO mapreduce.Job:  map 81% reduce 11%
15/04/14 05:03:17 INFO mapreduce.Job:  map 82% reduce 14%
15/04/14 05:03:18 INFO mapreduce.Job:  map 83% reduce 14%
15/04/14 05:03:19 INFO mapreduce.Job:  map 84% reduce 14%
15/04/14 05:03:20 INFO mapreduce.Job:  map 84% reduce 17%
15/04/14 05:03:21 INFO mapreduce.Job:  map 86% reduce 17%
15/04/14 05:03:22 INFO mapreduce.Job:  map 87% reduce 17%
15/04/14 05:03:23 INFO mapreduce.Job:  map 88% reduce 20%
15/04/14 05:03:24 INFO mapreduce.Job:  map 90% reduce 20%
15/04/14 05:03:25 INFO mapreduce.Job:  map 91% reduce 20%
15/04/14 05:03:26 INFO mapreduce.Job:  map 92% reduce 24%
15/04/14 05:03:27 INFO mapreduce.Job:  map 94% reduce 24%
15/04/14 05:03:29 INFO mapreduce.Job:  map 95% reduce 27%
15/04/14 05:03:30 INFO mapreduce.Job:  map 96% reduce 27%
15/04/14 05:03:31 INFO mapreduce.Job:  map 97% reduce 27%
15/04/14 05:03:32 INFO mapreduce.Job:  map 98% reduce 29%
15/04/14 05:03:35 INFO mapreduce.Job:  map 99% reduce 30%
15/04/14 05:03:39 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 05:03:41 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 05:03:50 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 05:03:53 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 05:03:56 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 05:03:59 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 05:04:02 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 05:04:05 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 05:04:08 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 05:04:11 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 05:04:14 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 05:04:17 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 05:04:47 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 05:05:41 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 05:06:52 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 05:08:18 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 05:09:29 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 05:10:47 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 05:12:03 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 05:13:15 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 05:14:21 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 05:15:29 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 05:16:29 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 05:17:28 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 05:18:44 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 05:19:30 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 05:20:38 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 05:21:39 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 05:22:49 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 05:23:50 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 05:25:26 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 05:26:47 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 05:27:57 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 05:29:02 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 05:30:28 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 05:32:38 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 05:34:22 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 05:35:20 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 05:37:31 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 05:42:33 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 05:48:50 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 05:55:03 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 05:59:42 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 06:00:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000013_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-r-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4846_r_000013_0/part-00013 (inode 3642736): File does not exist. Holder DFSClient_attempt_1422482982071_4846_r_000013_0_648611131_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:00:29 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 06:00:40 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 06:01:19 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 06:02:34 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000005_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-r-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4846_r_000005_0/part-00005 (inode 3642732): File does not exist. Holder DFSClient_attempt_1422482982071_4846_r_000005_0_-526830136_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:02:35 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 06:02:46 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 06:02:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000007_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077963050_4222391 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/14 06:02:57 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:03:08 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:03:13 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000023_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-r-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4846_r_000023_0/part-00023 (inode 3642684): File does not exist. Holder DFSClient_attempt_1422482982071_4846_r_000023_0_-1915847618_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:03:14 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:03:22 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:03:25 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:03:48 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:04:01 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 06:05:52 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000014_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077963051_4222392 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/14 06:05:53 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:06:03 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:06:55 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:09:33 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000026_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077963128_4222469 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/14 06:09:34 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:09:45 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:10:39 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:19:28 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:24:21 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000023_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:24:22 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:24:59 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:25:57 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000007_2, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:26:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000007_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:26:30 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:26:41 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:27:14 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:29:42 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:31:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000014_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:31:10 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:31:20 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:31:32 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000011_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:31:33 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:31:48 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:34:12 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:35:20 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000013_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:35:29 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000013_2, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:35:30 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:35:40 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:36:14 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:37:02 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:38:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000012_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077963197_4222538 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/14 06:38:10 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:38:20 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:39:17 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:42:37 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:43:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000005_2, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:44:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000005_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:44:11 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:44:21 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:44:49 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 06:53:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000026_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:53:29 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:53:40 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:54:16 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 07:08:37 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 07:21:25 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 07:25:07 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 07:26:51 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 07:31:50 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 07:34:54 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 07:36:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000011_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 07:36:28 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 07:37:45 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000010_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 07:37:46 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 07:43:41 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 08:31:33 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 08:32:41 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 08:39:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4846_r_000010_1, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 08:39:44 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 08:41:03 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 08:41:03 INFO mapreduce.Job: Job job_1422482982071_4846 failed with state KILLED due to: Kill job job_1422482982071_4846 received from dotcz12 (auth:SIMPLE) at 129.114.57.130
Job received Kill while in RUNNING state.

15/04/14 08:41:03 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=27900249520
		FILE: Number of bytes written=58340556909
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4598499
		HDFS: Number of read operations=504
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=68
	Job Counters 
		Failed map tasks=4
		Failed reduce tasks=20
		Killed reduce tasks=4
		Launched map tasks=138
		Launched reduce tasks=58
		Other local map tasks=4
		Data-local map tasks=65
		Rack-local map tasks=69
		Total time spent by all maps in occupied slots (ms)=133695126
		Total time spent by all reduces in occupied slots (ms)=333935914
		Total time spent by all map tasks (ms)=66847563
		Total time spent by all reduce tasks (ms)=166967957
		Total vcore-seconds taken by all map tasks=66847563
		Total vcore-seconds taken by all reduce tasks=166967957
		Total megabyte-seconds taken by all map tasks=541197870048
		Total megabyte-seconds taken by all reduce tasks=2003615484000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424243933
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=316095
		Reduce shuffle bytes=27900276598
		Reduce input records=2758765204
		Reduce output records=316095
		Spilled Records=5889607964
		Shuffled Maps =4556
		Failed Shuffles=0
		Merged Map outputs=4556
		GC time elapsed (ms)=241188
		CPU time spent (ms)=170984280
		Physical memory (bytes) snapshot=297459294208
		Virtual memory (bytes) snapshot=1684462882816
		Total committed heap usage (bytes)=442712961024
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4598499
15/04/14 08:41:03 ERROR streaming.StreamJob: Job not Successful!
Streaming Command Failed!

real	226m21.409s
user	0m40.883s
sys	0m9.029s
