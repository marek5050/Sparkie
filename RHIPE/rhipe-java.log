2015-03-08 08:04:50,801 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:04:51,884 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:05:20,104 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:05:21,197 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:05:47,097 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:05:48,348 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:06:16,593 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:06:17,526 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:06:18,261 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 08:06:18,261 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 08:06:18,262 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 08:06:18,263 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 08:06:18,264 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-f57e9a99ced970ed90cb7d1e824b30a9#rhipe-temp-params-f57e9a99ced970ed90cb7d1e824b30a9
2015-03-08 08:06:18,270 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 08:06:18,274 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/output3
2015-03-08 08:06:18,308 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:06:18,441 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 08:06:18,584 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:1
2015-03-08 08:06:19,084 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0743
2015-03-08 08:06:19,436 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0743
2015-03-08 08:06:19,499 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0743/
2015-03-08 08:06:53,050 INFO  [main][org.apache.hadoop.io.compress.CodecPool] Got brand-new decompressor [.deflate]
2015-03-08 08:10:00,778 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:10:01,879 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:10:03,156 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 08:10:03,157 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 08:10:03,159 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 08:10:03,161 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 08:10:03,164 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-3802d88c07ed3fc611e5578c658048d8#rhipe-temp-params-3802d88c07ed3fc611e5578c658048d8
2015-03-08 08:10:03,178 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 08:10:03,189 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/output3
2015-03-08 08:10:03,260 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:10:03,575 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 08:10:03,768 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:1
2015-03-08 08:10:04,226 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0744
2015-03-08 08:10:04,607 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0744
2015-03-08 08:10:04,676 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0744/
2015-03-08 08:10:38,224 INFO  [main][org.apache.hadoop.io.compress.CodecPool] Got brand-new decompressor [.deflate]
2015-03-08 08:17:42,217 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 08:17:42,965 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:17:44,919 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 08:17:44,919 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 08:17:44,920 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 08:17:44,921 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 08:17:44,922 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-3b04c9eeb8a2f6cca910c9e791238c54#rhipe-temp-params-3b04c9eeb8a2f6cca910c9e791238c54
2015-03-08 08:17:44,928 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 08:17:44,933 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/pages-articles
2015-03-08 08:17:44,960 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 08:17:45,356 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 08:17:45,891 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:385
2015-03-08 08:17:46,380 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0745
2015-03-08 08:17:46,764 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0745
2015-03-08 08:17:46,833 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0745/
2015-03-08 09:52:56,982 INFO  [main][org.apache.hadoop.io.compress.CodecPool] Got brand-new decompressor [.deflate]
2015-03-08 17:52:43,481 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 17:52:44,612 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:53:27,152 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 17:53:27,893 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:53:29,252 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 17:53:29,252 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 17:53:29,255 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 17:53:29,256 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 17:53:29,259 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-9bd698ce146c233f8b28a130a7d8c25b#rhipe-temp-params-9bd698ce146c233f8b28a130a7d8c25b
2015-03-08 17:53:29,273 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 17:53:29,283 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/RHIPE_30GB_2012-pages-articles
2015-03-08 17:53:29,346 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:53:29,666 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 17:53:30,046 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:266
2015-03-08 17:53:30,526 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0747
2015-03-08 17:53:30,901 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0747
2015-03-08 17:53:30,965 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0747/
2015-03-08 17:55:14,233 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 17:55:15,187 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:55:16,216 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 17:55:16,219 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 17:55:16,220 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 17:55:16,223 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-770fe3da3b4241f0501855e894a73dcc#rhipe-temp-params-770fe3da3b4241f0501855e894a73dcc
2015-03-08 17:55:16,237 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 17:55:16,248 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/RHIPE_30GB_2012-pages-articles
2015-03-08 17:55:16,329 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:55:16,626 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 17:55:17,085 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:266
2015-03-08 17:55:17,565 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0748
2015-03-08 17:55:17,940 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0748
2015-03-08 17:55:18,004 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0748/
2015-03-08 17:56:33,207 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 17:56:34,438 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:56:35,130 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 17:56:35,130 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 17:56:35,131 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 17:56:35,132 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 17:56:35,133 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-49b7e618241ef2ea7d0ebee5b2ea8930#rhipe-temp-params-49b7e618241ef2ea7d0ebee5b2ea8930
2015-03-08 17:56:35,138 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 17:56:35,142 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/RHIPE_30GB_2012-pages-articles
2015-03-08 17:56:35,176 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 17:56:35,336 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 17:56:35,674 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:266
2015-03-08 17:56:36,107 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0749
2015-03-08 17:56:36,470 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0749
2015-03-08 17:56:36,530 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0749/
2015-03-08 18:03:28,930 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-08 18:03:29,657 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 18:03:30,549 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2015-03-08 18:03:30,550 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2015-03-08 18:03:30,552 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.reuse.jvm.num.tasks is deprecated. Instead, use mapreduce.job.jvm.numtasks
2015-03-08 18:03:30,554 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
2015-03-08 18:03:30,557 INFO  [main][org.godhuli.rhipe.RHMR] Adding to cache file:/tmp/rhipe-temp-params-caa5077e3d21a09c36ed8e46aa11ea37#rhipe-temp-params-caa5077e3d21a09c36ed8e46aa11ea37
2015-03-08 18:03:30,571 INFO  [main][org.apache.hadoop.conf.Configuration.deprecation] mapreduce.task.classpath.user.precedence is deprecated. Instead, use mapreduce.job.user.classpath.first
2015-03-08 18:03:30,581 DEBUG [main][org.godhuli.rhipe.RHMR] /user/dotcz12/out/RHIPE_30GB_3_2012-pages-articles
2015-03-08 18:03:30,645 INFO  [main][org.apache.hadoop.yarn.client.RMProxy] Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
2015-03-08 18:03:30,950 INFO  [main][org.apache.hadoop.mapreduce.lib.input.FileInputFormat] Total input paths to process : 1
2015-03-08 18:03:31,326 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] number of splits:266
2015-03-08 18:03:31,801 INFO  [main][org.apache.hadoop.mapreduce.JobSubmitter] Submitting tokens for job: job_1422482982071_0750
2015-03-08 18:03:32,159 INFO  [main][org.apache.hadoop.yarn.client.api.impl.YarnClientImpl] Submitted application application_1422482982071_0750
2015-03-08 18:03:32,223 INFO  [main][org.apache.hadoop.mapreduce.Job] The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_0750/
2015-03-08 18:14:20,055 INFO  [main][org.apache.hadoop.io.compress.CodecPool] Got brand-new decompressor [.deflate]
2015-03-08 18:33:33,750 INFO  [main][org.apache.hadoop.mapred.ClientServiceDelegate] Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
2015-03-08 18:33:34,756 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:35,757 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:36,758 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:37,758 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:38,759 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:39,760 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:40,760 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:41,761 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:42,762 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:43,763 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:43,869 INFO  [main][org.apache.hadoop.mapred.ClientServiceDelegate] Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
2015-03-08 18:33:44,871 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:45,872 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:46,872 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:47,873 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:48,874 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:49,875 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:50,875 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:51,876 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:52,877 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:53,878 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:53,985 INFO  [main][org.apache.hadoop.mapred.ClientServiceDelegate] Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
2015-03-08 18:33:54,986 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:55,987 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:56,988 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:57,989 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:58,990 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:33:59,991 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:34:00,992 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:34:01,993 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:34:02,994 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:34:03,995 INFO  [main][org.apache.hadoop.ipc.Client] Retrying connect to server: name.rustler.tacc.utexas.edu/129.114.57.132:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-03-08 18:34:04,097 WARN  [main][org.apache.hadoop.security.UserGroupInformation] PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:java.io.IOException: java.net.ConnectException: Call From login.rustler.tacc.utexas.edu/129.114.57.130 to name.rustler.tacc.utexas.edu:10020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2015-03-08 19:42:10,323 INFO  [main][org.apache.hadoop.io.compress.CodecPool] Got brand-new decompressor [.deflate]
2015-03-08 21:45:12,392 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49807, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,394 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.171:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49807, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49807, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,396 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56734, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,396 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.144:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56734, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56734, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,398 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33027, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,398 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.197:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33027, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33027, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,399 INFO  [main][org.apache.hadoop.hdfs.DFSClient] Could not obtain BP-1272212113-129.114.57.132-1408108439175:blk_1077447488_3706825 from any node: java.io.IOException: No live nodes contain current block No live nodes contain current block Block locations: 129.114.57.171:50010 129.114.57.144:50010 129.114.57.197:50010 Dead nodes:  129.114.57.171:50010 129.114.57.144:50010 129.114.57.197:50010. Will get new block locations from namenode and retry...
2015-03-08 21:45:12,399 WARN  [main][org.apache.hadoop.hdfs.DFSClient] DFS chooseDataNode: got # 1 IOException, will wait for 442.5173890348626 msec.
2015-03-08 21:45:12,859 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49811, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,860 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.171:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49811, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:49811, remote=/129.114.57.171:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,862 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56738, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,863 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.144:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56738, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:56738, remote=/129.114.57.144:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,864 WARN  [main][org.apache.hadoop.hdfs.BlockReaderFactory] I/O error constructing remote block reader.
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33031, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,865 WARN  [main][org.apache.hadoop.hdfs.DFSClient] Failed to connect to /129.114.57.197:50010 for block, add to deadNodes and continue. java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33031, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
java.io.IOException: Got error for OP_READ_BLOCK, self=/129.114.57.130:33031, remote=/129.114.57.197:50010, for file /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000, for pool BP-1272212113-129.114.57.132-1408108439175 block 1077447488_3706825
	at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
2015-03-08 21:45:12,866 INFO  [main][org.apache.hadoop.hdfs.DFSClient] Could not obtain BP-1272212113-129.114.57.132-1408108439175:blk_1077447488_3706825 from any node: java.io.IOException: No live nodes contain current block No live nodes contain current block Block locations: 129.114.57.171:50010 129.114.57.144:50010 129.114.57.197:50010 Dead nodes:  129.114.57.171:50010 129.114.57.144:50010 129.114.57.197:50010 129.114.57.198:50010. Will get new block locations from namenode and retry...
2015-03-08 21:45:12,866 WARN  [main][org.apache.hadoop.hdfs.DFSClient] DFS chooseDataNode: got # 1 IOException, will wait for 707.0741918426455 msec.
2015-03-08 21:45:13,576 WARN  [main][org.apache.hadoop.hdfs.DFSClient] DFS Read
java.io.IOException: Blocklist for /user/dotcz12/out/RHIPE_30GB_2012-pages-articles/part-r-00000 has changed!
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:277)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:906)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:559)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:644)
	at java.io.DataInputStream.readInt(DataInputStream.java:389)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2357)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2257)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2303)
	at org.godhuli.rhipe.SequenceFileIterator.nextChunk(SequenceFileIterator.java:123)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at RJavaTools.invokeMethod(RJavaTools.java:386)
