15/04/13 21:22:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 35
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob192153202406168472.jar tmpDir=null
15/04/13 21:22:13 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:22:13 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:22:14 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:22:14 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:22:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4755
15/04/13 21:22:15 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4755
15/04/13 21:22:15 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4755/
15/04/13 21:22:15 INFO mapreduce.Job: Running job: job_1422482982071_4755
15/04/13 21:22:19 INFO mapreduce.Job: Job job_1422482982071_4755 running in uber mode : false
15/04/13 21:22:19 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:22:31 INFO mapreduce.Job:  map 17% reduce 0%
15/04/13 21:22:32 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:22:34 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 21:22:35 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 21:22:37 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 21:22:38 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 21:22:40 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:22:41 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 21:22:42 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 21:22:43 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:22:44 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:22:45 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:23:30 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 21:23:31 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 21:23:32 INFO mapreduce.Job:  map 77% reduce 0%
15/04/13 21:23:33 INFO mapreduce.Job:  map 83% reduce 0%
15/04/13 21:23:34 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 21:23:35 INFO mapreduce.Job:  map 91% reduce 0%
15/04/13 21:23:36 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 21:23:37 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:23:38 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:23:39 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:23:40 INFO mapreduce.Job:  map 100% reduce 6%
15/04/13 21:23:41 INFO mapreduce.Job:  map 100% reduce 16%
15/04/13 21:23:42 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 21:23:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000001_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000001_0/part-00001 (inode 3633844): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000001_0_-1760266471_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000015_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000006_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000006_0/part-00006 (inode 3633846): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000006_0_973639845_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000006_0/part-00006 (inode 3633846): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000006_0_973639845_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000016_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000032_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000025_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000022_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000020_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000027_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000014_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000014_0/part-00014 (inode 3633864): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000014_0_-1100513962_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000014_0/part-00014 (inode 3633864): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000014_0_-1100513962_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000024_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000024_0/part-00024 (inode 3633866): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000024_0_-524508712_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4755_r_000024_0/part-00024 (inode 3633866): File does not exist. Holder DFSClient_attempt_1422482982071_4755_r_000024_0_-524508712_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000030_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:43 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000034_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000028_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000019_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000012_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000031_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000026_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000017_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4755_r_000023_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:23:47 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:23:48 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:23:49 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:23:50 INFO mapreduce.Job: Job job_1422482982071_4755 completed successfully
15/04/13 21:23:50 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=70110743
		FILE: Number of bytes written=156464070
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=506
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=69
	Job Counters 
		Failed reduce tasks=22
		Launched map tasks=134
		Launched reduce tasks=57
		Data-local map tasks=65
		Rack-local map tasks=69
		Total time spent by all maps in occupied slots (ms)=18934316
		Total time spent by all reduces in occupied slots (ms)=785936
		Total time spent by all map tasks (ms)=9467158
		Total time spent by all reduce tasks (ms)=392968
		Total vcore-seconds taken by all map tasks=9467158
		Total vcore-seconds taken by all reduce tasks=392968
		Total megabyte-seconds taken by all map tasks=76646111168
		Total megabyte-seconds taken by all reduce tasks=4715616000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70138673
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70138673
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =4690
		Failed Shuffles=0
		Merged Map outputs=4690
		GC time elapsed (ms)=46523
		CPU time spent (ms)=7626900
		Physical memory (bytes) snapshot=267910574080
		Virtual memory (bytes) snapshot=1697808576512
		Total committed heap usage (bytes)=444801126400
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:23:50 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m42.805s
user	0m11.782s
sys	0m0.722s
15/04/13 21:23:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 35
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8475251843391291085.jar tmpDir=null
15/04/13 21:23:55 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:55 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:23:56 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:23:56 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:23:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4758
15/04/13 21:23:57 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4758
15/04/13 21:23:57 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4758/
15/04/13 21:23:57 INFO mapreduce.Job: Running job: job_1422482982071_4758
15/04/13 21:24:02 INFO mapreduce.Job: Job job_1422482982071_4758 running in uber mode : false
15/04/13 21:24:02 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:24:12 INFO mapreduce.Job:  map 12% reduce 0%
15/04/13 21:24:13 INFO mapreduce.Job:  map 17% reduce 0%
15/04/13 21:24:14 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 21:24:15 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 21:24:16 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:24:17 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 21:24:18 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 21:24:19 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 21:24:20 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 21:24:21 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 21:24:22 INFO mapreduce.Job:  map 52% reduce 0%
15/04/13 21:24:23 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:24:24 INFO mapreduce.Job:  map 59% reduce 0%
15/04/13 21:24:25 INFO mapreduce.Job:  map 63% reduce 0%
15/04/13 21:24:26 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 21:24:28 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:24:29 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:24:42 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 21:24:43 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 21:24:44 INFO mapreduce.Job:  map 77% reduce 0%
15/04/13 21:24:45 INFO mapreduce.Job:  map 80% reduce 0%
15/04/13 21:24:46 INFO mapreduce.Job:  map 86% reduce 0%
15/04/13 21:24:47 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 21:24:48 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 21:24:49 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:24:50 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:24:53 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 21:24:56 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 21:24:59 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 21:25:02 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 21:25:05 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:25:08 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:25:11 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:25:14 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:25:17 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:25:20 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:25:20 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000020_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000020_0/part-00020 (inode 3634142): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000020_0_-1698997693_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000020_0/part-00020 (inode 3634142): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000020_0_-1698997693_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:21 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:25:23 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:25:23 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000019_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000019_0/part-00019 (inode 3634144): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000019_0_-159593508_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000019_0/part-00019 (inode 3634144): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000019_0_-159593508_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:24 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:25:25 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000002_0/part-00002 (inode 3634146): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000002_0_1891688731_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000002_0/part-00002 (inode 3634146): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000002_0_1891688731_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:26 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000021_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000021_0/part-00021 (inode 3634166): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000021_0_-1696770095_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000021_0/part-00021 (inode 3634166): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000021_0_-1696770095_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:26 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000009_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000009_0/part-00009 (inode 3634148): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000009_0_938432025_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000009_0/part-00009 (inode 3634148): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000009_0_938432025_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:27 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:25:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000003_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000003_0/part-00003 (inode 3634152): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000003_0_574182393_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000003_0/part-00003 (inode 3634152): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000003_0_574182393_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:28 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:25:29 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:25:31 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:25:32 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:25:32 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000000_0/part-00000 (inode 3634150): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000000_0_1183397981_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000000_0/part-00000 (inode 3634150): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000000_0_1183397981_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:33 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:25:35 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:25:38 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:25:38 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000033_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000033_0/part-00033 (inode 3634158): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000033_0_372104712_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000033_0/part-00033 (inode 3634158): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000033_0_372104712_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:38 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000025_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000025_0/part-00025 (inode 3634164): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000025_0_-1905732983_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000025_0/part-00025 (inode 3634164): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000025_0_-1905732983_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:38 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000022_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000022_0/part-00022 (inode 3634162): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000022_0_-1135315096_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000022_0/part-00022 (inode 3634162): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000022_0_-1135315096_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:39 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:25:40 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:25:40 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000008_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000008_0/part-00008 (inode 3634168): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000008_0_-2075765446_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000008_0/part-00008 (inode 3634168): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000008_0_-2075765446_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:40 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000034_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000034_0/part-00034 (inode 3634174): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000034_0_184417454_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000034_0/part-00034 (inode 3634174): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000034_0_184417454_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:41 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:25:42 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:25:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000006_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000006_0/part-00006 (inode 3634156): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000006_0_-449106494_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000006_0/part-00006 (inode 3634156): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000006_0_-449106494_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:43 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:25:44 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:25:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000027_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000027_0/part-00027 (inode 3634160): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000027_0_-2007847746_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000027_0/part-00027 (inode 3634160): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000027_0_-2007847746_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:45 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:25:45 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000029_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000029_0/part-00029 (inode 3634172): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000029_0_378832603_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000029_0/part-00029 (inode 3634172): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000029_0_378832603_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:46 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:25:47 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:25:47 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000004_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000004_0/part-00004 (inode 3634154): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000004_0_1541780409_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000004_0/part-00004 (inode 3634154): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000004_0_1541780409_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:48 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 21:25:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000017_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000017_0/part-00017 (inode 3634170): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000017_0_2101739839_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000017_0/part-00017 (inode 3634170): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000017_0_2101739839_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:50 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:25:51 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:25:52 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:25:54 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:25:55 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000032_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000032_0/part-00032 (inode 3634212): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000032_0_1269476383_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000032_0/part-00032 (inode 3634212): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000032_0_1269476383_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:56 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:25:57 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000024_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000024_0/part-00024 (inode 3634233): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000024_0_1312363008_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000024_0/part-00024 (inode 3634233): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000024_0_1312363008_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:25:58 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:25:59 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:26:01 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:26:02 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:26:03 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:26:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000030_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000030_0/part-00030 (inode 3634178): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000030_0_-677619866_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000030_0/part-00030 (inode 3634178): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000030_0_-677619866_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:04 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:26:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000016_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_0/part-00016 (inode 3634225): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_0_1843161923_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_0/part-00016 (inode 3634225): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_0_1843161923_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:05 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:26:07 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:26:07 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000015_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000015_0/part-00015 (inode 3634176): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000015_0_-915124613_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000015_0/part-00015 (inode 3634176): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000015_0_-915124613_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:08 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 21:26:09 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:26:11 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:26:14 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:26:15 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:26:16 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:26:17 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:26:18 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:26:18 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000031_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000031_0/part-00031 (inode 3634214): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000031_0_426507498_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000031_0/part-00031 (inode 3634214): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000031_0_426507498_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:19 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:26:21 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:26:24 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:26:24 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000001_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000001_0/part-00001 (inode 3634184): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000001_0_-422232736_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000001_0/part-00001 (inode 3634184): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000001_0_-422232736_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:24 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000007_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000007_0/part-00007 (inode 3634304): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000007_0_-1117484943_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000007_0/part-00007 (inode 3634304): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000007_0_-1117484943_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:25 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:26:27 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:26:30 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:26:34 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:26:35 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:26:36 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:26:39 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:26:39 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000028_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000028_0/part-00028 (inode 3634227): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000028_0_-540933257_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000028_0/part-00028 (inode 3634227): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000028_0_-540933257_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:26:40 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:26:42 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:26:45 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:26:48 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:26:50 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:26:51 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:26:55 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:26:58 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:27:00 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:27:03 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:27:06 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:27:07 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 21:27:09 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 21:27:09 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000000_1/part-00000 (inode 3634457): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000000_1_-1158725777_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:10 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:27:11 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000017_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000017_1/part-00017 (inode 3634479): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000017_1_360527646_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000017_1/part-00017 (inode 3634479): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000017_1_360527646_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:12 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:27:12 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000013_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000013_0/part-00013 (inode 3634350): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000013_0_1778018733_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000013_0/part-00013 (inode 3634350): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000013_0_1778018733_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:13 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:27:14 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:27:16 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000032_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000032_1/part-00032 (inode 3634481): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000032_1_1474387906_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000032_1/part-00032 (inode 3634481): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000032_1_1474387906_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:17 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:27:18 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000006_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000006_1/part-00006 (inode 3634461): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000006_1_-1858251220_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:19 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:27:19 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000005_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000005_0/part-00005 (inode 3634235): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000005_0_1220639807_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000005_0/part-00005 (inode 3634235): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000005_0_1220639807_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:20 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:27:21 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:27:23 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:27:24 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:27:25 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:27:26 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:27:26 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000021_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000021_1/part-00021 (inode 3634453): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000021_1_1374944361_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:27 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:27:28 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:27:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000022_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000022_1/part-00022 (inode 3634463): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000022_1_2128243860_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:29 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:27:30 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:27:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000016_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_1/part-00016 (inode 3634489): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_1_1787726989_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_1/part-00016 (inode 3634489): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_1_1787726989_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:32 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:27:33 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:27:34 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000008_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000008_1/part-00008 (inode 3634469): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000008_1_-1275322012_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:35 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:27:35 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000004_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000004_1/part-00004 (inode 3634475): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000004_1_-1903044497_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:36 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:27:36 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000033_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000033_1/part-00033 (inode 3634467): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000033_1_-441533977_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:37 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:27:37 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000024_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000024_1/part-00024 (inode 3634513): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000024_1_-1226051737_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000024_1/part-00024 (inode 3634513): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000024_1_-1226051737_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:38 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:27:38 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000030_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000030_1/part-00030 (inode 3634483): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000030_1_-440499637_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000030_1/part-00030 (inode 3634483): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000030_1_-440499637_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:39 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:27:39 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000025_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000025_1/part-00025 (inode 3634465): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000025_1_-1090437378_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:40 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:27:41 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000015_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000015_1/part-00015 (inode 3634485): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000015_1_692669700_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000015_1/part-00015 (inode 3634485): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000015_1_692669700_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:42 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:43 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:27:44 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000027_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000027_1/part-00027 (inode 3634473): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000027_1_-312810233_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:45 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:46 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:27:46 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000034_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000034_1/part-00034 (inode 3634471): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000034_1_-1792373297_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:27:47 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:48 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:27:48 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000026_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000026_0/part-00026 (inode 3634356): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000026_0_-588926537_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000026_0/part-00026 (inode 3634356): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000026_0_-588926537_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:27:49 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:27:50 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:27:51 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:27:53 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:27:55 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:27:56 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:27:57 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:27:59 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:28:01 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:28:01 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000029_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000029_1/part-00029 (inode 3634477): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000029_1_-517887214_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:28:02 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:04 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:28:06 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:28:06 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000007_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000007_1/part-00007 (inode 3634548): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000007_1_-1842009778_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000007_1/part-00007 (inode 3634548): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000007_1_-1842009778_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:07 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:08 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000001_1, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000001_1/part-00001 (inode 3634511): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000001_1_1324949991_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000001_1/part-00001 (inode 3634511): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000001_1_1324949991_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:09 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:28:10 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:28:12 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:16 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:28:18 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:28:19 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:28:20 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:28:23 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:28:26 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:28:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000018_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000018_0/part-00018 (inode 3634436): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000018_0_525920561_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:28:28 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:28:29 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:28:31 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:28:34 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:28:38 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:28:39 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:28:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000014_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000014_0/part-00014 (inode 3634443): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000014_0_-1777969251_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:28:43 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:28:46 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:28:47 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000027_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000027_2/part-00027 (inode 3634687): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000027_2_1662467008_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000027_2/part-00027 (inode 3634687): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000027_2_1662467008_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:47 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000000_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000000_2/part-00000 (inode 3634643): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000000_2_1034050757_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:28:47 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000012_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000012_0/part-00012 (inode 3634459): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000012_0_-1693507176_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000012_0/part-00012 (inode 3634459): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000012_0_-1693507176_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:48 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:28:48 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000016_2, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_2/part-00016 (inode 3634675): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_2_-215037481_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000016_2/part-00016 (inode 3634675): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000016_2_-215037481_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:28:49 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:28:51 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:28:51 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000023_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000023_0/part-00023 (inode 3634445): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000023_0_1746205269_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:28:51 INFO mapreduce.Job: Task Id : attempt_1422482982071_4758_r_000006_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4758_r_000006_2/part-00006 (inode 3634645): File does not exist. Holder DFSClient_attempt_1422482982071_4758_r_000006_2_1505654853_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:28:52 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:28:53 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:28:54 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:28:58 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:28:58 INFO mapreduce.Job: Job job_1422482982071_4758 failed with state FAILED due to: Task failed task_1422482982071_4758_r_000027
Job failed as tasks failed. failedMaps:0 failedReduces:1

15/04/13 21:28:58 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=2253006855
		FILE: Number of bytes written=32690563885
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=670962
		HDFS: Number of read operations=417
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Failed reduce tasks=57
		Killed reduce tasks=34
		Launched map tasks=134
		Launched reduce tasks=96
		Data-local map tasks=57
		Rack-local map tasks=77
		Total time spent by all maps in occupied slots (ms)=10895106
		Total time spent by all reduces in occupied slots (ms)=17098736
		Total time spent by all map tasks (ms)=5447553
		Total time spent by all reduce tasks (ms)=8549368
		Total vcore-seconds taken by all map tasks=5447553
		Total vcore-seconds taken by all reduce tasks=8549368
		Total megabyte-seconds taken by all map tasks=44103389088
		Total megabyte-seconds taken by all reduce tasks=102592416000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424243933
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=46132
		Reduce shuffle bytes=2253010845
		Reduce input records=186924839
		Reduce output records=46132
		Spilled Records=3317767599
		Shuffled Maps =670
		Failed Shuffles=0
		Merged Map outputs=670
		GC time elapsed (ms)=45580
		CPU time spent (ms)=6877190
		Physical memory (bytes) snapshot=263824371712
		Virtual memory (bytes) snapshot=1296912662528
		Total committed heap usage (bytes)=381652881408
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=670962
15/04/13 21:28:58 ERROR streaming.StreamJob: Job not Successful!
Streaming Command Failed!

real	5m7.931s
user	0m11.050s
sys	0m0.704s
50 25
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob7681339428450477289.jar tmpDir=null
15/04/13 21:29:03 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:29:03 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:29:04 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:29:04 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:29:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4764
15/04/13 21:29:05 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4764
15/04/13 21:29:05 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4764/
15/04/13 21:29:05 INFO mapreduce.Job: Running job: job_1422482982071_4764
15/04/13 21:29:11 INFO mapreduce.Job: Job job_1422482982071_4764 running in uber mode : false
15/04/13 21:29:11 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:29:21 INFO mapreduce.Job:  map 6% reduce 0%
15/04/13 21:29:22 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:29:23 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:29:24 INFO mapreduce.Job:  map 23% reduce 0%
15/04/13 21:29:25 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:29:26 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 21:29:27 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 21:29:28 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 21:29:29 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 21:29:30 INFO mapreduce.Job:  map 46% reduce 0%
15/04/13 21:29:31 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 21:29:32 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 21:29:33 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 21:29:34 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:29:35 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:30:21 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 21:30:22 INFO mapreduce.Job:  map 78% reduce 0%
15/04/13 21:30:23 INFO mapreduce.Job:  map 84% reduce 0%
15/04/13 21:30:24 INFO mapreduce.Job:  map 88% reduce 0%
15/04/13 21:30:25 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 21:30:26 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 21:30:27 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 21:30:28 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:30:29 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:30:31 INFO mapreduce.Job:  map 99% reduce 5%
15/04/13 21:30:32 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 21:30:33 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 21:30:35 INFO mapreduce.Job:  map 100% reduce 32%
15/04/13 21:30:36 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 21:30:37 INFO mapreduce.Job:  map 100% reduce 38%
15/04/13 21:30:38 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 21:30:39 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:30:40 INFO mapreduce.Job: Job job_1422482982071_4764 completed successfully
15/04/13 21:30:40 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110683
		FILE: Number of bytes written=155471270
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=477
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=25
		Data-local map tasks=60
		Rack-local map tasks=74
		Total time spent by all maps in occupied slots (ms)=18641942
		Total time spent by all reduces in occupied slots (ms)=748778
		Total time spent by all map tasks (ms)=9320971
		Total time spent by all reduce tasks (ms)=374389
		Total vcore-seconds taken by all map tasks=9320971
		Total vcore-seconds taken by all reduce tasks=374389
		Total megabyte-seconds taken by all map tasks=75462581216
		Total megabyte-seconds taken by all reduce tasks=4492668000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70130633
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70130633
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =3350
		Failed Shuffles=0
		Merged Map outputs=3350
		GC time elapsed (ms)=45603
		CPU time spent (ms)=7801860
		Physical memory (bytes) snapshot=265838379008
		Virtual memory (bytes) snapshot=1564803702784
		Total committed heap usage (bytes)=423752179712
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:30:40 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m41.482s
user	0m10.684s
sys	0m0.546s
50 25
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3656143650748364127.jar tmpDir=null
15/04/13 21:30:45 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:30:45 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:30:45 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:30:45 WARN security.UserGroupInformation: PriviledgedActionException as:dotcz12 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
15/04/13 21:30:45 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://name.rustler.tacc.utexas.edu:8020/user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st already exists
Streaming Command Failed!

real	0m5.596s
user	0m8.015s
sys	0m0.481s
15/04/13 21:30:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 15
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6488436876671540737.jar tmpDir=null
15/04/13 21:30:51 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:30:52 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:30:53 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:30:53 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:30:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4767
15/04/13 21:30:54 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4767
15/04/13 21:30:54 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4767/
15/04/13 21:30:54 INFO mapreduce.Job: Running job: job_1422482982071_4767
15/04/13 21:30:59 INFO mapreduce.Job: Job job_1422482982071_4767 running in uber mode : false
15/04/13 21:30:59 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:31:09 INFO mapreduce.Job:  map 8% reduce 0%
15/04/13 21:31:10 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:31:11 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:31:12 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 21:31:13 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:31:14 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 21:31:15 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 21:31:16 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 21:31:17 INFO mapreduce.Job:  map 43% reduce 0%
15/04/13 21:31:18 INFO mapreduce.Job:  map 48% reduce 0%
15/04/13 21:31:19 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 21:31:20 INFO mapreduce.Job:  map 55% reduce 0%
15/04/13 21:31:21 INFO mapreduce.Job:  map 59% reduce 0%
15/04/13 21:31:22 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:31:23 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:31:28 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:32:08 INFO mapreduce.Job:  map 70% reduce 0%
15/04/13 21:32:09 INFO mapreduce.Job:  map 77% reduce 0%
15/04/13 21:32:10 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:32:11 INFO mapreduce.Job:  map 89% reduce 0%
15/04/13 21:32:12 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 21:32:13 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:32:14 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 21:32:15 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 21:32:18 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:32:19 INFO mapreduce.Job:  map 100% reduce 24%
15/04/13 21:32:21 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 21:32:21 INFO mapreduce.Job: Task Id : attempt_1422482982071_4767_r_000012_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:32:21 INFO mapreduce.Job: Task Id : attempt_1422482982071_4767_r_000014_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:32:21 INFO mapreduce.Job: Task Id : attempt_1422482982071_4767_r_000008_0, Status : FAILED
Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/13 21:32:21 INFO mapreduce.Job: Task Id : attempt_1422482982071_4767_r_000005_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4767_r_000005_0/part-00005 (inode 3635191): File does not exist. Holder DFSClient_attempt_1422482982071_4767_r_000005_0_523901551_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:32:22 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:32:23 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:32:26 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:32:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4767_r_000007_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4767_r_000007_0/part-00007 (inode 3635282): File does not exist. Holder DFSClient_attempt_1422482982071_4767_r_000007_0_919512800_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 21:32:29 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 21:32:30 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:32:35 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:32:35 INFO mapreduce.Job: Job job_1422482982071_4767 completed successfully
15/04/13 21:32:35 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=70110623
		FILE: Number of bytes written=154478470
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=447
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Failed reduce tasks=5
		Killed reduce tasks=1
		Launched map tasks=134
		Launched reduce tasks=21
		Data-local map tasks=63
		Rack-local map tasks=71
		Total time spent by all maps in occupied slots (ms)=18573566
		Total time spent by all reduces in occupied slots (ms)=371644
		Total time spent by all map tasks (ms)=9286783
		Total time spent by all reduce tasks (ms)=185822
		Total vcore-seconds taken by all map tasks=9286783
		Total vcore-seconds taken by all reduce tasks=185822
		Total megabyte-seconds taken by all map tasks=75185795168
		Total megabyte-seconds taken by all reduce tasks=2229864000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70122593
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70122593
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =2010
		Failed Shuffles=0
		Merged Map outputs=2010
		GC time elapsed (ms)=42960
		CPU time spent (ms)=8457350
		Physical memory (bytes) snapshot=263895261184
		Virtual memory (bytes) snapshot=1430778888192
		Total committed heap usage (bytes)=402699251712
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:32:35 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m49.317s
user	0m12.853s
sys	0m0.660s
15/04/13 21:32:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 15
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2039478278002162143.jar tmpDir=null
15/04/13 21:32:40 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:40 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:32:41 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:32:41 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:32:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4770
15/04/13 21:32:42 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4770
15/04/13 21:32:42 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4770/
15/04/13 21:32:42 INFO mapreduce.Job: Running job: job_1422482982071_4770
15/04/13 21:32:47 INFO mapreduce.Job: Job job_1422482982071_4770 running in uber mode : false
15/04/13 21:32:47 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:32:57 INFO mapreduce.Job:  map 7% reduce 0%
15/04/13 21:32:58 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:32:59 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 21:33:00 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 21:33:01 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 21:33:02 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 21:33:03 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 21:33:04 INFO mapreduce.Job:  map 40% reduce 0%
15/04/13 21:33:05 INFO mapreduce.Job:  map 43% reduce 0%
15/04/13 21:33:06 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 21:33:07 INFO mapreduce.Job:  map 52% reduce 0%
15/04/13 21:33:08 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:33:09 INFO mapreduce.Job:  map 59% reduce 0%
15/04/13 21:33:10 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:33:11 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:33:13 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:33:29 INFO mapreduce.Job:  map 71% reduce 0%
15/04/13 21:33:30 INFO mapreduce.Job:  map 76% reduce 0%
15/04/13 21:33:31 INFO mapreduce.Job:  map 81% reduce 0%
15/04/13 21:33:32 INFO mapreduce.Job:  map 83% reduce 0%
15/04/13 21:33:33 INFO mapreduce.Job:  map 89% reduce 0%
15/04/13 21:33:34 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 21:33:35 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 21:33:36 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:33:39 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 21:33:40 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 21:34:01 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 21:34:04 INFO mapreduce.Job:  map 100% reduce 24%
15/04/13 21:34:07 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 21:34:10 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 21:34:13 INFO mapreduce.Job:  map 100% reduce 30%
15/04/13 21:34:15 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 21:34:16 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 21:34:18 INFO mapreduce.Job:  map 100% reduce 35%
15/04/13 21:34:19 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 21:34:22 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 21:34:25 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 21:34:28 INFO mapreduce.Job:  map 100% reduce 44%
15/04/13 21:34:29 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 21:34:31 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 21:34:34 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:34:37 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 21:34:40 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:34:43 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:34:46 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:34:49 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:34:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4770_r_000012_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4770_r_000012_0/part-00012 (inode 3635613): File does not exist. Holder DFSClient_attempt_1422482982071_4770_r_000012_0_104127300_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4770_r_000012_0/part-00012 (inode 3635613): File does not exist. Holder DFSClient_attempt_1422482982071_4770_r_000012_0_104127300_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 21:34:51 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:34:52 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:34:55 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:34:57 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:35:01 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 21:35:07 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:35:13 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 21:35:16 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:35:19 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:35:25 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:35:34 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:35:40 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:35:43 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:35:44 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:35:47 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:35:52 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:35:55 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:35:58 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:36:05 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:36:14 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:36:24 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:36:26 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:36:36 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:36:41 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 21:36:47 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 21:36:54 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 21:37:02 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 21:37:09 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 21:37:20 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 21:37:27 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 21:37:34 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 21:37:44 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 21:37:58 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 21:38:11 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:38:19 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:38:30 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:38:48 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 21:39:16 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:39:46 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:40:43 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 21:41:58 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 21:42:57 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:43:54 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 21:45:28 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:46:44 INFO mapreduce.Job: Job job_1422482982071_4770 completed successfully
15/04/13 21:46:44 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=30424216021
		FILE: Number of bytes written=60862641984
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=447
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Failed reduce tasks=1
		Launched map tasks=134
		Launched reduce tasks=16
		Data-local map tasks=61
		Rack-local map tasks=73
		Total time spent by all maps in occupied slots (ms)=11210462
		Total time spent by all reduces in occupied slots (ms)=11849094
		Total time spent by all map tasks (ms)=5605231
		Total time spent by all reduce tasks (ms)=5924547
		Total vcore-seconds taken by all map tasks=5605231
		Total vcore-seconds taken by all reduce tasks=5924547
		Total megabyte-seconds taken by all map tasks=45379950176
		Total megabyte-seconds taken by all reduce tasks=71094564000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424227853
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424227853
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =2010
		Failed Shuffles=0
		Merged Map outputs=2010
		GC time elapsed (ms)=157615
		CPU time spent (ms)=13787100
		Physical memory (bytes) snapshot=292297072640
		Virtual memory (bytes) snapshot=1431192588288
		Total committed heap usage (bytes)=402922086400
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:46:44 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	14m9.161s
user	0m14.371s
sys	0m1.134s
15/04/13 21:46:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 10
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob5401811887844404211.jar tmpDir=null
15/04/13 21:46:49 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:46:50 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:46:51 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:46:51 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:46:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4780
15/04/13 21:46:52 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4780
15/04/13 21:46:52 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4780/
15/04/13 21:46:52 INFO mapreduce.Job: Running job: job_1422482982071_4780
15/04/13 21:46:56 INFO mapreduce.Job: Job job_1422482982071_4780 running in uber mode : false
15/04/13 21:46:56 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:47:07 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 21:47:08 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:47:10 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 21:47:11 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 21:47:13 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 21:47:14 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 21:47:16 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 21:47:17 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 21:47:19 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 21:47:20 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 21:47:22 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:48:06 INFO mapreduce.Job:  map 71% reduce 0%
15/04/13 21:48:07 INFO mapreduce.Job:  map 76% reduce 0%
15/04/13 21:48:08 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:48:09 INFO mapreduce.Job:  map 88% reduce 0%
15/04/13 21:48:10 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 21:48:11 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:48:12 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:48:15 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 21:48:15 INFO mapreduce.Job: Job job_1422482982071_4780 completed successfully
15/04/13 21:48:16 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110593
		FILE: Number of bytes written=153982070
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=432
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=10
		Data-local map tasks=61
		Rack-local map tasks=73
		Total time spent by all maps in occupied slots (ms)=18672132
		Total time spent by all reduces in occupied slots (ms)=132192
		Total time spent by all map tasks (ms)=9336066
		Total time spent by all reduce tasks (ms)=66096
		Total vcore-seconds taken by all map tasks=9336066
		Total vcore-seconds taken by all reduce tasks=66096
		Total megabyte-seconds taken by all map tasks=75584790336
		Total megabyte-seconds taken by all reduce tasks=793152000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70118573
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70118573
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =1340
		Failed Shuffles=0
		Merged Map outputs=1340
		GC time elapsed (ms)=40669
		CPU time spent (ms)=9210920
		Physical memory (bytes) snapshot=262795223040
		Virtual memory (bytes) snapshot=1363559616512
		Total committed heap usage (bytes)=392174211072
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 21:48:16 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m31.645s
user	0m12.157s
sys	0m0.721s
15/04/13 21:48:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 10
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob7494721264472709329.jar tmpDir=null
15/04/13 21:48:21 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:48:21 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 21:48:21 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 21:48:22 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 21:48:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4782
15/04/13 21:48:23 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4782
15/04/13 21:48:23 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4782/
15/04/13 21:48:23 INFO mapreduce.Job: Running job: job_1422482982071_4782
15/04/13 21:48:27 INFO mapreduce.Job: Job job_1422482982071_4782 running in uber mode : false
15/04/13 21:48:27 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 21:48:37 INFO mapreduce.Job:  map 8% reduce 0%
15/04/13 21:48:38 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 21:48:39 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 21:48:40 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 21:48:41 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 21:48:42 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 21:48:43 INFO mapreduce.Job:  map 35% reduce 0%
15/04/13 21:48:44 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 21:48:45 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 21:48:46 INFO mapreduce.Job:  map 46% reduce 0%
15/04/13 21:48:47 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 21:48:48 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 21:48:49 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 21:48:50 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 21:48:51 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 21:49:08 INFO mapreduce.Job:  map 70% reduce 0%
15/04/13 21:49:09 INFO mapreduce.Job:  map 77% reduce 0%
15/04/13 21:49:10 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 21:49:11 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 21:49:12 INFO mapreduce.Job:  map 90% reduce 0%
15/04/13 21:49:13 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 21:49:14 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 21:49:15 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 21:49:18 INFO mapreduce.Job:  map 100% reduce 11%
15/04/13 21:49:19 INFO mapreduce.Job:  map 100% reduce 13%
15/04/13 21:49:39 INFO mapreduce.Job:  map 100% reduce 15%
15/04/13 21:49:42 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 21:49:43 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 21:49:45 INFO mapreduce.Job:  map 100% reduce 23%
15/04/13 21:49:46 INFO mapreduce.Job:  map 100% reduce 24%
15/04/13 21:49:48 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 21:49:51 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 21:49:58 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 21:50:07 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 21:50:10 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 21:50:11 INFO mapreduce.Job:  map 100% reduce 34%
15/04/13 21:50:12 INFO mapreduce.Job:  map 100% reduce 35%
15/04/13 21:50:14 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 21:50:16 INFO mapreduce.Job:  map 100% reduce 38%
15/04/13 21:50:17 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 21:50:20 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 21:50:22 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 21:50:23 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 21:50:34 INFO mapreduce.Job:  map 100% reduce 43%
15/04/13 21:50:35 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 21:50:36 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 21:50:37 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 21:50:38 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 21:50:39 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 21:50:40 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 21:50:41 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 21:50:43 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 21:50:44 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 21:50:46 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 21:50:47 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 21:50:50 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 21:50:53 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 21:50:55 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 21:50:59 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 21:51:02 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 21:51:05 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 21:51:11 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 21:51:17 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 21:51:20 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 21:51:30 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 21:51:53 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 21:52:15 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 21:52:24 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 21:52:41 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 21:52:50 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 21:53:10 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 21:53:19 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 21:53:42 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 21:53:54 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 21:54:06 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 21:54:20 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 21:54:26 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 21:54:41 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 21:55:02 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 21:55:25 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 21:55:36 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 21:55:39 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 21:55:51 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 21:56:16 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 21:56:33 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 21:56:51 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 21:57:04 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 21:57:25 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 21:57:50 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 21:58:20 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 21:58:35 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 21:58:59 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 22:00:02 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 22:01:36 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:02:42 INFO mapreduce.Job: Job job_1422482982071_4782 completed successfully
15/04/13 22:02:42 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=30424216009
		FILE: Number of bytes written=60862147202
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=432
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=10
		Data-local map tasks=64
		Rack-local map tasks=70
		Total time spent by all maps in occupied slots (ms)=10967152
		Total time spent by all reduces in occupied slots (ms)=10726208
		Total time spent by all map tasks (ms)=5483576
		Total time spent by all reduce tasks (ms)=5363104
		Total vcore-seconds taken by all map tasks=5483576
		Total vcore-seconds taken by all reduce tasks=5363104
		Total megabyte-seconds taken by all map tasks=44395031296
		Total megabyte-seconds taken by all reduce tasks=64357248000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424223833
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424223833
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =1340
		Failed Shuffles=0
		Merged Map outputs=1340
		GC time elapsed (ms)=152298
		CPU time spent (ms)=13112200
		Physical memory (bytes) snapshot=284995985408
		Virtual memory (bytes) snapshot=1363548868608
		Total committed heap usage (bytes)=392935776256
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:02:42 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	14m26.874s
user	0m14.491s
sys	0m1.073s
15/04/13 22:02:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 5
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob5929524678936374292.jar tmpDir=null
15/04/13 22:02:48 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:02:48 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:02:49 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:02:49 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:02:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4787
15/04/13 22:02:50 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4787
15/04/13 22:02:50 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4787/
15/04/13 22:02:50 INFO mapreduce.Job: Running job: job_1422482982071_4787
15/04/13 22:02:54 INFO mapreduce.Job: Job job_1422482982071_4787 running in uber mode : false
15/04/13 22:02:54 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:03:04 INFO mapreduce.Job:  map 8% reduce 0%
15/04/13 22:03:05 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 22:03:06 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:03:07 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 22:03:08 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 22:03:10 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:03:11 INFO mapreduce.Job:  map 37% reduce 0%
15/04/13 22:03:12 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 22:03:13 INFO mapreduce.Job:  map 45% reduce 0%
15/04/13 22:03:14 INFO mapreduce.Job:  map 49% reduce 0%
15/04/13 22:03:15 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 22:03:16 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 22:03:17 INFO mapreduce.Job:  map 60% reduce 0%
15/04/13 22:03:18 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 22:03:19 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:04:02 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 22:04:03 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 22:04:04 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 22:04:05 INFO mapreduce.Job:  map 79% reduce 0%
15/04/13 22:04:06 INFO mapreduce.Job:  map 86% reduce 0%
15/04/13 22:04:07 INFO mapreduce.Job:  map 90% reduce 0%
15/04/13 22:04:08 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 22:04:09 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 22:04:10 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 22:04:11 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 22:04:12 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:04:13 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 22:04:14 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 22:04:15 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:04:16 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 22:04:17 INFO mapreduce.Job: Job job_1422482982071_4787 completed successfully
15/04/13 22:04:17 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110563
		FILE: Number of bytes written=153485397
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=417
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=5
		Data-local map tasks=62
		Rack-local map tasks=72
		Total time spent by all maps in occupied slots (ms)=18372366
		Total time spent by all reduces in occupied slots (ms)=109788
		Total time spent by all map tasks (ms)=9186183
		Total time spent by all reduce tasks (ms)=54894
		Total vcore-seconds taken by all map tasks=9186183
		Total vcore-seconds taken by all reduce tasks=54894
		Total megabyte-seconds taken by all map tasks=74371337568
		Total megabyte-seconds taken by all reduce tasks=658728000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70114553
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70114553
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =670
		Failed Shuffles=0
		Merged Map outputs=670
		GC time elapsed (ms)=38400
		CPU time spent (ms)=9918370
		Physical memory (bytes) snapshot=261809405952
		Virtual memory (bytes) snapshot=1297156091904
		Total committed heap usage (bytes)=381647945728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 22:04:17 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m34.580s
user	0m11.005s
sys	0m0.694s
15/04/13 22:04:20 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 5
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2505971523614974890.jar tmpDir=null
15/04/13 22:04:22 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:04:23 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 22:04:23 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 22:04:24 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 22:04:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4788
15/04/13 22:04:24 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4788
15/04/13 22:04:25 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4788/
15/04/13 22:04:25 INFO mapreduce.Job: Running job: job_1422482982071_4788
15/04/13 22:04:29 INFO mapreduce.Job: Job job_1422482982071_4788 running in uber mode : false
15/04/13 22:04:29 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 22:04:39 INFO mapreduce.Job:  map 7% reduce 0%
15/04/13 22:04:40 INFO mapreduce.Job:  map 16% reduce 0%
15/04/13 22:04:41 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 22:04:42 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 22:04:43 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 22:04:44 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 22:04:45 INFO mapreduce.Job:  map 37% reduce 0%
15/04/13 22:04:46 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 22:04:47 INFO mapreduce.Job:  map 45% reduce 0%
15/04/13 22:04:48 INFO mapreduce.Job:  map 49% reduce 0%
15/04/13 22:04:49 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 22:04:50 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 22:04:51 INFO mapreduce.Job:  map 60% reduce 0%
15/04/13 22:04:52 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 22:04:53 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 22:05:09 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 22:05:10 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 22:05:11 INFO mapreduce.Job:  map 81% reduce 0%
15/04/13 22:05:12 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 22:05:13 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 22:05:14 INFO mapreduce.Job:  map 94% reduce 0%
15/04/13 22:05:15 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 22:05:16 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 22:05:17 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 22:05:18 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 22:05:20 INFO mapreduce.Job:  map 100% reduce 7%
15/04/13 22:05:23 INFO mapreduce.Job:  map 100% reduce 8%
15/04/13 22:05:44 INFO mapreduce.Job:  map 100% reduce 9%
15/04/13 22:05:47 INFO mapreduce.Job:  map 100% reduce 11%
15/04/13 22:05:50 INFO mapreduce.Job:  map 100% reduce 12%
15/04/13 22:06:02 INFO mapreduce.Job:  map 100% reduce 14%
15/04/13 22:06:11 INFO mapreduce.Job:  map 100% reduce 16%
15/04/13 22:06:17 INFO mapreduce.Job:  map 100% reduce 17%
15/04/13 22:06:20 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 22:06:29 INFO mapreduce.Job:  map 100% reduce 19%
15/04/13 22:06:36 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 22:06:39 INFO mapreduce.Job:  map 100% reduce 21%
15/04/13 22:06:45 INFO mapreduce.Job:  map 100% reduce 22%
15/04/13 22:06:48 INFO mapreduce.Job:  map 100% reduce 23%
15/04/13 22:06:55 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 22:07:00 INFO mapreduce.Job:  map 100% reduce 26%
15/04/13 22:07:04 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 22:07:15 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 22:07:19 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 22:07:22 INFO mapreduce.Job:  map 100% reduce 30%
15/04/13 22:07:27 INFO mapreduce.Job:  map 100% reduce 36%
15/04/13 22:07:28 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 22:07:31 INFO mapreduce.Job:  map 100% reduce 38%
15/04/13 22:07:37 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 22:07:40 INFO mapreduce.Job:  map 100% reduce 40%
15/04/13 22:07:42 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 22:07:43 INFO mapreduce.Job:  map 100% reduce 43%
15/04/13 22:07:46 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 22:07:49 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 22:07:52 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 22:08:09 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 22:08:10 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 22:08:12 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 22:08:13 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 22:08:15 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 22:08:16 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 22:08:18 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:08:19 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:08:22 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:08:25 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 22:08:28 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 22:08:41 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:09:16 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 22:10:11 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 22:10:17 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 22:10:44 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 22:11:35 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:12:29 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:12:55 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:13:19 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 22:14:14 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 22:14:58 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 22:15:41 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:15:47 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:16:12 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 22:16:36 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:16:57 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 22:17:24 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 22:17:37 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 22:17:49 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 22:18:16 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 22:18:31 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 22:19:01 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 22:19:11 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000004_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000004_0/part-00004 (inode 3637216): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000004_0_-113273842_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:19:12 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 22:19:23 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 22:19:59 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:20:06 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:20:17 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:20:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000000_0/part-00000 (inode 3637224): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000000_0_-178662262_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000000_0/part-00000 (inode 3637224): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000000_0_-178662262_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/13 22:20:28 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 22:20:33 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 22:20:38 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:20:55 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:21:00 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 22:21:12 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 22:21:23 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 22:21:24 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 22:21:35 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 22:21:45 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:21:48 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 22:21:51 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:22:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000002_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000002_0/part-00002 (inode 3637218): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000002_0_1013718279_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:22:04 INFO mapreduce.Job:  map 100% reduce 55%
15/04/13 22:22:15 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 22:22:18 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 22:22:39 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 22:22:47 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:22:57 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:23:06 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000003_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000003_0/part-00003 (inode 3637220): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000003_0_-1708493574_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:23:07 INFO mapreduce.Job:  map 100% reduce 41%
15/04/13 22:23:08 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 22:23:10 INFO mapreduce.Job:  map 100% reduce 43%
15/04/13 22:23:18 INFO mapreduce.Job:  map 100% reduce 44%
15/04/13 22:23:31 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 22:23:37 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 22:23:42 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 22:24:05 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 22:24:07 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 22:24:08 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 22:24:11 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 22:24:14 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 22:24:32 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 22:24:35 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:24:41 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:24:44 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 22:24:47 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 22:24:50 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 22:24:51 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000001_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000001_0/part-00001 (inode 3637222): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000001_0_-143857_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:24:52 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 22:24:53 INFO mapreduce.Job:  map 100% reduce 46%
15/04/13 22:24:59 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 22:25:03 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 22:25:23 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 22:25:45 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 22:25:50 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 22:25:53 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 22:25:56 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 22:26:05 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 22:26:15 INFO mapreduce.Job:  map 100% reduce 60%
15/04/13 22:26:38 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 22:26:42 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 22:27:09 INFO mapreduce.Job:  map 100% reduce 63%
15/04/13 22:27:12 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 22:27:40 INFO mapreduce.Job:  map 100% reduce 65%
15/04/13 22:27:44 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 22:28:11 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:28:14 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 22:28:17 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 22:28:20 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:29:58 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:30:11 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 22:30:13 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 22:30:34 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 22:31:01 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 22:31:34 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:32:01 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:32:41 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 22:33:08 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:33:21 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 22:33:48 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 22:34:30 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 22:35:06 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 22:35:41 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 22:36:03 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 22:36:44 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 22:36:57 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 22:37:41 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 22:38:08 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 22:38:20 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 22:38:48 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 22:39:30 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 22:40:15 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 22:41:10 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 22:42:03 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000000_1/part-00000 (inode 3637558): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000000_1_174081036_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:42:04 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:42:15 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:42:45 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 22:42:58 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:43:18 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 22:43:51 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 22:44:24 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 22:44:33 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 22:44:57 INFO mapreduce.Job: Task Id : attempt_1422482982071_4788_r_000001_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-st/_temporary/1/_temporary/attempt_1422482982071_4788_r_000001_1/part-00001 (inode 3637564): File does not exist. Holder DFSClient_attempt_1422482982071_4788_r_000001_1_773596045_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/13 22:44:58 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 22:44:59 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 22:45:08 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 22:45:29 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 22:45:32 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 22:45:35 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 22:45:36 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 22:45:38 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 22:46:03 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 22:46:33 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 22:47:00 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 22:47:26 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 22:47:57 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 22:48:00 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 22:48:03 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 22:48:06 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 22:48:09 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 22:50:02 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 22:54:41 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 22:55:25 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 22:56:21 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 22:57:47 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 22:58:25 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 22:59:37 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 23:00:27 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 23:01:18 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 23:02:12 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 23:03:17 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:04:06 INFO mapreduce.Job: Job job_1422482982071_4788 completed successfully
15/04/13 23:04:06 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=30424215991
		FILE: Number of bytes written=60861652141
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=417
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Failed reduce tasks=7
		Launched map tasks=134
		Launched reduce tasks=12
		Data-local map tasks=62
		Rack-local map tasks=72
		Total time spent by all maps in occupied slots (ms)=10820292
		Total time spent by all reduces in occupied slots (ms)=25585244
		Total time spent by all map tasks (ms)=5410146
		Total time spent by all reduce tasks (ms)=12792622
		Total vcore-seconds taken by all map tasks=5410146
		Total vcore-seconds taken by all reduce tasks=12792622
		Total megabyte-seconds taken by all map tasks=43800542016
		Total megabyte-seconds taken by all reduce tasks=153511464000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424219813
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424219813
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =670
		Failed Shuffles=0
		Merged Map outputs=670
		GC time elapsed (ms)=135271
		CPU time spent (ms)=12905470
		Physical memory (bytes) snapshot=273188036608
		Virtual memory (bytes) snapshot=1296942620672
		Total committed heap usage (bytes)=382153826304
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 23:04:06 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	59m49.299s
user	0m25.452s
sys	0m2.927s
15/04/13 23:04:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 4
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2701347892105934640.jar tmpDir=null
15/04/13 23:04:12 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:04:12 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:04:13 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:04:13 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 23:04:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4799
15/04/13 23:04:14 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4799
15/04/13 23:04:14 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4799/
15/04/13 23:04:14 INFO mapreduce.Job: Running job: job_1422482982071_4799
15/04/13 23:04:19 INFO mapreduce.Job: Job job_1422482982071_4799 running in uber mode : false
15/04/13 23:04:19 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:04:30 INFO mapreduce.Job:  map 3% reduce 0%
15/04/13 23:04:31 INFO mapreduce.Job:  map 17% reduce 0%
15/04/13 23:04:32 INFO mapreduce.Job:  map 19% reduce 0%
15/04/13 23:04:33 INFO mapreduce.Job:  map 21% reduce 0%
15/04/13 23:04:34 INFO mapreduce.Job:  map 30% reduce 0%
15/04/13 23:04:35 INFO mapreduce.Job:  map 31% reduce 0%
15/04/13 23:04:36 INFO mapreduce.Job:  map 33% reduce 0%
15/04/13 23:04:37 INFO mapreduce.Job:  map 42% reduce 0%
15/04/13 23:04:38 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 23:04:39 INFO mapreduce.Job:  map 45% reduce 0%
15/04/13 23:04:40 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 23:04:41 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 23:04:42 INFO mapreduce.Job:  map 57% reduce 0%
15/04/13 23:04:43 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 23:04:44 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 23:04:46 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:05:28 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 23:05:29 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 23:05:30 INFO mapreduce.Job:  map 79% reduce 0%
15/04/13 23:05:31 INFO mapreduce.Job:  map 89% reduce 0%
15/04/13 23:05:32 INFO mapreduce.Job:  map 94% reduce 0%
15/04/13 23:05:33 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 23:05:34 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:05:38 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 23:05:39 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 23:05:40 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:05:40 INFO mapreduce.Job: Job job_1422482982071_4799 completed successfully
15/04/13 23:05:40 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=70110557
		FILE: Number of bytes written=153386120
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=414
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=4
		Data-local map tasks=64
		Rack-local map tasks=70
		Total time spent by all maps in occupied slots (ms)=18141176
		Total time spent by all reduces in occupied slots (ms)=63322
		Total time spent by all map tasks (ms)=9070588
		Total time spent by all reduce tasks (ms)=31661
		Total vcore-seconds taken by all map tasks=9070588
		Total vcore-seconds taken by all reduce tasks=31661
		Total megabyte-seconds taken by all map tasks=73435480448
		Total megabyte-seconds taken by all reduce tasks=379932000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70113749
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70113749
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =536
		Failed Shuffles=0
		Merged Map outputs=536
		GC time elapsed (ms)=45866
		CPU time spent (ms)=9975030
		Physical memory (bytes) snapshot=261639086080
		Virtual memory (bytes) snapshot=1283991187456
		Total committed heap usage (bytes)=379555205120
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 23:05:40 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	1m33.483s
user	0m11.374s
sys	0m0.685s
15/04/13 23:05:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
50 4
py googlebooks-eng-all-5gram-20120701-st mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob4341452628700261147.jar tmpDir=null
15/04/13 23:05:45 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:05:45 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:05:46 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:05:46 INFO mapreduce.JobSubmitter: number of splits:134
15/04/13 23:05:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4801
15/04/13 23:05:47 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4801
15/04/13 23:05:47 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4801/
15/04/13 23:05:47 INFO mapreduce.Job: Running job: job_1422482982071_4801
15/04/13 23:05:52 INFO mapreduce.Job: Job job_1422482982071_4801 running in uber mode : false
15/04/13 23:05:52 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:06:02 INFO mapreduce.Job:  map 6% reduce 0%
15/04/13 23:06:03 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 23:06:04 INFO mapreduce.Job:  map 20% reduce 0%
15/04/13 23:06:05 INFO mapreduce.Job:  map 24% reduce 0%
15/04/13 23:06:06 INFO mapreduce.Job:  map 29% reduce 0%
15/04/13 23:06:07 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 23:06:08 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 23:06:09 INFO mapreduce.Job:  map 41% reduce 0%
15/04/13 23:06:10 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 23:06:11 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 23:06:12 INFO mapreduce.Job:  map 53% reduce 0%
15/04/13 23:06:14 INFO mapreduce.Job:  map 56% reduce 0%
15/04/13 23:06:15 INFO mapreduce.Job:  map 59% reduce 0%
15/04/13 23:06:16 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 23:06:17 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:06:34 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 23:06:35 INFO mapreduce.Job:  map 73% reduce 0%
15/04/13 23:06:36 INFO mapreduce.Job:  map 81% reduce 0%
15/04/13 23:06:37 INFO mapreduce.Job:  map 88% reduce 0%
15/04/13 23:06:38 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 23:06:39 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 23:06:40 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 23:06:41 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:06:44 INFO mapreduce.Job:  map 100% reduce 6%
15/04/13 23:07:08 INFO mapreduce.Job:  map 100% reduce 7%
15/04/13 23:07:11 INFO mapreduce.Job:  map 100% reduce 10%
15/04/13 23:07:29 INFO mapreduce.Job:  map 100% reduce 11%
15/04/13 23:07:35 INFO mapreduce.Job:  map 100% reduce 12%
15/04/13 23:07:36 INFO mapreduce.Job:  map 100% reduce 13%
15/04/13 23:07:39 INFO mapreduce.Job:  map 100% reduce 15%
15/04/13 23:07:59 INFO mapreduce.Job:  map 100% reduce 16%
15/04/13 23:08:02 INFO mapreduce.Job:  map 100% reduce 17%
15/04/13 23:08:03 INFO mapreduce.Job:  map 100% reduce 18%
15/04/13 23:08:06 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 23:08:29 INFO mapreduce.Job:  map 100% reduce 22%
15/04/13 23:08:30 INFO mapreduce.Job:  map 100% reduce 23%
15/04/13 23:08:33 INFO mapreduce.Job:  map 100% reduce 25%
15/04/13 23:08:56 INFO mapreduce.Job:  map 100% reduce 27%
15/04/13 23:09:00 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 23:09:22 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 23:09:25 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 23:09:28 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 23:09:31 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 23:09:34 INFO mapreduce.Job:  map 100% reduce 45%
15/04/13 23:09:37 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 23:09:49 INFO mapreduce.Job:  map 100% reduce 49%
15/04/13 23:09:52 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 23:09:55 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 23:09:57 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 23:09:58 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 23:10:24 INFO mapreduce.Job:  map 100% reduce 58%
15/04/13 23:10:55 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 23:11:25 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 23:11:28 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 23:11:31 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 23:11:34 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 23:11:37 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 23:11:40 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 23:12:08 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 23:12:48 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 23:13:54 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 23:16:04 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 23:16:31 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 23:16:51 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 23:17:00 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 23:17:52 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 23:18:13 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 23:18:56 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 23:19:35 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 23:19:46 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 23:19:51 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 23:20:06 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 23:20:30 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 23:21:00 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 23:21:30 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 23:22:05 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 23:22:30 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 23:23:11 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:23:22 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 23:23:55 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:24:20 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 23:24:48 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 23:25:23 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 23:25:59 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 23:26:41 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 23:28:47 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 23:31:37 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 23:33:59 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 23:36:55 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:38:14 INFO mapreduce.Job: Job job_1422482982071_4801 completed successfully
15/04/13 23:38:14 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=30424215985
		FILE: Number of bytes written=60861553184
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=414
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=134
		Launched reduce tasks=4
		Data-local map tasks=62
		Rack-local map tasks=72
		Total time spent by all maps in occupied slots (ms)=11043526
		Total time spent by all reduces in occupied slots (ms)=10865324
		Total time spent by all map tasks (ms)=5521763
		Total time spent by all reduce tasks (ms)=5432662
		Total vcore-seconds taken by all map tasks=5521763
		Total vcore-seconds taken by all reduce tasks=5432662
		Total megabyte-seconds taken by all map tasks=44704193248
		Total megabyte-seconds taken by all reduce tasks=65191944000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424219009
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=325361
		Reduce shuffle bytes=30424219009
		Reduce input records=3130842760
		Reduce output records=325361
		Spilled Records=6261685520
		Shuffled Maps =536
		Failed Shuffles=0
		Merged Map outputs=536
		GC time elapsed (ms)=199980
		CPU time spent (ms)=13219850
		Physical memory (bytes) snapshot=273560301568
		Virtual memory (bytes) snapshot=1284017926144
		Total committed heap usage (bytes)=382818406400
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/13 23:38:14 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-st

real	32m33.973s
user	0m19.651s
sys	0m1.677s
15/04/13 23:38:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 35
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6150705845879219293.jar tmpDir=null
15/04/13 23:38:18 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:38:18 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:38:19 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:38:19 INFO mapreduce.JobSubmitter: number of splits:202
15/04/13 23:38:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4803
15/04/13 23:38:20 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4803
15/04/13 23:38:20 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4803/
15/04/13 23:38:20 INFO mapreduce.Job: Running job: job_1422482982071_4803
15/04/13 23:38:24 INFO mapreduce.Job: Job job_1422482982071_4803 running in uber mode : false
15/04/13 23:38:24 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:38:34 INFO mapreduce.Job:  map 4% reduce 0%
15/04/13 23:38:35 INFO mapreduce.Job:  map 10% reduce 0%
15/04/13 23:38:36 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 23:38:37 INFO mapreduce.Job:  map 21% reduce 0%
15/04/13 23:38:38 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 23:38:39 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 23:38:40 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 23:38:41 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 23:38:42 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 23:38:43 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 23:38:44 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 23:38:45 INFO mapreduce.Job:  map 50% reduce 0%
15/04/13 23:38:46 INFO mapreduce.Job:  map 55% reduce 0%
15/04/13 23:38:47 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 23:38:48 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 23:38:49 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 23:38:50 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 23:38:51 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 23:38:52 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:39:36 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 23:39:37 INFO mapreduce.Job:  map 74% reduce 0%
15/04/13 23:39:38 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 23:39:39 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 23:39:40 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 23:39:41 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 23:39:42 INFO mapreduce.Job:  map 98% reduce 0%
15/04/13 23:39:43 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 23:39:44 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:39:47 INFO mapreduce.Job:  map 100% reduce 52%
15/04/13 23:39:48 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:39:49 INFO mapreduce.Job: Job job_1422482982071_4803 completed successfully
15/04/13 23:39:50 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467165
		FILE: Number of bytes written=213727762
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=711
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=35
		Data-local map tasks=150
		Rack-local map tasks=52
		Total time spent by all maps in occupied slots (ms)=29055290
		Total time spent by all reduces in occupied slots (ms)=611526
		Total time spent by all map tasks (ms)=14527645
		Total time spent by all reduce tasks (ms)=305763
		Total vcore-seconds taken by all map tasks=14527645
		Total vcore-seconds taken by all reduce tasks=305763
		Total megabyte-seconds taken by all map tasks=117615813920
		Total megabyte-seconds taken by all reduce tasks=3669156000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95509375
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95509375
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =7070
		Failed Shuffles=0
		Merged Map outputs=7070
		GC time elapsed (ms)=67842
		CPU time spent (ms)=12157030
		Physical memory (bytes) snapshot=400207376384
		Virtual memory (bytes) snapshot=2322518753280
		Total committed heap usage (bytes)=633131655168
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/13 23:39:50 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m35.719s
user	0m7.422s
sys	0m0.586s
15/04/13 23:39:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 35
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-35-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1711512767290590942.jar tmpDir=null
15/04/13 23:39:55 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:39:55 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:39:56 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:39:56 INFO mapreduce.JobSubmitter: number of splits:202
15/04/13 23:39:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4804
15/04/13 23:39:57 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4804
15/04/13 23:39:57 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4804/
15/04/13 23:39:57 INFO mapreduce.Job: Running job: job_1422482982071_4804
15/04/13 23:40:01 INFO mapreduce.Job: Job job_1422482982071_4804 running in uber mode : false
15/04/13 23:40:01 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:40:11 INFO mapreduce.Job:  map 5% reduce 0%
15/04/13 23:40:12 INFO mapreduce.Job:  map 10% reduce 0%
15/04/13 23:40:13 INFO mapreduce.Job:  map 14% reduce 0%
15/04/13 23:40:14 INFO mapreduce.Job:  map 21% reduce 0%
15/04/13 23:40:15 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 23:40:16 INFO mapreduce.Job:  map 27% reduce 0%
15/04/13 23:40:17 INFO mapreduce.Job:  map 33% reduce 0%
15/04/13 23:40:18 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 23:40:19 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 23:40:20 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 23:40:21 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 23:40:22 INFO mapreduce.Job:  map 50% reduce 0%
15/04/13 23:40:23 INFO mapreduce.Job:  map 55% reduce 0%
15/04/13 23:40:24 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 23:40:25 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 23:40:26 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 23:40:27 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 23:40:28 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 23:40:29 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:40:43 INFO mapreduce.Job:  map 71% reduce 0%
15/04/13 23:40:44 INFO mapreduce.Job:  map 78% reduce 0%
15/04/13 23:40:45 INFO mapreduce.Job:  map 82% reduce 0%
15/04/13 23:40:46 INFO mapreduce.Job:  map 88% reduce 0%
15/04/13 23:40:47 INFO mapreduce.Job:  map 92% reduce 0%
15/04/13 23:40:48 INFO mapreduce.Job:  map 95% reduce 0%
15/04/13 23:40:49 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 23:40:50 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 23:40:51 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:40:53 INFO mapreduce.Job:  map 100% reduce 28%
15/04/13 23:40:56 INFO mapreduce.Job:  map 100% reduce 29%
15/04/13 23:40:59 INFO mapreduce.Job:  map 100% reduce 31%
15/04/13 23:41:02 INFO mapreduce.Job:  map 100% reduce 33%
15/04/13 23:41:05 INFO mapreduce.Job:  map 100% reduce 35%
15/04/13 23:41:08 INFO mapreduce.Job:  map 100% reduce 37%
15/04/13 23:41:11 INFO mapreduce.Job:  map 100% reduce 39%
15/04/13 23:41:14 INFO mapreduce.Job:  map 100% reduce 42%
15/04/13 23:41:17 INFO mapreduce.Job:  map 100% reduce 44%
15/04/13 23:41:20 INFO mapreduce.Job:  map 100% reduce 47%
15/04/13 23:41:23 INFO mapreduce.Job:  map 100% reduce 48%
15/04/13 23:41:26 INFO mapreduce.Job:  map 100% reduce 50%
15/04/13 23:41:29 INFO mapreduce.Job:  map 100% reduce 51%
15/04/13 23:41:32 INFO mapreduce.Job:  map 100% reduce 53%
15/04/13 23:41:35 INFO mapreduce.Job:  map 100% reduce 54%
15/04/13 23:41:38 INFO mapreduce.Job:  map 100% reduce 56%
15/04/13 23:41:41 INFO mapreduce.Job:  map 100% reduce 57%
15/04/13 23:41:44 INFO mapreduce.Job:  map 100% reduce 59%
15/04/13 23:41:47 INFO mapreduce.Job:  map 100% reduce 61%
15/04/13 23:41:50 INFO mapreduce.Job:  map 100% reduce 62%
15/04/13 23:41:53 INFO mapreduce.Job:  map 100% reduce 64%
15/04/13 23:41:56 INFO mapreduce.Job:  map 100% reduce 66%
15/04/13 23:41:59 INFO mapreduce.Job:  map 100% reduce 67%
15/04/13 23:42:02 INFO mapreduce.Job:  map 100% reduce 68%
15/04/13 23:42:03 INFO mapreduce.Job:  map 100% reduce 69%
15/04/13 23:42:06 INFO mapreduce.Job:  map 100% reduce 70%
15/04/13 23:42:09 INFO mapreduce.Job:  map 100% reduce 71%
15/04/13 23:42:14 INFO mapreduce.Job:  map 100% reduce 72%
15/04/13 23:42:17 INFO mapreduce.Job:  map 100% reduce 73%
15/04/13 23:42:23 INFO mapreduce.Job:  map 100% reduce 74%
15/04/13 23:42:27 INFO mapreduce.Job:  map 100% reduce 75%
15/04/13 23:42:32 INFO mapreduce.Job:  map 100% reduce 76%
15/04/13 23:42:33 INFO mapreduce.Job:  map 100% reduce 77%
15/04/13 23:42:36 INFO mapreduce.Job:  map 100% reduce 78%
15/04/13 23:42:39 INFO mapreduce.Job:  map 100% reduce 79%
15/04/13 23:42:41 INFO mapreduce.Job:  map 100% reduce 80%
15/04/13 23:42:44 INFO mapreduce.Job:  map 100% reduce 81%
15/04/13 23:42:48 INFO mapreduce.Job:  map 100% reduce 82%
15/04/13 23:42:51 INFO mapreduce.Job:  map 100% reduce 83%
15/04/13 23:42:57 INFO mapreduce.Job:  map 100% reduce 84%
15/04/13 23:43:09 INFO mapreduce.Job:  map 100% reduce 85%
15/04/13 23:43:15 INFO mapreduce.Job:  map 100% reduce 86%
15/04/13 23:43:22 INFO mapreduce.Job:  map 100% reduce 87%
15/04/13 23:43:31 INFO mapreduce.Job:  map 100% reduce 88%
15/04/13 23:43:37 INFO mapreduce.Job:  map 100% reduce 89%
15/04/13 23:43:46 INFO mapreduce.Job:  map 100% reduce 90%
15/04/13 23:44:01 INFO mapreduce.Job:  map 100% reduce 91%
15/04/13 23:44:22 INFO mapreduce.Job:  map 100% reduce 92%
15/04/13 23:44:43 INFO mapreduce.Job:  map 100% reduce 93%
15/04/13 23:45:16 INFO mapreduce.Job:  map 100% reduce 94%
15/04/13 23:45:46 INFO mapreduce.Job:  map 100% reduce 95%
15/04/13 23:46:29 INFO mapreduce.Job:  map 100% reduce 96%
15/04/13 23:47:32 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 23:49:03 INFO mapreduce.Job:  map 100% reduce 98%
15/04/13 23:50:42 INFO mapreduce.Job:  map 100% reduce 99%
15/04/13 23:55:36 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:57:10 INFO mapreduce.Job: Job job_1422482982071_4804 completed successfully
15/04/13 23:57:10 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322849
		FILE: Number of bytes written=94747363924
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=711
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Killed reduce tasks=3
		Launched map tasks=202
		Launched reduce tasks=38
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=16347058
		Total time spent by all reduces in occupied slots (ms)=21110814
		Total time spent by all map tasks (ms)=8173529
		Total time spent by all reduce tasks (ms)=10555407
		Total vcore-seconds taken by all map tasks=8173529
		Total vcore-seconds taken by all reduce tasks=10555407
		Total megabyte-seconds taken by all map tasks=66172890784
		Total megabyte-seconds taken by all reduce tasks=126664884000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362364885
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362364885
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =7070
		Failed Shuffles=0
		Merged Map outputs=7070
		GC time elapsed (ms)=222191
		CPU time spent (ms)=20883090
		Physical memory (bytes) snapshot=447036420096
		Virtual memory (bytes) snapshot=2321770078208
		Total committed heap usage (bytes)=633653248000
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/13 23:57:10 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	17m20.811s
user	0m15.614s
sys	0m1.101s
15/04/13 23:57:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 25
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2205848145883733089.jar tmpDir=null
15/04/13 23:57:15 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:57:15 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:57:16 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:57:17 INFO mapreduce.JobSubmitter: number of splits:202
15/04/13 23:57:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4806
15/04/13 23:57:17 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4806
15/04/13 23:57:17 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4806/
15/04/13 23:57:17 INFO mapreduce.Job: Running job: job_1422482982071_4806
15/04/13 23:57:22 INFO mapreduce.Job: Job job_1422482982071_4806 running in uber mode : false
15/04/13 23:57:22 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:57:32 INFO mapreduce.Job:  map 5% reduce 0%
15/04/13 23:57:33 INFO mapreduce.Job:  map 10% reduce 0%
15/04/13 23:57:34 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 23:57:35 INFO mapreduce.Job:  map 22% reduce 0%
15/04/13 23:57:36 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 23:57:37 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 23:57:38 INFO mapreduce.Job:  map 33% reduce 0%
15/04/13 23:57:39 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 23:57:40 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 23:57:41 INFO mapreduce.Job:  map 44% reduce 0%
15/04/13 23:57:42 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 23:57:43 INFO mapreduce.Job:  map 50% reduce 0%
15/04/13 23:57:44 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 23:57:45 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 23:57:46 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 23:57:47 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 23:57:48 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 23:57:49 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 23:57:50 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:58:34 INFO mapreduce.Job:  map 68% reduce 0%
15/04/13 23:58:35 INFO mapreduce.Job:  map 72% reduce 0%
15/04/13 23:58:36 INFO mapreduce.Job:  map 80% reduce 0%
15/04/13 23:58:37 INFO mapreduce.Job:  map 86% reduce 0%
15/04/13 23:58:38 INFO mapreduce.Job:  map 90% reduce 0%
15/04/13 23:58:39 INFO mapreduce.Job:  map 93% reduce 0%
15/04/13 23:58:40 INFO mapreduce.Job:  map 97% reduce 0%
15/04/13 23:58:41 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 23:58:42 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:58:45 INFO mapreduce.Job:  map 100% reduce 97%
15/04/13 23:58:46 INFO mapreduce.Job:  map 100% reduce 100%
15/04/13 23:58:47 INFO mapreduce.Job: Job job_1422482982071_4806 completed successfully
15/04/13 23:58:47 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467105
		FILE: Number of bytes written=212714562
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=681
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=25
		Data-local map tasks=149
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=29144260
		Total time spent by all reduces in occupied slots (ms)=406738
		Total time spent by all map tasks (ms)=14572130
		Total time spent by all reduce tasks (ms)=203369
		Total vcore-seconds taken by all map tasks=14572130
		Total vcore-seconds taken by all reduce tasks=203369
		Total megabyte-seconds taken by all map tasks=117975964480
		Total megabyte-seconds taken by all reduce tasks=2440428000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95497255
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95497255
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =5050
		Failed Shuffles=0
		Merged Map outputs=5050
		GC time elapsed (ms)=64289
		CPU time spent (ms)=12406500
		Physical memory (bytes) snapshot=398347948032
		Virtual memory (bytes) snapshot=2188301234176
		Total committed heap usage (bytes)=612080869376
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/13 23:58:47 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m36.950s
user	0m10.099s
sys	0m0.701s
15/04/13 23:58:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 25
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-25-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=25 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3448927656864759701.jar tmpDir=null
15/04/13 23:58:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:58:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/13 23:58:54 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/13 23:58:54 INFO mapreduce.JobSubmitter: number of splits:202
15/04/13 23:58:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4807
15/04/13 23:58:55 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4807
15/04/13 23:58:55 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4807/
15/04/13 23:58:55 INFO mapreduce.Job: Running job: job_1422482982071_4807
15/04/13 23:58:59 INFO mapreduce.Job: Job job_1422482982071_4807 running in uber mode : false
15/04/13 23:58:59 INFO mapreduce.Job:  map 0% reduce 0%
15/04/13 23:59:09 INFO mapreduce.Job:  map 5% reduce 0%
15/04/13 23:59:10 INFO mapreduce.Job:  map 10% reduce 0%
15/04/13 23:59:11 INFO mapreduce.Job:  map 15% reduce 0%
15/04/13 23:59:12 INFO mapreduce.Job:  map 21% reduce 0%
15/04/13 23:59:13 INFO mapreduce.Job:  map 25% reduce 0%
15/04/13 23:59:14 INFO mapreduce.Job:  map 28% reduce 0%
15/04/13 23:59:15 INFO mapreduce.Job:  map 32% reduce 0%
15/04/13 23:59:16 INFO mapreduce.Job:  map 36% reduce 0%
15/04/13 23:59:17 INFO mapreduce.Job:  map 39% reduce 0%
15/04/13 23:59:18 INFO mapreduce.Job:  map 43% reduce 0%
15/04/13 23:59:19 INFO mapreduce.Job:  map 47% reduce 0%
15/04/13 23:59:20 INFO mapreduce.Job:  map 50% reduce 0%
15/04/13 23:59:21 INFO mapreduce.Job:  map 54% reduce 0%
15/04/13 23:59:22 INFO mapreduce.Job:  map 58% reduce 0%
15/04/13 23:59:23 INFO mapreduce.Job:  map 61% reduce 0%
15/04/13 23:59:24 INFO mapreduce.Job:  map 64% reduce 0%
15/04/13 23:59:25 INFO mapreduce.Job:  map 65% reduce 0%
15/04/13 23:59:26 INFO mapreduce.Job:  map 66% reduce 0%
15/04/13 23:59:27 INFO mapreduce.Job:  map 67% reduce 0%
15/04/13 23:59:40 INFO mapreduce.Job:  map 69% reduce 0%
15/04/13 23:59:41 INFO mapreduce.Job:  map 76% reduce 0%
15/04/13 23:59:42 INFO mapreduce.Job:  map 81% reduce 0%
15/04/13 23:59:43 INFO mapreduce.Job:  map 87% reduce 0%
15/04/13 23:59:44 INFO mapreduce.Job:  map 91% reduce 0%
15/04/13 23:59:45 INFO mapreduce.Job:  map 94% reduce 0%
15/04/13 23:59:46 INFO mapreduce.Job:  map 96% reduce 0%
15/04/13 23:59:47 INFO mapreduce.Job:  map 99% reduce 0%
15/04/13 23:59:48 INFO mapreduce.Job:  map 100% reduce 0%
15/04/13 23:59:51 INFO mapreduce.Job:  map 100% reduce 20%
15/04/13 23:59:55 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 00:00:13 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 00:00:16 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 00:00:19 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 00:00:22 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 00:00:25 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 00:00:28 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 00:00:31 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:00:34 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 00:00:37 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 00:00:40 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 00:00:43 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:00:46 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:00:49 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:00:52 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:00:55 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:00:58 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:01:01 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:01:04 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:01:07 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 00:01:10 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 00:01:16 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 00:01:19 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 00:01:22 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 00:01:25 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 00:01:28 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 00:01:34 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 00:01:37 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 00:01:43 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 00:01:49 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:01:56 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 00:02:04 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 00:02:11 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 00:02:13 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 00:02:22 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 00:02:29 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 00:02:32 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 00:02:41 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 00:02:47 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 00:02:57 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 00:03:06 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 00:03:18 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 00:03:33 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 00:03:44 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 00:03:57 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 00:04:11 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 00:04:27 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 00:05:08 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 00:05:24 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 00:05:42 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:05:54 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:06:16 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:07:10 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:07:31 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:08:10 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:09:45 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:13:26 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:16:05 INFO mapreduce.Job: Job job_1422482982071_4807 completed successfully
15/04/14 00:16:05 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322843
		FILE: Number of bytes written=94746353978
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=681
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=50
	Job Counters 
		Killed reduce tasks=1
		Launched map tasks=202
		Launched reduce tasks=26
		Data-local map tasks=149
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=16444562
		Total time spent by all reduces in occupied slots (ms)=18404536
		Total time spent by all map tasks (ms)=8222281
		Total time spent by all reduce tasks (ms)=9202268
		Total vcore-seconds taken by all map tasks=8222281
		Total vcore-seconds taken by all reduce tasks=9202268
		Total megabyte-seconds taken by all map tasks=66567586976
		Total megabyte-seconds taken by all reduce tasks=110427216000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362352765
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362352765
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =5050
		Failed Shuffles=0
		Merged Map outputs=5050
		GC time elapsed (ms)=216043
		CPU time spent (ms)=20825560
		Physical memory (bytes) snapshot=443688124416
		Virtual memory (bytes) snapshot=2188911632384
		Total committed heap usage (bytes)=612489302016
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:16:05 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	17m18.114s
user	0m15.732s
sys	0m1.252s
15/04/14 00:16:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 15
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2532078875299823326.jar tmpDir=null
15/04/14 00:16:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:16:11 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:16:12 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:16:12 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:16:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4809
15/04/14 00:16:13 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4809
15/04/14 00:16:13 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4809/
15/04/14 00:16:13 INFO mapreduce.Job: Running job: job_1422482982071_4809
15/04/14 00:16:17 INFO mapreduce.Job: Job job_1422482982071_4809 running in uber mode : false
15/04/14 00:16:17 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:16:27 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 00:16:28 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:16:29 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 00:16:30 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 00:16:31 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 00:16:32 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:16:33 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 00:16:34 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:16:35 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 00:16:36 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 00:16:37 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:16:38 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:16:39 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 00:16:40 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:16:41 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:16:42 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 00:16:43 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:16:44 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:16:45 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:17:31 INFO mapreduce.Job:  map 70% reduce 0%
15/04/14 00:17:32 INFO mapreduce.Job:  map 76% reduce 0%
15/04/14 00:17:33 INFO mapreduce.Job:  map 81% reduce 0%
15/04/14 00:17:34 INFO mapreduce.Job:  map 86% reduce 0%
15/04/14 00:17:35 INFO mapreduce.Job:  map 90% reduce 0%
15/04/14 00:17:36 INFO mapreduce.Job:  map 95% reduce 0%
15/04/14 00:17:37 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:17:38 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:17:39 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:17:41 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:17:42 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:17:43 INFO mapreduce.Job: Job job_1422482982071_4809 completed successfully
15/04/14 00:17:43 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467045
		FILE: Number of bytes written=211701362
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=651
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=15
		Data-local map tasks=151
		Rack-local map tasks=51
		Total time spent by all maps in occupied slots (ms)=29453668
		Total time spent by all reduces in occupied slots (ms)=226644
		Total time spent by all map tasks (ms)=14726834
		Total time spent by all reduce tasks (ms)=113322
		Total vcore-seconds taken by all map tasks=14726834
		Total vcore-seconds taken by all reduce tasks=113322
		Total megabyte-seconds taken by all map tasks=119228448064
		Total megabyte-seconds taken by all reduce tasks=1359864000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95485135
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95485135
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =3030
		Failed Shuffles=0
		Merged Map outputs=3030
		GC time elapsed (ms)=63069
		CPU time spent (ms)=13845140
		Physical memory (bytes) snapshot=396153729024
		Virtual memory (bytes) snapshot=2055105417216
		Total committed heap usage (bytes)=591031578624
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:17:43 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m37.400s
user	0m12.080s
sys	0m0.589s
15/04/14 00:17:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 15
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-15-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=15 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1512959774601980021.jar tmpDir=null
15/04/14 00:17:48 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:17:48 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:17:49 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:17:49 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:17:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4810
15/04/14 00:17:50 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4810
15/04/14 00:17:50 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4810/
15/04/14 00:17:50 INFO mapreduce.Job: Running job: job_1422482982071_4810
15/04/14 00:17:54 INFO mapreduce.Job: Job job_1422482982071_4810 running in uber mode : false
15/04/14 00:17:54 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:18:05 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 00:18:06 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:18:07 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 00:18:08 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 00:18:09 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 00:18:10 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 00:18:11 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 00:18:12 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:18:13 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 00:18:14 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 00:18:15 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:18:16 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:18:17 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 00:18:18 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:18:19 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:18:20 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:18:22 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:18:23 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:18:36 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 00:18:37 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 00:18:38 INFO mapreduce.Job:  map 76% reduce 0%
15/04/14 00:18:39 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 00:18:40 INFO mapreduce.Job:  map 86% reduce 0%
15/04/14 00:18:41 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 00:18:42 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 00:18:43 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 00:18:44 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:18:45 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:18:47 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 00:18:50 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 00:19:11 INFO mapreduce.Job:  map 100% reduce 16%
15/04/14 00:19:14 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 00:19:15 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 00:19:17 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 00:19:23 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 00:19:26 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 00:19:29 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 00:19:32 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 00:19:38 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 00:19:41 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 00:19:42 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 00:19:44 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 00:19:45 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 00:19:48 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 00:19:50 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 00:19:56 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 00:19:59 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 00:20:02 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 00:20:05 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 00:20:08 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 00:20:11 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 00:20:12 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:20:14 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:20:17 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:20:23 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:20:26 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:20:30 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:20:35 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000014_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000014_0/part-00014 (inode 3639278): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000014_0_-1774394211_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000014_0/part-00014 (inode 3639278): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000014_0_-1774394211_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:20:36 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:20:39 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:20:46 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:20:49 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:21:01 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:21:04 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:21:07 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:21:16 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 00:21:22 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:21:24 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000003_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000003_0/part-00003 (inode 3639351): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000003_0_130630275_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000003_0/part-00003 (inode 3639351): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000003_0_130630275_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:21:25 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:21:26 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000011_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000011_0/part-00011 (inode 3639344): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000011_0_-121658888_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000011_0/part-00011 (inode 3639344): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000011_0_-121658888_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:21:27 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:21:31 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000013_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000013_0/part-00013 (inode 3639359): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000013_0_1174053296_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000013_0/part-00013 (inode 3639359): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000013_0_1174053296_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:21:32 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:21:33 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:21:36 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:21:37 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:21:39 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:21:41 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:21:42 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:21:44 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:21:51 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:21:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000010_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000010_0/part-00010 (inode 3639346): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000010_0_-833357079_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000010_0/part-00010 (inode 3639346): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000010_0_-833357079_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:21:57 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:22:00 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:22:01 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:22:04 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:22:05 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000004_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000004_0/part-00004 (inode 3639363): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000004_0_-1799067024_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000004_0/part-00004 (inode 3639363): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000004_0_-1799067024_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:22:06 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:22:07 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:22:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000001_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000001_0/part-00001 (inode 3639361): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000001_0_-1439094704_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000001_0/part-00001 (inode 3639361): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000001_0_-1439094704_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:22:11 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:22:13 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:22:16 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:22:17 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:22:22 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:22:27 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:22:31 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:22:34 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:22:40 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:22:42 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000009_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000009_0/part-00009 (inode 3639355): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000009_0_-1499591062_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000009_0/part-00009 (inode 3639355): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000009_0_-1499591062_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:22:43 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:22:45 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000002_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000002_0/part-00002 (inode 3639357): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000002_0_1399181890_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000002_0/part-00002 (inode 3639357): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000002_0_1399181890_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:22:46 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 00:22:48 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:22:50 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:22:52 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:22:54 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:22:56 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:22:59 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:23:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000000_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000000_0/part-00000 (inode 3639353): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000000_0_693078169_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000000_0/part-00000 (inode 3639353): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000000_0_693078169_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:23:11 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:23:14 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:23:16 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:23:19 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:23:21 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 00:23:22 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:23:23 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:23:24 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:23:27 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:23:30 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 00:23:36 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:23:42 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 00:23:44 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 00:23:47 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 00:23:50 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 00:23:55 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 00:23:58 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 00:24:00 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 00:24:06 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 00:24:13 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 00:24:17 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 00:24:20 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 00:24:25 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:24:42 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 00:25:00 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 00:25:14 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 00:25:17 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000008_0, Status : FAILED
Error: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000008_0/part-00008 (inode 3639365): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000008_0_1805786582_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:334)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)
	at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)
	at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000008_0/part-00008 (inode 3639365): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000008_0_1805786582_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

15/04/14 00:25:18 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 00:25:28 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 00:25:38 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 00:26:01 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 00:26:12 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 00:26:21 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 00:26:29 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 00:26:42 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 00:26:48 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 00:27:08 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 00:27:21 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 00:27:36 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 00:27:51 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 00:27:54 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 00:28:01 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 00:28:24 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 00:28:36 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 00:28:48 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 00:29:13 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 00:29:45 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 00:30:46 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 00:31:05 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:31:39 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:32:33 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:33:57 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:34:17 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:34:53 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:35:24 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:38:00 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:39:10 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000008_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000008_2/part-00008 (inode 3639655): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000008_2_1224690542_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 00:39:17 INFO mapreduce.Job: Task Id : attempt_1422482982071_4810_r_000008_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4810_r_000008_1/part-00008 (inode 3639653): File does not exist. Holder DFSClient_attempt_1422482982071_4810_r_000008_1_1123780027_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 00:39:18 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 00:39:29 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 00:40:26 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 00:40:59 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 00:41:29 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 00:41:32 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:46:04 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 00:51:35 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:52:48 INFO mapreduce.Job: Job job_1422482982071_4810 completed successfully
15/04/14 00:52:48 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=47362322801
		FILE: Number of bytes written=94745343996
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=651
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Job Counters 
		Failed reduce tasks=13
		Killed reduce tasks=1
		Launched map tasks=202
		Launched reduce tasks=29
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=16909354
		Total time spent by all reduces in occupied slots (ms)=26570366
		Total time spent by all map tasks (ms)=8454677
		Total time spent by all reduce tasks (ms)=13285183
		Total vcore-seconds taken by all map tasks=8454677
		Total vcore-seconds taken by all reduce tasks=13285183
		Total megabyte-seconds taken by all map tasks=68449064992
		Total megabyte-seconds taken by all reduce tasks=159422196000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362340645
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362340645
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =3030
		Failed Shuffles=0
		Merged Map outputs=3030
		GC time elapsed (ms)=220243
		CPU time spent (ms)=21220230
		Physical memory (bytes) snapshot=428022226944
		Virtual memory (bytes) snapshot=2054497853440
		Total committed heap usage (bytes)=591803707392
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:52:48 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	35m5.072s
user	0m20.632s
sys	0m1.880s
15/04/14 00:52:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 10
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob5972048265828582367.jar tmpDir=null
15/04/14 00:52:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:52:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:52:54 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:52:55 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:52:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4821
15/04/14 00:52:55 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4821
15/04/14 00:52:56 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4821/
15/04/14 00:52:56 INFO mapreduce.Job: Running job: job_1422482982071_4821
15/04/14 00:53:00 INFO mapreduce.Job: Job job_1422482982071_4821 running in uber mode : false
15/04/14 00:53:00 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:53:10 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 00:53:11 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:53:12 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 00:53:13 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 00:53:14 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 00:53:15 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:53:16 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 00:53:17 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:53:18 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 00:53:19 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 00:53:20 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:53:21 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 00:53:22 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 00:53:23 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:53:24 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 00:53:25 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 00:53:26 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:53:27 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:53:28 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:54:14 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 00:54:15 INFO mapreduce.Job:  map 72% reduce 0%
15/04/14 00:54:16 INFO mapreduce.Job:  map 78% reduce 0%
15/04/14 00:54:17 INFO mapreduce.Job:  map 84% reduce 0%
15/04/14 00:54:18 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 00:54:19 INFO mapreduce.Job:  map 94% reduce 0%
15/04/14 00:54:20 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 00:54:21 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:54:22 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:54:25 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 00:54:26 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 00:54:27 INFO mapreduce.Job: Job job_1422482982071_4821 completed successfully
15/04/14 00:54:27 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95467015
		FILE: Number of bytes written=211194762
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=636
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=10
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=29961494
		Total time spent by all reduces in occupied slots (ms)=159956
		Total time spent by all map tasks (ms)=14980747
		Total time spent by all reduce tasks (ms)=79978
		Total vcore-seconds taken by all map tasks=14980747
		Total vcore-seconds taken by all reduce tasks=79978
		Total megabyte-seconds taken by all map tasks=121284127712
		Total megabyte-seconds taken by all reduce tasks=959736000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95479075
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95479075
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =2020
		Failed Shuffles=0
		Merged Map outputs=2020
		GC time elapsed (ms)=62812
		CPU time spent (ms)=14701990
		Physical memory (bytes) snapshot=395201634304
		Virtual memory (bytes) snapshot=1987649318912
		Total committed heap usage (bytes)=580505890816
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 00:54:27 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m39.427s
user	0m11.572s
sys	0m0.636s
15/04/14 00:54:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 10
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-10-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=10 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob8781476509108581733.jar tmpDir=null
15/04/14 00:54:33 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:54:33 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 00:54:33 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 00:54:34 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 00:54:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4822
15/04/14 00:54:35 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4822
15/04/14 00:54:35 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4822/
15/04/14 00:54:35 INFO mapreduce.Job: Running job: job_1422482982071_4822
15/04/14 00:54:40 INFO mapreduce.Job: Job job_1422482982071_4822 running in uber mode : false
15/04/14 00:54:40 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 00:54:50 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 00:54:51 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 00:54:52 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 00:54:53 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 00:54:54 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 00:54:55 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 00:54:56 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 00:54:57 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 00:54:58 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 00:54:59 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 00:55:00 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 00:55:01 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 00:55:02 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 00:55:03 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 00:55:04 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 00:55:05 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 00:55:07 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 00:55:08 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 00:55:23 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 00:55:24 INFO mapreduce.Job:  map 75% reduce 0%
15/04/14 00:55:25 INFO mapreduce.Job:  map 80% reduce 0%
15/04/14 00:55:26 INFO mapreduce.Job:  map 85% reduce 0%
15/04/14 00:55:27 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 00:55:28 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 00:55:29 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 00:55:30 INFO mapreduce.Job:  map 98% reduce 0%
15/04/14 00:55:31 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 00:55:32 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 00:55:34 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 00:55:58 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 00:56:01 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 00:56:07 INFO mapreduce.Job:  map 100% reduce 16%
15/04/14 00:56:10 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 00:56:22 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 00:56:26 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 00:56:28 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 00:56:29 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 00:56:32 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 00:56:38 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 00:56:41 INFO mapreduce.Job:  map 100% reduce 27%
15/04/14 00:56:44 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 00:56:53 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 00:56:56 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 00:56:59 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 00:57:02 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 00:57:11 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 00:57:14 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 00:57:17 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 00:57:20 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 00:57:23 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 00:57:24 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 00:57:26 INFO mapreduce.Job:  map 100% reduce 45%
15/04/14 00:57:27 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 00:57:29 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 00:57:32 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 00:57:33 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 00:57:35 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 00:57:36 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 00:57:38 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 00:57:47 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 00:57:50 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 00:57:53 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 00:57:56 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 00:57:59 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 00:58:05 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 00:58:11 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 00:58:21 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 00:58:30 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 00:58:37 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 00:58:45 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 00:58:48 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 00:58:54 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 00:59:07 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 00:59:25 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 00:59:36 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 01:00:00 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 01:00:15 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 01:00:18 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 01:00:21 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 01:00:39 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 01:01:03 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 01:01:34 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 01:01:37 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 01:01:58 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 01:02:22 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 01:02:52 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 01:03:10 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 01:03:41 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 01:03:59 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 01:04:26 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 01:04:53 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 01:05:38 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 01:05:51 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:06:39 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:06:40 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:07:00 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:07:21 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:07:57 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:09:37 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:10:05 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:11:53 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:14:06 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 01:15:54 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 01:16:11 INFO mapreduce.Job: Task Id : attempt_1422482982071_4822_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4822_r_000000_0/part-00000 (inode 3640317): File does not exist. Holder DFSClient_attempt_1422482982071_4822_r_000000_0_-420870565_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 01:16:12 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:16:22 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:17:50 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:18:56 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:19:34 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:19:46 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:19:49 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 01:19:55 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:22:52 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:23:35 INFO mapreduce.Job: Task Id : attempt_1422482982071_4822_r_000002_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4822_r_000002_0/part-00002 (inode 3640321): File does not exist. Holder DFSClient_attempt_1422482982071_4822_r_000002_0_-1944990629_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 01:23:36 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 01:24:34 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:26:02 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:27:27 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:28:24 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:28:27 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:29:43 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:31:18 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 01:32:21 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:34:33 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:36:47 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 01:37:27 INFO mapreduce.Job: Task Id : attempt_1422482982071_4822_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4822_r_000000_1/part-00000 (inode 3640614): File does not exist. Holder DFSClient_attempt_1422482982071_4822_r_000000_1_-1282948778_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 01:37:28 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:38:12 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 01:38:48 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:40:01 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:40:38 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:40:50 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:40:59 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 01:41:05 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 01:44:33 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:51:15 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 01:51:56 INFO mapreduce.Job: Task Id : attempt_1422482982071_4822_r_000002_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4822_r_000002_2/part-00002 (inode 3640618): File does not exist. Holder DFSClient_attempt_1422482982071_4822_r_000002_2_-467303666_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 01:52:07 INFO mapreduce.Job: Task Id : attempt_1422482982071_4822_r_000002_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-1272212113-129.114.57.132-1408108439175:blk_1077961365_4220706 does not exist or is not under Constructionnull
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:5956)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:874)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:826)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

15/04/14 01:52:08 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 01:52:45 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 01:53:22 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 01:54:10 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 01:55:29 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 01:56:27 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 01:56:30 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 01:56:33 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 01:58:34 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 02:03:23 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 02:12:33 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 02:19:42 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 02:20:27 INFO mapreduce.Job: Job job_1422482982071_4822 failed with state FAILED due to: Task failed task_1422482982071_4822_r_000002
Job failed as tasks failed. failedMaps:0 failedReduces:1

15/04/14 02:20:27 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=38162264650
		FILE: Number of bytes written=85544685942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4162544
		HDFS: Number of read operations=633
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Job Counters 
		Failed reduce tasks=6
		Launched map tasks=202
		Launched reduce tasks=15
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=17327854
		Total time spent by all reduces in occupied slots (ms)=32430230
		Total time spent by all map tasks (ms)=8663927
		Total time spent by all reduce tasks (ms)=16215115
		Total vcore-seconds taken by all map tasks=8663927
		Total vcore-seconds taken by all reduce tasks=16215115
		Total megabyte-seconds taken by all map tasks=70143152992
		Total megabyte-seconds taken by all reduce tasks=194581380000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362334585
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=286256
		Reduce shuffle bytes=38162275306
		Reduce input records=4073496473
		Reduce output records=286256
		Spilled Records=9137284097
		Shuffled Maps =1818
		Failed Shuffles=0
		Merged Map outputs=1818
		GC time elapsed (ms)=183108
		CPU time spent (ms)=19023860
		Physical memory (bytes) snapshot=415902736384
		Virtual memory (bytes) snapshot=1974561968128
		Total committed heap usage (bytes)=579576197120
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4162544
15/04/14 02:20:27 ERROR streaming.StreamJob: Job not Successful!
Streaming Command Failed!

real	85m59.261s
user	0m28.549s
sys	0m3.730s
15/04/14 02:20:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 5
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob488586198821981976.jar tmpDir=null
15/04/14 02:20:32 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:20:33 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:20:34 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 02:20:34 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 02:20:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4833
15/04/14 02:20:35 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4833
15/04/14 02:20:35 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4833/
15/04/14 02:20:35 INFO mapreduce.Job: Running job: job_1422482982071_4833
15/04/14 02:20:39 INFO mapreduce.Job: Job job_1422482982071_4833 running in uber mode : false
15/04/14 02:20:39 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 02:20:49 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 02:20:50 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 02:20:51 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 02:20:52 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 02:20:53 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 02:20:54 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 02:20:55 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 02:20:56 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 02:20:57 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 02:20:58 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 02:20:59 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 02:21:00 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 02:21:01 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 02:21:02 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 02:21:03 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 02:21:04 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 02:21:05 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 02:21:06 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 02:21:10 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 02:21:20 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 02:21:53 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 02:21:54 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 02:21:55 INFO mapreduce.Job:  map 77% reduce 0%
15/04/14 02:21:56 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 02:21:57 INFO mapreduce.Job:  map 84% reduce 0%
15/04/14 02:21:58 INFO mapreduce.Job:  map 88% reduce 0%
15/04/14 02:21:59 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 02:22:01 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 02:22:02 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 02:22:05 INFO mapreduce.Job:  map 99% reduce 32%
15/04/14 02:22:11 INFO mapreduce.Job:  map 99% reduce 33%
15/04/14 02:22:14 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 02:22:23 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 02:22:26 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 02:22:26 INFO mapreduce.Job: Job job_1422482982071_4833 completed successfully
15/04/14 02:22:26 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95466985
		FILE: Number of bytes written=210687753
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=621
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=5
		Data-local map tasks=149
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=30432860
		Total time spent by all reduces in occupied slots (ms)=291862
		Total time spent by all map tasks (ms)=15216430
		Total time spent by all reduce tasks (ms)=145931
		Total vcore-seconds taken by all map tasks=15216430
		Total vcore-seconds taken by all reduce tasks=145931
		Total megabyte-seconds taken by all map tasks=123192217280
		Total megabyte-seconds taken by all reduce tasks=1751172000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95473015
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95473015
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =1010
		Failed Shuffles=0
		Merged Map outputs=1010
		GC time elapsed (ms)=66400
		CPU time spent (ms)=16644130
		Physical memory (bytes) snapshot=394252275712
		Virtual memory (bytes) snapshot=1921237577728
		Total committed heap usage (bytes)=569980907520
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 02:22:26 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m59.111s
user	0m12.252s
sys	0m0.687s
15/04/14 02:22:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 5
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-5-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=5 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2178105638153901758.jar tmpDir=null
15/04/14 02:22:31 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:22:31 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 02:22:32 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 02:22:33 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 02:22:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4834
15/04/14 02:22:33 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4834
15/04/14 02:22:34 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4834/
15/04/14 02:22:34 INFO mapreduce.Job: Running job: job_1422482982071_4834
15/04/14 02:22:38 INFO mapreduce.Job: Job job_1422482982071_4834 running in uber mode : false
15/04/14 02:22:38 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 02:22:48 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 02:22:49 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 02:22:50 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 02:22:51 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 02:22:52 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 02:22:53 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 02:22:54 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 02:22:55 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 02:22:56 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 02:22:57 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 02:22:58 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 02:22:59 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 02:23:00 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 02:23:01 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 02:23:02 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 02:23:03 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 02:23:04 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 02:23:05 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 02:23:06 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 02:23:20 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 02:23:21 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 02:23:22 INFO mapreduce.Job:  map 72% reduce 0%
15/04/14 02:23:23 INFO mapreduce.Job:  map 78% reduce 0%
15/04/14 02:23:24 INFO mapreduce.Job:  map 83% reduce 0%
15/04/14 02:23:25 INFO mapreduce.Job:  map 86% reduce 0%
15/04/14 02:23:26 INFO mapreduce.Job:  map 89% reduce 0%
15/04/14 02:23:27 INFO mapreduce.Job:  map 93% reduce 0%
15/04/14 02:23:28 INFO mapreduce.Job:  map 97% reduce 0%
15/04/14 02:23:29 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 02:23:31 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 02:23:32 INFO mapreduce.Job:  map 100% reduce 4%
15/04/14 02:23:35 INFO mapreduce.Job:  map 100% reduce 5%
15/04/14 02:23:59 INFO mapreduce.Job:  map 100% reduce 7%
15/04/14 02:24:02 INFO mapreduce.Job:  map 100% reduce 8%
15/04/14 02:24:14 INFO mapreduce.Job:  map 100% reduce 9%
15/04/14 02:24:23 INFO mapreduce.Job:  map 100% reduce 10%
15/04/14 02:24:29 INFO mapreduce.Job:  map 100% reduce 11%
15/04/14 02:24:32 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 02:24:41 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 02:24:51 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 02:25:00 INFO mapreduce.Job:  map 100% reduce 15%
15/04/14 02:25:06 INFO mapreduce.Job:  map 100% reduce 16%
15/04/14 02:25:09 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 02:25:22 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 02:25:30 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 02:25:36 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 02:25:42 INFO mapreduce.Job:  map 100% reduce 21%
15/04/14 02:25:49 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 02:26:01 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 02:26:04 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 02:26:07 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 02:26:13 INFO mapreduce.Job:  map 100% reduce 26%
15/04/14 02:26:16 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 02:26:19 INFO mapreduce.Job:  map 100% reduce 29%
15/04/14 02:26:25 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 02:26:28 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 02:26:31 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 02:26:34 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 02:26:55 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 02:27:01 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 02:27:10 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 02:27:22 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 02:27:31 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 02:27:34 INFO mapreduce.Job:  map 100% reduce 40%
15/04/14 02:27:37 INFO mapreduce.Job:  map 100% reduce 41%
15/04/14 02:27:40 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 02:27:43 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 02:27:46 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 02:27:49 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 02:27:52 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 02:27:55 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 02:27:58 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 02:28:01 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 02:28:04 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 02:28:08 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 02:28:11 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 02:28:33 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 02:29:00 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 02:29:03 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 02:29:06 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 02:29:09 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 02:29:12 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 02:29:15 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 02:29:48 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 02:30:18 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 02:31:19 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 02:32:09 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 02:33:09 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 02:34:37 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 02:35:36 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 02:37:47 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 02:38:12 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 02:39:40 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 02:39:41 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 02:40:04 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 02:40:37 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 02:41:17 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 02:42:07 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 02:42:12 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 02:42:40 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 02:43:12 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 02:44:37 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 02:45:37 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 02:46:58 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 02:47:10 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 02:48:00 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 02:48:54 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 02:49:51 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 02:50:48 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 02:50:53 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 02:51:47 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 02:52:50 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 02:56:43 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 03:00:15 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 03:01:29 INFO mapreduce.Job: Job job_1422482982071_4834 completed successfully
15/04/14 03:01:29 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322759
		FILE: Number of bytes written=94744333605
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=621
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Job Counters 
		Killed map tasks=1
		Launched map tasks=203
		Launched reduce tasks=5
		Data-local map tasks=150
		Rack-local map tasks=53
		Total time spent by all maps in occupied slots (ms)=17777316
		Total time spent by all reduces in occupied slots (ms)=17470096
		Total time spent by all map tasks (ms)=8888658
		Total time spent by all reduce tasks (ms)=8735048
		Total vcore-seconds taken by all map tasks=8888658
		Total vcore-seconds taken by all reduce tasks=8735048
		Total megabyte-seconds taken by all map tasks=71962575168
		Total megabyte-seconds taken by all reduce tasks=104820576000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362328525
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362328525
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =1010
		Failed Shuffles=0
		Merged Map outputs=1010
		GC time elapsed (ms)=245998
		CPU time spent (ms)=21497450
		Physical memory (bytes) snapshot=407455047680
		Virtual memory (bytes) snapshot=1921617104896
		Total committed heap usage (bytes)=572270596096
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 03:01:29 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	39m2.952s
user	0m21.635s
sys	0m1.926s
15/04/14 03:01:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 4
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py -combiner ./reducer.py -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob2807658766155841141.jar tmpDir=null
15/04/14 03:01:34 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:01:35 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:01:35 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 03:01:36 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 03:01:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4837
15/04/14 03:01:36 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4837
15/04/14 03:01:37 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4837/
15/04/14 03:01:37 INFO mapreduce.Job: Running job: job_1422482982071_4837
15/04/14 03:01:42 INFO mapreduce.Job: Job job_1422482982071_4837 running in uber mode : false
15/04/14 03:01:42 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 03:01:52 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 03:01:53 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 03:01:54 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 03:01:55 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 03:01:56 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 03:01:57 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 03:01:58 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 03:01:59 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 03:02:00 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 03:02:01 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 03:02:02 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 03:02:03 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 03:02:04 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 03:02:05 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 03:02:06 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 03:02:07 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 03:02:08 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 03:02:09 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 03:02:10 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 03:02:56 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 03:02:57 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 03:02:58 INFO mapreduce.Job:  map 76% reduce 0%
15/04/14 03:02:59 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 03:03:00 INFO mapreduce.Job:  map 86% reduce 0%
15/04/14 03:03:01 INFO mapreduce.Job:  map 91% reduce 0%
15/04/14 03:03:02 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 03:03:03 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 03:03:04 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 03:03:07 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 03:03:10 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 03:03:10 INFO mapreduce.Job: Job job_1422482982071_4837 completed successfully
15/04/14 03:03:10 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=95466979
		FILE: Number of bytes written=210586436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=618
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=202
		Launched reduce tasks=4
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=30480222
		Total time spent by all reduces in occupied slots (ms)=88296
		Total time spent by all map tasks (ms)=15240111
		Total time spent by all reduce tasks (ms)=44148
		Total vcore-seconds taken by all map tasks=15240111
		Total vcore-seconds taken by all reduce tasks=44148
		Total megabyte-seconds taken by all map tasks=123383938656
		Total megabyte-seconds taken by all reduce tasks=529776000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=95471803
		Input split bytes=31916
		Combine input records=5063787624
		Combine output records=6174842
		Reduce input groups=317917
		Reduce shuffle bytes=95471803
		Reduce input records=6174842
		Reduce output records=317917
		Spilled Records=12349684
		Shuffled Maps =808
		Failed Shuffles=0
		Merged Map outputs=808
		GC time elapsed (ms)=62564
		CPU time spent (ms)=16782780
		Physical memory (bytes) snapshot=393904836608
		Virtual memory (bytes) snapshot=1907701153792
		Total committed heap usage (bytes)=567874461696
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 03:03:10 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	1m41.702s
user	0m11.512s
sys	0m0.652s
15/04/14 03:03:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
50 4
py googlebooks-eng-all-5gram-20120701-on mapper.py reducer.py
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=py-50-4-googlebooks-eng-all-5gram-20120701-on -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=4 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.py,./reducer.py  -mapper ./mapper.py -reducer ./reducer.py -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-on -output ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob6893962965802345770.jar tmpDir=null
15/04/14 03:03:16 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:03:16 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 03:03:17 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 03:03:17 INFO mapreduce.JobSubmitter: number of splits:202
15/04/14 03:03:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4838
15/04/14 03:03:18 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4838
15/04/14 03:03:18 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4838/
15/04/14 03:03:18 INFO mapreduce.Job: Running job: job_1422482982071_4838
15/04/14 03:03:22 INFO mapreduce.Job: Job job_1422482982071_4838 running in uber mode : false
15/04/14 03:03:22 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 03:03:33 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 03:03:34 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 03:03:35 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 03:03:36 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 03:03:37 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 03:03:38 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 03:03:39 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 03:03:40 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 03:03:41 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 03:03:42 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 03:03:43 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 03:03:44 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 03:03:45 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 03:03:46 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 03:03:47 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 03:03:48 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 03:03:49 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 03:03:50 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 03:03:51 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 03:03:52 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 03:04:07 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 03:04:08 INFO mapreduce.Job:  map 73% reduce 0%
15/04/14 03:04:09 INFO mapreduce.Job:  map 78% reduce 0%
15/04/14 03:04:10 INFO mapreduce.Job:  map 82% reduce 0%
15/04/14 03:04:11 INFO mapreduce.Job:  map 87% reduce 0%
15/04/14 03:04:12 INFO mapreduce.Job:  map 92% reduce 0%
15/04/14 03:04:13 INFO mapreduce.Job:  map 96% reduce 0%
15/04/14 03:04:14 INFO mapreduce.Job:  map 99% reduce 0%
15/04/14 03:04:16 INFO mapreduce.Job:  map 100% reduce 0%
15/04/14 03:04:18 INFO mapreduce.Job:  map 100% reduce 4%
15/04/14 03:04:45 INFO mapreduce.Job:  map 100% reduce 6%
15/04/14 03:04:48 INFO mapreduce.Job:  map 100% reduce 7%
15/04/14 03:05:09 INFO mapreduce.Job:  map 100% reduce 8%
15/04/14 03:05:16 INFO mapreduce.Job:  map 100% reduce 10%
15/04/14 03:05:37 INFO mapreduce.Job:  map 100% reduce 11%
15/04/14 03:05:40 INFO mapreduce.Job:  map 100% reduce 12%
15/04/14 03:05:46 INFO mapreduce.Job:  map 100% reduce 13%
15/04/14 03:06:04 INFO mapreduce.Job:  map 100% reduce 14%
15/04/14 03:06:14 INFO mapreduce.Job:  map 100% reduce 16%
15/04/14 03:06:29 INFO mapreduce.Job:  map 100% reduce 17%
15/04/14 03:06:41 INFO mapreduce.Job:  map 100% reduce 18%
15/04/14 03:06:44 INFO mapreduce.Job:  map 100% reduce 19%
15/04/14 03:06:53 INFO mapreduce.Job:  map 100% reduce 20%
15/04/14 03:07:11 INFO mapreduce.Job:  map 100% reduce 22%
15/04/14 03:07:20 INFO mapreduce.Job:  map 100% reduce 23%
15/04/14 03:07:38 INFO mapreduce.Job:  map 100% reduce 24%
15/04/14 03:07:41 INFO mapreduce.Job:  map 100% reduce 25%
15/04/14 03:07:47 INFO mapreduce.Job:  map 100% reduce 28%
15/04/14 03:07:50 INFO mapreduce.Job:  map 100% reduce 30%
15/04/14 03:07:53 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 03:07:56 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 03:08:11 INFO mapreduce.Job:  map 100% reduce 35%
15/04/14 03:08:14 INFO mapreduce.Job:  map 100% reduce 36%
15/04/14 03:08:32 INFO mapreduce.Job:  map 100% reduce 37%
15/04/14 03:08:59 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 03:09:05 INFO mapreduce.Job:  map 100% reduce 43%
15/04/14 03:09:08 INFO mapreduce.Job:  map 100% reduce 46%
15/04/14 03:09:14 INFO mapreduce.Job:  map 100% reduce 47%
15/04/14 03:09:24 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 03:09:27 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 03:09:30 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 03:09:42 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 03:10:12 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 03:10:42 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 03:11:09 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 03:11:40 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 03:12:07 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 03:12:37 INFO mapreduce.Job:  map 100% reduce 62%
15/04/14 03:12:40 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 03:12:43 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 03:12:47 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 03:12:50 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 03:12:53 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 03:12:56 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 03:12:59 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 03:13:14 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 03:14:14 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 03:16:30 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 03:18:21 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 03:19:01 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 03:20:08 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 03:20:56 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 03:21:59 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 03:22:55 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 03:23:54 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 03:24:31 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 03:25:20 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 03:26:07 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 03:26:11 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 03:26:51 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 03:27:22 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 03:28:10 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 03:29:53 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 03:31:55 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 03:33:01 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 03:33:43 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 03:34:07 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 03:35:04 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 03:36:09 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 03:36:16 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 03:36:16 INFO mapreduce.Job: Task Id : attempt_1422482982071_4838_r_000001_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4838_r_000001_0/part-00001 (inode 3641828): File does not exist. Holder DFSClient_attempt_1422482982071_4838_r_000001_0_1937082549_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 03:36:27 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 03:36:54 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 03:37:27 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 03:37:28 INFO mapreduce.Job: Task Id : attempt_1422482982071_4838_r_000002_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4838_r_000002_0/part-00002 (inode 3641830): File does not exist. Holder DFSClient_attempt_1422482982071_4838_r_000002_0_-1518032149_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 03:37:29 INFO mapreduce.Job:  map 100% reduce 48%
15/04/14 03:37:39 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 03:37:57 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 03:38:07 INFO mapreduce.Job:  map 100% reduce 51%
15/04/14 03:38:28 INFO mapreduce.Job:  map 100% reduce 52%
15/04/14 03:38:40 INFO mapreduce.Job:  map 100% reduce 53%
15/04/14 03:39:01 INFO mapreduce.Job:  map 100% reduce 54%
15/04/14 03:39:28 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 03:39:37 INFO mapreduce.Job:  map 100% reduce 56%
15/04/14 03:39:59 INFO mapreduce.Job:  map 100% reduce 57%
15/04/14 03:40:11 INFO mapreduce.Job:  map 100% reduce 58%
15/04/14 03:40:38 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 03:40:56 INFO mapreduce.Job:  map 100% reduce 60%
15/04/14 03:41:09 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 03:41:23 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 03:41:26 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 03:41:39 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 03:42:09 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 03:42:39 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 03:43:06 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 03:43:09 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 03:43:12 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 03:43:39 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 03:45:25 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 03:47:40 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 03:49:24 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 03:52:04 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 03:52:34 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 03:52:55 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 03:54:36 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 03:55:47 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 03:56:39 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 03:57:46 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 03:59:08 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 03:59:20 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 04:00:35 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 04:01:19 INFO mapreduce.Job: Task Id : attempt_1422482982071_4838_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4838_r_000000_0/part-00000 (inode 3641832): File does not exist. Holder DFSClient_attempt_1422482982071_4838_r_000000_0_-1964552073_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 04:01:20 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 04:01:30 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 04:02:25 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 04:03:16 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 04:03:30 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 04:04:17 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 04:04:44 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 04:05:39 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 04:06:15 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 04:07:06 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 04:07:23 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 04:07:45 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 04:08:18 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 04:09:05 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 04:09:52 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 04:10:38 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 04:10:41 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 04:10:44 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 04:10:47 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 04:10:50 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 04:10:53 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 04:10:56 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 04:10:59 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 04:24:09 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 04:25:03 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 04:31:15 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 04:35:58 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 04:41:31 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 04:47:31 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 04:53:32 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 04:57:00 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 04:59:18 INFO mapreduce.Job: Task Id : attempt_1422482982071_4838_r_000000_1, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4838_r_000000_1/part-00000 (inode 3642246): File does not exist. Holder DFSClient_attempt_1422482982071_4838_r_000000_1_235973031_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:00:18 INFO mapreduce.Job: Task Id : attempt_1422482982071_4838_r_000000_2, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dotcz12/out/stream-py-0-googlebooks-eng-all-5gram-20120701-on/_temporary/1/_temporary/attempt_1422482982071_4838_r_000000_2/part-00000 (inode 3642248): File does not exist. Holder DFSClient_attempt_1422482982071_4838_r_000000_2_1162962084_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3077)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3047)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:628)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:406)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)
	at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:00:19 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 05:00:29 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 05:01:33 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 05:02:39 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 05:03:45 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 05:04:46 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 05:05:46 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 05:06:43 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 05:07:48 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 05:08:54 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 05:09:00 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 05:09:03 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 05:09:06 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 05:09:09 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 05:09:12 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 05:09:15 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 05:09:18 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 05:09:21 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 05:22:36 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 05:23:32 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 05:29:28 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 05:34:22 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 05:39:44 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 05:45:42 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 05:51:15 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 05:54:34 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 05:56:47 INFO mapreduce.Job: Job job_1422482982071_4838 completed successfully
15/04/14 05:56:48 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=47362322759
		FILE: Number of bytes written=94744232614
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=27108027181
		HDFS: Number of bytes written=4622792
		HDFS: Number of read operations=618
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Failed reduce tasks=5
		Launched map tasks=202
		Launched reduce tasks=9
		Data-local map tasks=148
		Rack-local map tasks=54
		Total time spent by all maps in occupied slots (ms)=17844180
		Total time spent by all reduces in occupied slots (ms)=46053400
		Total time spent by all map tasks (ms)=8922090
		Total time spent by all reduce tasks (ms)=23026700
		Total vcore-seconds taken by all map tasks=8922090
		Total vcore-seconds taken by all reduce tasks=23026700
		Total megabyte-seconds taken by all map tasks=72233240640
		Total megabyte-seconds taken by all reduce tasks=276320400000
	Map-Reduce Framework
		Map input records=632973453
		Map output records=5063787624
		Map output bytes=37234747217
		Map output materialized bytes=47362327313
		Input split bytes=31916
		Combine input records=0
		Combine output records=0
		Reduce input groups=317917
		Reduce shuffle bytes=47362327313
		Reduce input records=5063787624
		Reduce output records=317917
		Spilled Records=10127575248
		Shuffled Maps =808
		Failed Shuffles=0
		Merged Map outputs=808
		GC time elapsed (ms)=265262
		CPU time spent (ms)=21463340
		Physical memory (bytes) snapshot=405660094464
		Virtual memory (bytes) snapshot=1907706830848
		Total committed heap usage (bytes)=570834432000
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27107995265
	File Output Format Counters 
		Bytes Written=4622792
15/04/14 05:56:48 INFO streaming.StreamJob: Output directory: ./out/stream-py-0-googlebooks-eng-all-5gram-20120701-on

real	173m37.136s
user	0m36.111s
sys	0m7.213s
15/04/14 05:56:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
50 35
r googlebooks-eng-all-5gram-20120701-st mapper.R reducer.R
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=r-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.R,./reducer.R -combiner ./reducer.R -mapper ./mapper.R -reducer ./reducer.R -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob3773889003504137841.jar tmpDir=null
15/04/14 05:56:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 05:56:53 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 05:56:54 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 05:56:54 INFO mapreduce.JobSubmitter: number of splits:134
15/04/14 05:56:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4850
15/04/14 05:56:55 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4850
15/04/14 05:56:55 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4850/
15/04/14 05:56:55 INFO mapreduce.Job: Running job: job_1422482982071_4850
15/04/14 05:56:59 INFO mapreduce.Job: Job job_1422482982071_4850 running in uber mode : false
15/04/14 05:56:59 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 05:57:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4850_m_000035_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:57:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4850_m_000014_0, Status : FAILED
Error: java.io.IOException: Broken pipe
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:345)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

15/04/14 05:57:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4850_m_000028_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:57:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4850_m_000076_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:57:04 INFO mapreduce.Job: Task Id : attempt_1422482982071_4850_m_000100_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 05:57:10 INFO mapreduce.Job:  map 1% reduce 0%
15/04/14 05:57:16 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 05:57:25 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 05:57:31 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 05:57:38 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 05:57:47 INFO mapreduce.Job:  map 6% reduce 0%
15/04/14 05:57:54 INFO mapreduce.Job:  map 7% reduce 0%
15/04/14 05:58:01 INFO mapreduce.Job:  map 8% reduce 0%
15/04/14 05:58:08 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 05:58:15 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 05:58:22 INFO mapreduce.Job:  map 11% reduce 0%
15/04/14 05:58:29 INFO mapreduce.Job:  map 12% reduce 0%
15/04/14 05:58:36 INFO mapreduce.Job:  map 13% reduce 0%
15/04/14 05:58:44 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 05:58:51 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 05:58:58 INFO mapreduce.Job:  map 16% reduce 0%
15/04/14 05:59:05 INFO mapreduce.Job:  map 17% reduce 0%
15/04/14 05:59:12 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 05:59:20 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 05:59:27 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 05:59:33 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 05:59:41 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 05:59:48 INFO mapreduce.Job:  map 23% reduce 0%
15/04/14 05:59:54 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 06:00:02 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 06:00:09 INFO mapreduce.Job:  map 26% reduce 0%
15/04/14 06:00:16 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 06:00:23 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 06:00:30 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 06:00:38 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 06:00:46 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 06:00:53 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 06:01:00 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 06:01:07 INFO mapreduce.Job:  map 34% reduce 0%
15/04/14 06:01:14 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 06:01:22 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 06:01:29 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 06:01:36 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 06:01:43 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 06:01:50 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 06:01:58 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 06:02:05 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 06:02:12 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 06:02:19 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 06:02:26 INFO mapreduce.Job:  map 45% reduce 0%
15/04/14 06:02:34 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 06:02:40 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 06:02:48 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 06:02:55 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 06:03:02 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 06:03:10 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 06:03:16 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 06:03:25 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 06:03:31 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 06:03:38 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 06:03:46 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 06:03:52 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 06:04:00 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 06:04:07 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 06:04:15 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 06:04:23 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 06:04:28 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 06:04:35 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 06:04:43 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 06:04:50 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 06:04:59 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 06:05:10 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 06:15:14 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 06:15:30 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 06:15:41 INFO mapreduce.Job:  map 69% reduce 2%
15/04/14 06:15:42 INFO mapreduce.Job:  map 69% reduce 3%
15/04/14 06:15:43 INFO mapreduce.Job:  map 70% reduce 3%
15/04/14 06:15:54 INFO mapreduce.Job:  map 71% reduce 4%
15/04/14 06:16:00 INFO mapreduce.Job:  map 71% reduce 5%
15/04/14 06:16:01 INFO mapreduce.Job:  map 72% reduce 5%
15/04/14 06:16:03 INFO mapreduce.Job:  map 73% reduce 5%
15/04/14 06:16:04 INFO mapreduce.Job:  map 74% reduce 5%
15/04/14 06:16:06 INFO mapreduce.Job:  map 75% reduce 7%
15/04/14 06:16:07 INFO mapreduce.Job:  map 75% reduce 8%
15/04/14 06:16:09 INFO mapreduce.Job:  map 76% reduce 8%
15/04/14 06:16:12 INFO mapreduce.Job:  map 77% reduce 9%
15/04/14 06:16:13 INFO mapreduce.Job:  map 78% reduce 10%
15/04/14 06:16:15 INFO mapreduce.Job:  map 79% reduce 11%
15/04/14 06:16:16 INFO mapreduce.Job:  map 79% reduce 12%
15/04/14 06:16:17 INFO mapreduce.Job:  map 80% reduce 12%
15/04/14 06:16:18 INFO mapreduce.Job:  map 80% reduce 13%
15/04/14 06:16:19 INFO mapreduce.Job:  map 82% reduce 14%
15/04/14 06:16:20 INFO mapreduce.Job:  map 83% reduce 14%
15/04/14 06:16:21 INFO mapreduce.Job:  map 83% reduce 15%
15/04/14 06:16:22 INFO mapreduce.Job:  map 83% reduce 16%
15/04/14 06:16:23 INFO mapreduce.Job:  map 84% reduce 16%
15/04/14 06:16:24 INFO mapreduce.Job:  map 85% reduce 17%
15/04/14 06:16:27 INFO mapreduce.Job:  map 85% reduce 18%
15/04/14 06:16:28 INFO mapreduce.Job:  map 86% reduce 18%
15/04/14 06:16:30 INFO mapreduce.Job:  map 87% reduce 19%
15/04/14 06:16:31 INFO mapreduce.Job:  map 87% reduce 20%
15/04/14 06:16:32 INFO mapreduce.Job:  map 88% reduce 20%
15/04/14 06:16:34 INFO mapreduce.Job:  map 89% reduce 21%
15/04/14 06:16:35 INFO mapreduce.Job:  map 90% reduce 21%
15/04/14 06:16:36 INFO mapreduce.Job:  map 90% reduce 23%
15/04/14 06:16:38 INFO mapreduce.Job:  map 91% reduce 23%
15/04/14 06:16:39 INFO mapreduce.Job:  map 91% reduce 24%
15/04/14 06:16:40 INFO mapreduce.Job:  map 92% reduce 24%
15/04/14 06:16:42 INFO mapreduce.Job:  map 94% reduce 25%
15/04/14 06:16:43 INFO mapreduce.Job:  map 94% reduce 26%
15/04/14 06:16:44 INFO mapreduce.Job:  map 95% reduce 26%
15/04/14 06:16:45 INFO mapreduce.Job:  map 96% reduce 28%
15/04/14 06:16:47 INFO mapreduce.Job:  map 96% reduce 29%
15/04/14 06:16:52 INFO mapreduce.Job:  map 97% reduce 29%
15/04/14 06:16:53 INFO mapreduce.Job:  map 97% reduce 30%
15/04/14 06:17:00 INFO mapreduce.Job:  map 98% reduce 30%
15/04/14 06:17:01 INFO mapreduce.Job:  map 98% reduce 31%
15/04/14 06:17:03 INFO mapreduce.Job:  map 99% reduce 31%
15/04/14 06:17:04 INFO mapreduce.Job:  map 99% reduce 32%
15/04/14 06:17:07 INFO mapreduce.Job:  map 99% reduce 33%
15/04/14 06:17:13 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 06:17:27 INFO mapreduce.Job:  map 100% reduce 42%
15/04/14 06:17:28 INFO mapreduce.Job:  map 100% reduce 63%
15/04/14 06:17:29 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 06:17:30 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 06:17:31 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 06:17:32 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 06:17:32 INFO mapreduce.Job: Job job_1422482982071_4850 completed successfully
15/04/14 06:17:32 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=70110743
		FILE: Number of bytes written=156461704
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4732772
		HDFS: Number of read operations=507
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=70
	Job Counters 
		Failed map tasks=5
		Launched map tasks=139
		Launched reduce tasks=35
		Other local map tasks=5
		Data-local map tasks=61
		Rack-local map tasks=73
		Total time spent by all maps in occupied slots (ms)=309364098
		Total time spent by all reduces in occupied slots (ms)=8272536
		Total time spent by all map tasks (ms)=154682049
		Total time spent by all reduce tasks (ms)=4136268
		Total vcore-seconds taken by all map tasks=154682049
		Total vcore-seconds taken by all reduce tasks=4136268
		Total megabyte-seconds taken by all map tasks=1252305868704
		Total megabyte-seconds taken by all reduce tasks=49635216000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=70138673
		Input split bytes=21172
		Combine input records=3130842760
		Combine output records=4478218
		Reduce input groups=325361
		Reduce shuffle bytes=70138673
		Reduce input records=4478218
		Reduce output records=325361
		Spilled Records=8956436
		Shuffled Maps =4690
		Failed Shuffles=0
		Merged Map outputs=4690
		GC time elapsed (ms)=191153
		CPU time spent (ms)=174363820
		Physical memory (bytes) snapshot=268162797568
		Virtual memory (bytes) snapshot=1696844812288
		Total committed heap usage (bytes)=444818288640
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4732772
15/04/14 06:17:32 INFO streaming.StreamJob: Output directory: ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st

real	20m44.571s
user	0m16.550s
sys	0m1.332s
15/04/14 06:17:35 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
50 35
r googlebooks-eng-all-5gram-20120701-st mapper.R reducer.R
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -Dmapreduce.job.name=r-50-35-googlebooks-eng-all-5gram-20120701-st -Dmapreduce.job.maps=50 -Dmapreduce.job.reduces=35 -Dmapreduce.map.java.opts=-Xmx12000M -Dmapreduce.reduce.java.opts=-Xmx12000M  -files ./mapper.R,./reducer.R  -mapper ./mapper.R -reducer ./reducer.R -input ./data/googletxt/googlebooks-eng-all-5gram-20120701-st -output ./out/stream-r-0-googlebooks-eng-all-5gram-20120701-st
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.1.0.jar] /tmp/streamjob1684701280838455254.jar tmpDir=null
15/04/14 06:17:37 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 06:17:38 INFO client.RMProxy: Connecting to ResourceManager at name.rustler.tacc.utexas.edu/129.114.57.132:8032
15/04/14 06:17:38 INFO mapred.FileInputFormat: Total input paths to process : 1
15/04/14 06:17:39 INFO mapreduce.JobSubmitter: number of splits:134
15/04/14 06:17:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1422482982071_4851
15/04/14 06:17:39 INFO impl.YarnClientImpl: Submitted application application_1422482982071_4851
15/04/14 06:17:40 INFO mapreduce.Job: The url to track the job: http://name.rustler.tacc.utexas.edu:8088/proxy/application_1422482982071_4851/
15/04/14 06:17:40 INFO mapreduce.Job: Running job: job_1422482982071_4851
15/04/14 06:17:44 INFO mapreduce.Job: Job job_1422482982071_4851 running in uber mode : false
15/04/14 06:17:44 INFO mapreduce.Job:  map 0% reduce 0%
15/04/14 06:17:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000012_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000030_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000037_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:49 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000078_0, Status : FAILED
Error: java.io.IOException: Broken pipe
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:345)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000055_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000061_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000099_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000088_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000084_0, Status : FAILED
Error: java.io.IOException: Broken pipe
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:345)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:50 INFO mapreduce.Job: Task Id : attempt_1422482982071_4851_m_000029_0, Status : FAILED
Error: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)
	at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)
	at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

15/04/14 06:17:56 INFO mapreduce.Job:  map 1% reduce 0%
15/04/14 06:18:02 INFO mapreduce.Job:  map 2% reduce 0%
15/04/14 06:18:08 INFO mapreduce.Job:  map 3% reduce 0%
15/04/14 06:18:15 INFO mapreduce.Job:  map 4% reduce 0%
15/04/14 06:18:22 INFO mapreduce.Job:  map 5% reduce 0%
15/04/14 06:18:28 INFO mapreduce.Job:  map 6% reduce 0%
15/04/14 06:18:35 INFO mapreduce.Job:  map 7% reduce 0%
15/04/14 06:18:41 INFO mapreduce.Job:  map 8% reduce 0%
15/04/14 06:18:48 INFO mapreduce.Job:  map 9% reduce 0%
15/04/14 06:18:54 INFO mapreduce.Job:  map 10% reduce 0%
15/04/14 06:19:01 INFO mapreduce.Job:  map 11% reduce 0%
15/04/14 06:19:08 INFO mapreduce.Job:  map 12% reduce 0%
15/04/14 06:19:14 INFO mapreduce.Job:  map 13% reduce 0%
15/04/14 06:19:20 INFO mapreduce.Job:  map 14% reduce 0%
15/04/14 06:19:27 INFO mapreduce.Job:  map 15% reduce 0%
15/04/14 06:19:33 INFO mapreduce.Job:  map 16% reduce 0%
15/04/14 06:19:40 INFO mapreduce.Job:  map 17% reduce 0%
15/04/14 06:19:48 INFO mapreduce.Job:  map 18% reduce 0%
15/04/14 06:19:54 INFO mapreduce.Job:  map 19% reduce 0%
15/04/14 06:20:00 INFO mapreduce.Job:  map 20% reduce 0%
15/04/14 06:20:07 INFO mapreduce.Job:  map 21% reduce 0%
15/04/14 06:20:13 INFO mapreduce.Job:  map 22% reduce 0%
15/04/14 06:20:20 INFO mapreduce.Job:  map 23% reduce 0%
15/04/14 06:20:27 INFO mapreduce.Job:  map 24% reduce 0%
15/04/14 06:20:31 INFO mapreduce.Job:  map 25% reduce 0%
15/04/14 06:20:39 INFO mapreduce.Job:  map 26% reduce 0%
15/04/14 06:20:45 INFO mapreduce.Job:  map 27% reduce 0%
15/04/14 06:20:51 INFO mapreduce.Job:  map 28% reduce 0%
15/04/14 06:20:59 INFO mapreduce.Job:  map 29% reduce 0%
15/04/14 06:21:06 INFO mapreduce.Job:  map 30% reduce 0%
15/04/14 06:21:13 INFO mapreduce.Job:  map 31% reduce 0%
15/04/14 06:21:21 INFO mapreduce.Job:  map 32% reduce 0%
15/04/14 06:21:28 INFO mapreduce.Job:  map 33% reduce 0%
15/04/14 06:21:36 INFO mapreduce.Job:  map 34% reduce 0%
15/04/14 06:21:42 INFO mapreduce.Job:  map 35% reduce 0%
15/04/14 06:21:50 INFO mapreduce.Job:  map 36% reduce 0%
15/04/14 06:21:57 INFO mapreduce.Job:  map 37% reduce 0%
15/04/14 06:22:04 INFO mapreduce.Job:  map 38% reduce 0%
15/04/14 06:22:12 INFO mapreduce.Job:  map 39% reduce 0%
15/04/14 06:22:19 INFO mapreduce.Job:  map 40% reduce 0%
15/04/14 06:22:27 INFO mapreduce.Job:  map 41% reduce 0%
15/04/14 06:22:34 INFO mapreduce.Job:  map 42% reduce 0%
15/04/14 06:22:41 INFO mapreduce.Job:  map 43% reduce 0%
15/04/14 06:22:48 INFO mapreduce.Job:  map 44% reduce 0%
15/04/14 06:22:55 INFO mapreduce.Job:  map 45% reduce 0%
15/04/14 06:23:04 INFO mapreduce.Job:  map 46% reduce 0%
15/04/14 06:23:11 INFO mapreduce.Job:  map 47% reduce 0%
15/04/14 06:23:19 INFO mapreduce.Job:  map 48% reduce 0%
15/04/14 06:23:26 INFO mapreduce.Job:  map 49% reduce 0%
15/04/14 06:23:33 INFO mapreduce.Job:  map 50% reduce 0%
15/04/14 06:23:40 INFO mapreduce.Job:  map 51% reduce 0%
15/04/14 06:23:47 INFO mapreduce.Job:  map 52% reduce 0%
15/04/14 06:23:55 INFO mapreduce.Job:  map 53% reduce 0%
15/04/14 06:24:02 INFO mapreduce.Job:  map 54% reduce 0%
15/04/14 06:24:10 INFO mapreduce.Job:  map 55% reduce 0%
15/04/14 06:24:17 INFO mapreduce.Job:  map 56% reduce 0%
15/04/14 06:24:24 INFO mapreduce.Job:  map 57% reduce 0%
15/04/14 06:24:31 INFO mapreduce.Job:  map 58% reduce 0%
15/04/14 06:24:38 INFO mapreduce.Job:  map 59% reduce 0%
15/04/14 06:24:46 INFO mapreduce.Job:  map 60% reduce 0%
15/04/14 06:24:53 INFO mapreduce.Job:  map 61% reduce 0%
15/04/14 06:25:01 INFO mapreduce.Job:  map 62% reduce 0%
15/04/14 06:25:08 INFO mapreduce.Job:  map 63% reduce 0%
15/04/14 06:25:16 INFO mapreduce.Job:  map 64% reduce 0%
15/04/14 06:25:22 INFO mapreduce.Job:  map 65% reduce 0%
15/04/14 06:25:26 INFO mapreduce.Job:  map 66% reduce 0%
15/04/14 06:25:30 INFO mapreduce.Job:  map 67% reduce 0%
15/04/14 06:25:32 INFO mapreduce.Job:  map 68% reduce 0%
15/04/14 06:25:34 INFO mapreduce.Job:  map 69% reduce 0%
15/04/14 06:25:36 INFO mapreduce.Job:  map 70% reduce 0%
15/04/14 06:25:37 INFO mapreduce.Job:  map 71% reduce 0%
15/04/14 06:25:39 INFO mapreduce.Job:  map 72% reduce 0%
15/04/14 06:25:40 INFO mapreduce.Job:  map 72% reduce 1%
15/04/14 06:25:41 INFO mapreduce.Job:  map 73% reduce 6%
15/04/14 06:25:43 INFO mapreduce.Job:  map 74% reduce 7%
15/04/14 06:25:44 INFO mapreduce.Job:  map 75% reduce 8%
15/04/14 06:25:46 INFO mapreduce.Job:  map 76% reduce 8%
15/04/14 06:25:47 INFO mapreduce.Job:  map 76% reduce 9%
15/04/14 06:25:48 INFO mapreduce.Job:  map 77% reduce 9%
15/04/14 06:25:50 INFO mapreduce.Job:  map 77% reduce 11%
15/04/14 06:25:51 INFO mapreduce.Job:  map 78% reduce 11%
15/04/14 06:25:53 INFO mapreduce.Job:  map 78% reduce 12%
15/04/14 06:25:54 INFO mapreduce.Job:  map 79% reduce 12%
15/04/14 06:25:56 INFO mapreduce.Job:  map 80% reduce 12%
15/04/14 06:25:58 INFO mapreduce.Job:  map 81% reduce 12%
15/04/14 06:25:59 INFO mapreduce.Job:  map 81% reduce 14%
15/04/14 06:26:00 INFO mapreduce.Job:  map 82% reduce 14%
15/04/14 06:26:01 INFO mapreduce.Job:  map 83% reduce 14%
15/04/14 06:26:02 INFO mapreduce.Job:  map 83% reduce 16%
15/04/14 06:26:03 INFO mapreduce.Job:  map 84% reduce 16%
15/04/14 06:26:04 INFO mapreduce.Job:  map 85% reduce 16%
15/04/14 06:26:05 INFO mapreduce.Job:  map 85% reduce 18%
15/04/14 06:26:06 INFO mapreduce.Job:  map 87% reduce 18%
15/04/14 06:26:07 INFO mapreduce.Job:  map 88% reduce 18%
15/04/14 06:26:08 INFO mapreduce.Job:  map 89% reduce 21%
15/04/14 06:26:09 INFO mapreduce.Job:  map 90% reduce 21%
15/04/14 06:26:10 INFO mapreduce.Job:  map 92% reduce 21%
15/04/14 06:26:11 INFO mapreduce.Job:  map 94% reduce 25%
15/04/14 06:26:13 INFO mapreduce.Job:  map 95% reduce 25%
15/04/14 06:26:14 INFO mapreduce.Job:  map 95% reduce 27%
15/04/14 06:26:16 INFO mapreduce.Job:  map 96% reduce 27%
15/04/14 06:26:17 INFO mapreduce.Job:  map 97% reduce 28%
15/04/14 06:26:19 INFO mapreduce.Job:  map 98% reduce 28%
15/04/14 06:26:20 INFO mapreduce.Job:  map 98% reduce 30%
15/04/14 06:26:25 INFO mapreduce.Job:  map 99% reduce 30%
15/04/14 06:26:27 INFO mapreduce.Job:  map 99% reduce 31%
15/04/14 06:26:30 INFO mapreduce.Job:  map 100% reduce 31%
15/04/14 06:26:39 INFO mapreduce.Job:  map 100% reduce 32%
15/04/14 06:26:42 INFO mapreduce.Job:  map 100% reduce 33%
15/04/14 06:26:48 INFO mapreduce.Job:  map 100% reduce 34%
15/04/14 06:26:51 INFO mapreduce.Job:  map 100% reduce 38%
15/04/14 06:26:52 INFO mapreduce.Job:  map 100% reduce 39%
15/04/14 06:26:54 INFO mapreduce.Job:  map 100% reduce 44%
15/04/14 06:26:57 INFO mapreduce.Job:  map 100% reduce 49%
15/04/14 06:26:58 INFO mapreduce.Job:  map 100% reduce 50%
15/04/14 06:27:00 INFO mapreduce.Job:  map 100% reduce 55%
15/04/14 06:27:03 INFO mapreduce.Job:  map 100% reduce 59%
15/04/14 06:27:06 INFO mapreduce.Job:  map 100% reduce 61%
15/04/14 06:27:12 INFO mapreduce.Job:  map 100% reduce 64%
15/04/14 06:27:15 INFO mapreduce.Job:  map 100% reduce 65%
15/04/14 06:27:18 INFO mapreduce.Job:  map 100% reduce 66%
15/04/14 06:27:31 INFO mapreduce.Job:  map 100% reduce 67%
15/04/14 06:28:50 INFO mapreduce.Job:  map 100% reduce 68%
15/04/14 06:30:05 INFO mapreduce.Job:  map 100% reduce 69%
15/04/14 06:31:39 INFO mapreduce.Job:  map 100% reduce 70%
15/04/14 06:32:42 INFO mapreduce.Job:  map 100% reduce 71%
15/04/14 06:34:03 INFO mapreduce.Job:  map 100% reduce 72%
15/04/14 06:35:25 INFO mapreduce.Job:  map 100% reduce 73%
15/04/14 06:36:17 INFO mapreduce.Job:  map 100% reduce 74%
15/04/14 06:37:20 INFO mapreduce.Job:  map 100% reduce 75%
15/04/14 06:38:27 INFO mapreduce.Job:  map 100% reduce 76%
15/04/14 06:39:21 INFO mapreduce.Job:  map 100% reduce 77%
15/04/14 06:40:42 INFO mapreduce.Job:  map 100% reduce 78%
15/04/14 06:41:18 INFO mapreduce.Job:  map 100% reduce 79%
15/04/14 06:42:10 INFO mapreduce.Job:  map 100% reduce 80%
15/04/14 06:42:57 INFO mapreduce.Job:  map 100% reduce 81%
15/04/14 06:44:08 INFO mapreduce.Job:  map 100% reduce 82%
15/04/14 06:45:04 INFO mapreduce.Job:  map 100% reduce 83%
15/04/14 06:46:02 INFO mapreduce.Job:  map 100% reduce 84%
15/04/14 06:47:15 INFO mapreduce.Job:  map 100% reduce 85%
15/04/14 06:48:33 INFO mapreduce.Job:  map 100% reduce 86%
15/04/14 06:49:26 INFO mapreduce.Job:  map 100% reduce 87%
15/04/14 06:50:58 INFO mapreduce.Job:  map 100% reduce 88%
15/04/14 06:52:08 INFO mapreduce.Job:  map 100% reduce 89%
15/04/14 06:54:03 INFO mapreduce.Job:  map 100% reduce 90%
15/04/14 06:56:18 INFO mapreduce.Job:  map 100% reduce 91%
15/04/14 06:58:24 INFO mapreduce.Job:  map 100% reduce 92%
15/04/14 06:59:35 INFO mapreduce.Job:  map 100% reduce 93%
15/04/14 07:05:50 INFO mapreduce.Job:  map 100% reduce 94%
15/04/14 07:11:09 INFO mapreduce.Job:  map 100% reduce 95%
15/04/14 07:16:16 INFO mapreduce.Job:  map 100% reduce 96%
15/04/14 07:20:38 INFO mapreduce.Job:  map 100% reduce 97%
15/04/14 07:25:39 INFO mapreduce.Job:  map 100% reduce 98%
15/04/14 07:49:25 INFO mapreduce.Job:  map 100% reduce 99%
15/04/14 08:43:53 INFO mapreduce.Job:  map 100% reduce 100%
15/04/14 08:43:53 INFO mapreduce.Job: Job job_1422482982071_4851 failed with state KILLED due to: Kill job job_1422482982071_4851 received from dotcz12 (auth:SIMPLE) at 129.114.57.130
Job received Kill while in RUNNING state.

15/04/14 08:43:53 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=27900249520
		FILE: Number of bytes written=58340556909
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=17901410693
		HDFS: Number of bytes written=4598499
		HDFS: Number of read operations=504
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=68
	Job Counters 
		Failed map tasks=10
		Killed reduce tasks=3
		Launched map tasks=144
		Launched reduce tasks=37
		Other local map tasks=10
		Data-local map tasks=61
		Rack-local map tasks=73
		Total time spent by all maps in occupied slots (ms)=131211096
		Total time spent by all reduces in occupied slots (ms)=183536662
		Total time spent by all map tasks (ms)=65605548
		Total time spent by all reduce tasks (ms)=91768331
		Total vcore-seconds taken by all map tasks=65605548
		Total vcore-seconds taken by all reduce tasks=91768331
		Total megabyte-seconds taken by all map tasks=531142516608
		Total megabyte-seconds taken by all reduce tasks=1101219972000
	Map-Reduce Framework
		Map input records=391355345
		Map output records=3130842760
		Map output bytes=24162530273
		Map output materialized bytes=30424243933
		Input split bytes=21172
		Combine input records=0
		Combine output records=0
		Reduce input groups=316095
		Reduce shuffle bytes=27900276598
		Reduce input records=2758765204
		Reduce output records=316095
		Spilled Records=5889607964
		Shuffled Maps =4556
		Failed Shuffles=0
		Merged Map outputs=4556
		GC time elapsed (ms)=257027
		CPU time spent (ms)=168129910
		Physical memory (bytes) snapshot=296855572480
		Virtual memory (bytes) snapshot=1684481044480
		Total committed heap usage (bytes)=442713747456
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=17901389521
	File Output Format Counters 
		Bytes Written=4598499
15/04/14 08:43:53 ERROR streaming.StreamJob: Job not Successful!
Streaming Command Failed!

real	146m20.627s
user	0m33.420s
sys	0m6.242s
